"use strict";(this.webpackChunkGreed=this.webpackChunkGreed||[]).push([[867],{867:(e,n,t)=>{function s(){return'\n# WebGPU-enabled PyTorch polyfill setup\nimport numpy as np\nimport sys\n\nclass WebGPUDevice:\n    def __init__(self, device_type):\n        self.type = device_type\n        \n    def __str__(self):\n        return self.type\n        \n    def __repr__(self):\n        return f"device(type=\'{self.type}\')"\n\nclass WebGPUTensor:\n    def __init__(self, data, device=\'cpu\', dtype=\'float32\', requires_grad=False, _force_webgpu=False):\n        if isinstance(data, (list, tuple)):\n            self._data = np.array(data, dtype=dtype)\n        elif isinstance(data, np.ndarray):\n            self._data = data.astype(dtype)\n        else:\n            self._data = np.array(data, dtype=dtype)\n        \n        # Determine actual device based on tensor size and WebGPU availability\n        self._original_device = device\n        self._force_webgpu = _force_webgpu\n        \n        # TEMPORARILY DISABLED: Auto-detect WebGPU to prevent recursion\n        # Auto-detect WebGPU usage for larger tensors or when forced\n        # if (_force_webgpu or self._should_use_webgpu(data)) and device != \'cpu\':\n        #     self.device = WebGPUDevice(\'webgpu\')\n        # elif device == \'cuda\' or device == \'gpu\':\n        #     # Map CUDA/GPU requests to WebGPU if available\n        #     self.device = WebGPUDevice(\'webgpu\')\n        # else:\n        #     self.device = WebGPUDevice(device) if isinstance(device, str) else device\n\n        # Simple device assignment to prevent recursion\n        self.device = device\n\n        self.dtype = dtype\n        self.requires_grad = requires_grad\n        self.shape = self._data.shape\n        self.ndim = self._data.ndim\n        self.grad = None\n        self.grad_fn = None\n\n    @property\n    def data(self):\n        """Return data wrapped with PyTorch-like methods"""\n        return TensorDataWrapper(self._data, self)\n    \n    def size(self, dim=None):\n        """Return the size of the tensor or a specific dimension"""\n        if dim is None:\n            return self.shape\n        else:\n            if dim < 0:\n                dim = self.ndim + dim\n            if dim >= self.ndim or dim < 0:\n                raise IndexError(f"Dimension out of range (expected to be in range of [{-self.ndim}, {self.ndim-1}], but got {dim})")\n            return self.shape[dim]\n\n    def numel(self):\n        """Return the total number of elements in the tensor"""\n        return self._data.size\n\n    def _should_use_webgpu(self, data):\n        """Determine if WebGPU should be used based on tensor characteristics"""\n        try:\n            # Use WebGPU for tensors with more than 1 element (very low threshold)\n            if hasattr(data, \'size\'):\n                return data.size > 1\n            elif hasattr(data, \'__len__\'):\n                return len(data) > 1\n            return False\n        except:\n            return False\n        \n    def numpy(self):\n        return self._data\n\n    def tolist(self):\n        return self._data.tolist()\n\n    def __str__(self):\n        """String representation of tensor"""\n        result = f"tensor({self._data.tolist()}, requires_grad={self.requires_grad})"\n        return result\n\n    def __repr__(self):\n        """Detailed representation of tensor"""\n        return f"WebGPUTensor({self._data}, device=\'{self.device}\', requires_grad={self.requires_grad})"\n\n    def item(self):\n        """Return the value of this tensor as a standard Python number"""\n        if self._data.size == 1:\n            value = self._data.item()\n            # Ensure we return proper Python types that can be used as indices\n            if self.dtype in [\'int32\', \'int64\', \'long\']:\n                return int(value)\n            elif self.dtype in [\'float32\', \'float64\', \'double\']:\n                return float(value)\n            else:\n                # For other types, try to convert appropriately\n                if isinstance(value, (int, np.integer)):\n                    return int(value)\n                elif isinstance(value, (float, np.floating)):\n                    return float(value)\n                else:\n                    return value\n        else:\n            raise ValueError("only one element tensors can be converted to Python scalars")\n\n    def __format__(self, format_spec):\n        """Support for f-string formatting"""\n        if self._data.size == 1:\n            return format(self._data.item(), format_spec)\n        else:\n            return format(str(self), format_spec)\n\n    def view(self, *shape):\n        """Reshape tensor maintaining data"""\n        if len(shape) == 1 and isinstance(shape[0], (list, tuple)):\n            shape = shape[0]\n        \n        # Handle -1 for automatic size calculation\n        if -1 in shape:\n            total_size = self._data.size\n            known_size = 1\n            unknown_idx = -1\n            for i, s in enumerate(shape):\n                if s == -1:\n                    unknown_idx = i\n                else:\n                    known_size *= s\n            if unknown_idx != -1:\n                shape = list(shape)\n                shape[unknown_idx] = total_size // known_size\n                shape = tuple(shape)\n        \n        reshaped_data = self._data.reshape(shape)\n        return WebGPUTensor(reshaped_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n    \n    def reshape(self, *shape):\n        return self.view(*shape)\n    \n    def transpose(self, dim0, dim1):\n        transposed_data = np.swapaxes(self._data, dim0, dim1)\n        return WebGPUTensor(transposed_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n\n    def unsqueeze(self, dim):\n        """Add a dimension of size 1"""\n        new_shape = list(self._data.shape)\n        if dim < 0:\n            dim = len(new_shape) + dim + 1\n        new_shape.insert(dim, 1)\n        reshaped_data = self._data.reshape(new_shape)\n        return WebGPUTensor(reshaped_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n\n    def flatten(self, start_dim=0, end_dim=-1):\n        """Flatten tensor dimensions"""\n        if end_dim == -1:\n            end_dim = self._data.ndim - 1\n\n        shape = list(self._data.shape)\n        flattened_size = 1\n        for i in range(start_dim, end_dim + 1):\n            flattened_size *= shape[i]\n\n        new_shape = shape[:start_dim] + [flattened_size] + shape[end_dim + 1:]\n        flattened_data = self._data.reshape(new_shape)\n        return WebGPUTensor(flattened_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n\n    def squeeze(self, dim=None):\n        """Remove dimensions of size 1"""\n        if dim is None:\n            # Remove all dimensions of size 1\n            squeezed_data = np.squeeze(self._data)\n        else:\n            # Remove specific dimension if it has size 1\n            if dim < 0:\n                dim = self._data.ndim + dim\n            if self._data.shape[dim] != 1:\n                return self  # No change if dimension is not size 1\n            squeezed_data = np.squeeze(self._data, axis=dim)\n\n        return WebGPUTensor(squeezed_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n\n    def sum(self, dim=None, keepdim=False):\n        if dim is None:\n            result_data = np.sum(self._data)\n        else:\n            result_data = np.sum(self._data, axis=dim, keepdims=keepdim)\n        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n\n        # Autograd disabled for stability - backward functions removed\n\n        return result\n    \n    def mean(self, dim=None, keepdim=False):\n        if dim is None:\n            result_data = np.mean(self._data)\n        else:\n            result_data = np.mean(self._data, axis=dim, keepdims=keepdim)\n        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n    \n    def std(self, dim=None, keepdim=False, unbiased=True):\n        """Compute standard deviation"""\n        if dim is None:\n            result_data = np.std(self._data, ddof=1 if unbiased else 0)\n        else:\n            result_data = np.std(self._data, axis=dim, keepdims=keepdim, ddof=1 if unbiased else 0)\n        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n    \n    def var(self, dim=None, keepdim=False, unbiased=True):\n        """Compute variance"""\n        if dim is None:\n            result_data = np.var(self._data, ddof=1 if unbiased else 0)\n        else:\n            result_data = np.var(self._data, axis=dim, keepdims=keepdim, ddof=1 if unbiased else 0)\n        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n    \n    def to(self, device):\n        new_device = WebGPUDevice(device) if isinstance(device, str) else device\n        return WebGPUTensor(self._data.copy(), device=new_device, dtype=self.dtype, requires_grad=self.requires_grad)\n    \n    def cpu(self):\n        return self.to(\'cpu\')\n    \n    def cuda(self):\n        return self.to(\'webgpu\')  # Map CUDA to WebGPU\n\n    def __getitem__(self, key):\n        """Support tensor slicing like X[:, 0]"""\n        if isinstance(key, tuple):\n            # Multi-dimensional indexing\n            indexed_data = self._data[key]\n        else:\n            # Single dimension indexing\n            indexed_data = self._data[key]\n\n\n        return WebGPUTensor(indexed_data, device=self.device, dtype=self.dtype, requires_grad=self.requires_grad)\n\n    def backward(self, gradient=None, retain_graph=False, create_graph=False):\n        """Backward propagation through the computation graph"""\n        if not self.requires_grad:\n            return\n\n        if gradient is None:\n            if self._data.size != 1:\n                raise RuntimeError("grad can be implicitly created only for scalar outputs")\n            gradient = WebGPUTensor(np.ones_like(self._data), device=self.device, dtype=self.dtype)\n\n        # Simple backward pass - accumulate gradients\n        if hasattr(self, \'_backward_fn\') and self._backward_fn:\n            self._backward_fn(gradient)\n        else:\n            # Initialize gradient for leaf variables\n            if self.grad is None:\n                self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype)\n            self.grad._data += gradient._data if hasattr(gradient, \'_data\') else gradient\n\n    def __repr__(self):\n        return f"tensor({self._data}, device=\'{self.device}\', dtype=\'{self.dtype}\')"\n    \n    def __float__(self):\n        """Convert single-element tensor to Python float"""\n        if self._data.size == 1:\n            return float(self._data.item())\n        else:\n            raise TypeError(f"only single-element tensors can be converted to Python scalars")\n    \n    def __int__(self):\n        """Convert single-element tensor to Python int"""\n        if self._data.size == 1:\n            return int(self._data.item())\n        else:\n            raise TypeError(f"only single-element tensors can be converted to Python scalars")\n\n    def __len__(self):\n        """Return the length of the first dimension"""\n        if self.ndim == 0:\n            raise TypeError("len() of unsized object")\n        return self.shape[0]\n\n    def __getitem__(self, key):\n        """Support tensor indexing like tensor[indices]"""\n        if isinstance(key, WebGPUTensor):\n            # Convert WebGPUTensor indices to numpy array\n            indices = key._data.astype(int)\n            result_data = self._data[indices]\n\n            # In PyTorch, indexing with tensor indices always preserves at least 1 dimension\n            # even when the index tensor has 1 element\n            if result_data.ndim == 0:\n                result_data = np.array([result_data])\n        else:\n            result_data = self._data[key]\n\n        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n    \n    def __setitem__(self, key, value):\n        """Support tensor item assignment like tensor[indices] = value"""\n        if isinstance(value, WebGPUTensor):\n            value_data = value._data\n        else:\n            value_data = value\n\n        if isinstance(key, WebGPUTensor):\n            # Convert WebGPUTensor indices to numpy array\n            indices = key._data.astype(int)\n            self._data[indices] = value_data\n        else:\n            self._data[key] = value_data\n    \n    # Comparison operators\n    def __eq__(self, other):\n        """Element-wise equality comparison"""\n        if isinstance(other, WebGPUTensor):\n            result_data = self._data == other._data\n        else:\n            result_data = self._data == other\n        return WebGPUTensor(result_data, device="webgpu", dtype=\'bool\')\n\n    def __ne__(self, other):\n        """Element-wise not-equal comparison"""\n        if isinstance(other, WebGPUTensor):\n            result_data = self._data != other._data\n        else:\n            result_data = self._data != other\n        return WebGPUTensor(result_data, device="webgpu", dtype=\'bool\')\n    \n    def __gt__(self, other):\n        """Element-wise greater than comparison"""\n        if isinstance(other, WebGPUTensor):\n            result_data = self._data > other._data\n        else:\n            result_data = self._data > other\n        return WebGPUTensor(result_data, device="webgpu", dtype=\'bool\')\n\n    def __lt__(self, other):\n        """Element-wise less than comparison"""\n        if isinstance(other, WebGPUTensor):\n            result_data = self._data < other._data\n        else:\n            result_data = self._data < other\n        return WebGPUTensor(result_data, device="webgpu", dtype=\'bool\')\n\n    def __ge__(self, other):\n        """Element-wise greater than or equal comparison"""\n        if isinstance(other, WebGPUTensor):\n            result_data = self._data >= other._data\n        else:\n            result_data = self._data >= other\n        return WebGPUTensor(result_data, device="webgpu", dtype=\'bool\')\n\n    def __le__(self, other):\n        """Element-wise less than or equal comparison"""\n        if isinstance(other, WebGPUTensor):\n            result_data = self._data <= other._data\n        else:\n            result_data = self._data <= other\n        return WebGPUTensor(result_data, device="webgpu", dtype=\'bool\')\n\n    # Arithmetic operators\n    def __add__(self, other):\n        if isinstance(other, WebGPUTensor):\n            result_data = self._data + other._data\n        else:\n            result_data = self._data + other\n\n        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype,\n                             requires_grad=self.requires_grad or (isinstance(other, WebGPUTensor) and other.requires_grad))\n\n        # Autograd disabled for stability - backward functions removed\n\n        return result\n    \n    def __sub__(self, other):\n        if isinstance(other, WebGPUTensor):\n            result_data = self._data - other._data\n        else:\n            result_data = self._data - other\n        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n    \n    def __mul__(self, other):\n        if isinstance(other, WebGPUTensor):\n            result_data = self._data * other._data\n        else:\n            result_data = self._data * other\n\n        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype,\n                             requires_grad=self.requires_grad or (isinstance(other, WebGPUTensor) and other.requires_grad))\n\n        # Autograd disabled for stability - backward functions removed\n\n        return result\n    \n    def __truediv__(self, other):\n        if isinstance(other, WebGPUTensor):\n            result_data = self._data / other._data\n        else:\n            result_data = self._data / other\n        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n\n    def __pow__(self, other):\n        if isinstance(other, WebGPUTensor):\n            result_data = np.power(self._data, other._data)\n        else:\n            result_data = np.power(self._data, other)\n\n        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype,\n                             requires_grad=self.requires_grad or (isinstance(other, WebGPUTensor) and other.requires_grad))\n\n        # Autograd disabled for stability - backward functions removed\n\n        return result\n    \n    def __radd__(self, other):\n        result_data = other + self._data\n        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n\n        # Autograd disabled for stability - backward functions removed\n\n        return result\n    \n    def __rmul__(self, other):\n        result_data = other * self._data\n        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n\n        # Autograd disabled for stability - backward functions removed\n\n        return result\n\n    def __rsub__(self, other):\n        result_data = other - self._data\n        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n\n        # Autograd disabled for stability - backward functions removed\n\n        return result\n\n    def __rtruediv__(self, other):\n        result_data = other / self._data\n        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n\n        # Autograd disabled for stability - backward functions removed\n\n        return result\n\n    def __rpow__(self, other):\n        result_data = np.power(other, self._data)\n        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n\n        # Autograd disabled for stability - backward functions removed\n\n        return result\n    \n    def __matmul__(self, other):\n        """Matrix multiplication operator (@)"""\n        if isinstance(other, WebGPUTensor):\n            if self.ndim == 2 and other.ndim == 2:\n                result_data = np.dot(self._data, other._data)\n            elif self.ndim == 1 and other.ndim == 2:\n                result_data = np.dot(self._data, other._data)\n            elif self.ndim == 2 and other.ndim == 1:\n                result_data = np.dot(self._data, other._data)\n            else:\n                result_data = np.matmul(self._data, other._data)\n        else:\n            result_data = np.matmul(self._data, other)\n        \n        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype,\n                             requires_grad=self.requires_grad or (isinstance(other, WebGPUTensor) and other.requires_grad))\n\n        # Autograd disabled for stability - backward functions removed\n        \n        return result\n    \n    def __rmatmul__(self, other):\n        """Reverse matrix multiplication"""\n        result_data = np.matmul(other, self._data)\n        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)\n    \n    def retain_grad(self):\n        """Enable gradient retention for non-leaf tensor"""\n        if not self.requires_grad:\n            raise RuntimeError("can\'t retain_grad on Tensor that has requires_grad=False")\n        self._retain_grad = True\n        return self\n    \n    def zero_grad(self):\n        """Zero out the gradients"""\n        if self.grad is not None:\n            self.grad._data.fill(0)\n\n    def backward(self, gradient=None, retain_graph=False, create_graph=False):\n        """Backward pass for automatic differentiation"""\n\n        if not self.requires_grad:\n            return\n\n        # Default gradient is ones with same shape as tensor\n        if gradient is None:\n            if self._data.size == 1:\n                # Scalar tensor - gradient is 1.0\n                gradient = WebGPUTensor([1.0], device=self.device, dtype=self.dtype)\n            else:\n                # Non-scalar tensor - gradient should be provided\n                gradient = WebGPUTensor(np.ones_like(self._data), device=self.device, dtype=self.dtype)\n\n        # Call the backward function if it exists\n        if hasattr(self, \'_backward_fn\') and self._backward_fn is not None:\n            self._backward_fn(gradient)\n        else:\n            # Initialize gradient for leaf variables\n            if self.grad is None:\n                self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype)\n            self.grad._data += gradient._data if hasattr(gradient, \'_data\') else gradient\n    \n    def _matmul_backward(self, grad_output, other):\n        """Backward pass for matrix multiplication"""\n        if isinstance(other, WebGPUTensor):\n            # d/da (a @ b) = grad_output @ b.T\n            if self.grad is None:\n                self.grad = WebGPUTensor(np.zeros_like(self._data), device="webgpu", dtype=self.dtype)\n            self_grad = np.matmul(grad_output._data, other._data.T)\n            self.grad._data += self_grad\n            \n            # d/db (a @ b) = a.T @ grad_output  \n            if other.requires_grad:\n                if other.grad is None:\n                    other.grad = WebGPUTensor(np.zeros_like(other._data), device="webgpu", dtype=other.dtype)\n                other_grad = np.matmul(self._data.T, grad_output._data)\n                other.grad._data += other_grad\n\n# Linear algebra operations module\nclass TorchLinalg:\n    """Linear algebra operations module"""\n    \n    def __init__(self):\n        pass\n    \n    def det(self, input_tensor):\n        """Compute determinant"""\n        if isinstance(input_tensor, WebGPUTensor):\n            if input_tensor.ndim != 2 or input_tensor.shape[0] != input_tensor.shape[1]:\n                raise RuntimeError("linalg.det() expects a 2D square tensor")\n            det_value = np.linalg.det(input_tensor._data.reshape(input_tensor.shape))\n            return WebGPUTensor([det_value], device="webgpu", dtype=input_tensor.dtype)\n        else:\n            return np.linalg.det(input_tensor)\n    \n    def inv(self, input_tensor):\n        """Compute matrix inverse"""\n        if isinstance(input_tensor, WebGPUTensor):\n            if input_tensor.ndim != 2 or input_tensor.shape[0] != input_tensor.shape[1]:\n                raise RuntimeError("linalg.inv() expects a 2D square tensor")\n            inv_data = np.linalg.inv(input_tensor._data.reshape(input_tensor.shape))\n            return WebGPUTensor(inv_data, device="webgpu", dtype=input_tensor.dtype)\n        else:\n            return np.linalg.inv(input_tensor)\n    \n    def norm(self, input_tensor, ord=None, dim=None, keepdim=False):\n        """Compute matrix or vector norm"""\n        if isinstance(input_tensor, WebGPUTensor):\n            if dim is None:\n                norm_value = np.linalg.norm(input_tensor._data, ord=ord)\n                return WebGPUTensor([norm_value], device="webgpu", dtype=input_tensor.dtype)\n            else:\n                norm_data = np.linalg.norm(input_tensor._data.reshape(input_tensor.shape), ord=ord, axis=dim, keepdims=keepdim)\n                return WebGPUTensor(norm_data, device="webgpu", dtype=input_tensor.dtype)\n        else:\n            return np.linalg.norm(input_tensor, ord=ord, axis=dim, keepdims=keepdim)\n    \n    def eig(self, input_tensor):\n        """Compute eigenvalues and eigenvectors"""\n        if isinstance(input_tensor, WebGPUTensor):\n            if input_tensor.ndim != 2 or input_tensor.shape[0] != input_tensor.shape[1]:\n                raise RuntimeError("linalg.eig() expects a 2D square tensor")\n            eigenvalues, eigenvectors = np.linalg.eig(input_tensor._data.reshape(input_tensor.shape))\n            return (\n                WebGPUTensor(eigenvalues, device="webgpu", dtype=input_tensor.dtype),\n                WebGPUTensor(eigenvectors, device="webgpu", dtype=input_tensor.dtype)\n            )\n        else:\n            return np.linalg.eig(input_tensor)\n    \n    def svd(self, input_tensor, full_matrices=True):\n        """Compute singular value decomposition"""\n        if isinstance(input_tensor, WebGPUTensor):\n            U, S, Vh = np.linalg.svd(input_tensor._data.reshape(input_tensor.shape), full_matrices=full_matrices)\n            return (\n                WebGPUTensor(U, device="webgpu", dtype=input_tensor.dtype),\n                WebGPUTensor(S, device="webgpu", dtype=input_tensor.dtype),\n                WebGPUTensor(Vh, device="webgpu", dtype=input_tensor.dtype)\n            )\n        else:\n            return np.linalg.svd(input_tensor, full_matrices=full_matrices)\n\n# Neural network functional operations\nclass TorchNNFunctional:\n    @staticmethod\n    def relu(input_tensor):\n        if isinstance(input_tensor, WebGPUTensor):\n            result_data = np.maximum(input_tensor._data, 0)\n            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype)\n        else:\n            return np.maximum(input_tensor, 0)\n    \n    @staticmethod\n    def sigmoid(input_tensor):\n        if isinstance(input_tensor, WebGPUTensor):\n            result_data = 1 / (1 + np.exp(-input_tensor._data))\n            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype)\n        else:\n            return 1 / (1 + np.exp(-input_tensor))\n\n# Neural network modules\nclass TorchNNModule:\n    def __init__(self):\n        self._parameters = {}\n        self._modules = {}\n        self._buffers = {}\n        self._non_persistent_buffers_set = set()\n        self._backward_hooks = {}\n        self._backward_pre_hooks = {}\n        self._forward_hooks = {}\n        self._forward_pre_hooks = {}\n        self._state_dict_hooks = {}\n        self._load_state_dict_pre_hooks = {}\n        self.training = True\n        \n    def parameters(self):\n        params = []\n        debug_id = getattr(self, \'_debug_id\', \'UNKNOWN\')\n        class_name = self.__class__.__name__\n        for param in self._parameters.values():\n            params.append(param)\n        for module_name, module in self._modules.items():\n            if hasattr(module, \'parameters\'):\n                subparams = module.parameters()\n                params.extend(subparams)\n        return params\n    \n    def __setattr__(self, name, value):\n        """Override setattr to automatically register modules and parameters"""\n        # Check if the value is a TorchNNModule (submodule)\n        if isinstance(value, TorchNNModule):\n            if hasattr(self, \'_modules\'):\n                self._modules[name] = value\n\n        # Check if the value is a WebGPUTensor with requires_grad (parameter)\n        elif isinstance(value, WebGPUTensor) and getattr(value, \'requires_grad\', False):\n            if hasattr(self, \'_parameters\'):\n                self._parameters[name] = value\n\n        # Default setattr behavior\n        object.__setattr__(self, name, value)\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n    def forward(self, x):\n        raise NotImplementedError\n\n    def to(self, device):\n        """Move module to specified device"""\n        # Convert device string to device object if needed\n        if isinstance(device, str):\n            if device == \'cuda\':\n                device = \'webgpu\'  # Map CUDA to WebGPU\n            target_device = device\n        else:\n            target_device = device.type if hasattr(device, \'type\') else str(device)\n\n        # Move all parameters to the new device\n        for param in self.parameters():\n            param.device = target_device\n            # Update the device reference for WebGPU acceleration\n            if hasattr(param, \'data\'):\n                param._data = param._data  # Data stays in numpy, device is just metadata\n\n        # Recursively move submodules\n        for module in self._modules.values():\n            if hasattr(module, \'to\'):\n                module.to(target_device)\n\n        return self\n\n    def cpu(self):\n        """Move module to CPU"""\n        return self.to(\'cpu\')\n\n    def cuda(self):\n        """Move module to WebGPU (CUDA equivalent)"""\n        return self.to(\'webgpu\')\n\n    def train(self, mode=True):\n        """Set the module in training mode"""\n        self.training = mode\n        # Recursively set training mode for all submodules\n        for module in self._modules.values():\n            if hasattr(module, \'train\'):\n                module.train(mode)\n        return self\n\n    def eval(self):\n        """Set the module in evaluation mode"""\n        return self.train(False)\n\nclass TorchNNLinear(TorchNNModule):\n    def __init__(self, in_features, out_features, bias=True):\n        import random\n        self._debug_id = random.randint(1000, 9999)\n        self._parameters = {}\n        self._modules = {}\n        self._buffers = {}\n        self._non_persistent_buffers_set = set()\n        self._backward_hooks = {}\n        self._backward_pre_hooks = {}\n        self._forward_hooks = {}\n        self._forward_pre_hooks = {}\n        self._state_dict_hooks = {}\n        self._load_state_dict_pre_hooks = {}\n        self.training = True\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        weight_data = np.random.randn(out_features, in_features) * np.sqrt(2.0 / in_features)\n        self.weight = WebGPUTensor(weight_data, requires_grad=True)\n        self._parameters[\'weight\'] = self.weight\n\n        if bias:\n            bias_data = np.zeros(out_features)\n            self.bias = WebGPUTensor(bias_data, requires_grad=True)\n            self._parameters[\'bias\'] = self.bias\n        else:\n            self.bias = None\n    \n    def forward(self, x):\n        if isinstance(x, WebGPUTensor):\n            # Linear transformation: y = xW^T + b\n            result_data = np.dot(x._data, self.weight._data.T)\n            if self.bias is not None:\n                result_data = result_data + self.bias._data\n\n            result = WebGPUTensor(result_data, device="webgpu", dtype=x.dtype,\n                                requires_grad=(x.requires_grad or self.weight.requires_grad or\n                                             (self.bias is not None and self.bias.requires_grad)))\n\n            # Set up backward function for linear layer\n            def linear_backward(grad_output):\n\n                if x.requires_grad:\n                    # Gradient w.r.t input: grad_output @ weight\n                    if x.grad is None:\n                        x.grad = WebGPUTensor(np.zeros_like(x._data), device="webgpu", dtype=x.dtype)\n                    x.grad._data += np.dot(grad_output._data, self.weight._data)\n\n                if self.weight.requires_grad:\n                    # Gradient w.r.t weight: x^T @ grad_output\n                    if self.weight.grad is None:\n                        self.weight.grad = WebGPUTensor(np.zeros_like(self.weight._data), device="webgpu", dtype=self.weight.dtype)\n                    weight_grad = np.dot(grad_output._data.T, x._data)\n                    self.weight.grad._data += weight_grad\n\n                if self.bias is not None and self.bias.requires_grad:\n                    # Gradient w.r.t bias: sum(grad_output, axis=0)\n                    if self.bias.grad is None:\n                        self.bias.grad = WebGPUTensor(np.zeros_like(self.bias._data), device="webgpu", dtype=self.bias.dtype)\n                    bias_grad = np.sum(grad_output._data, axis=0)\n                    self.bias.grad._data += bias_grad\n\n            result._backward_fn = linear_backward\n            result._inputs = [x, self.weight] + ([self.bias] if self.bias else [])\n\n            return result\n        else:\n            raise TypeError("Input must be WebGPUTensor")\n\nclass TorchNNReLU(TorchNNModule):\n    def __init__(self):\n        self._parameters = {}\n        self._modules = {}\n        self._buffers = {}\n        self._non_persistent_buffers_set = set()\n        self._backward_hooks = {}\n        self._backward_pre_hooks = {}\n        self._forward_hooks = {}\n        self._forward_pre_hooks = {}\n        self._state_dict_hooks = {}\n        self._load_state_dict_pre_hooks = {}\n        self.training = True\n\n    def forward(self, x):\n        if isinstance(x, WebGPUTensor):\n            result_data = np.maximum(x.data, 0)\n            return WebGPUTensor(result_data, device="webgpu", dtype=x.dtype, requires_grad=x.requires_grad)\n        else:\n            return np.maximum(x, 0)\n\nclass TorchNNMSELoss(TorchNNModule):\n    def __init__(self, reduction=\'mean\'):\n        self._parameters = {}\n        self._modules = {}\n        self._buffers = {}\n        self._non_persistent_buffers_set = set()\n        self._backward_hooks = {}\n        self._backward_pre_hooks = {}\n        self._forward_hooks = {}\n        self._forward_pre_hooks = {}\n        self._state_dict_hooks = {}\n        self._load_state_dict_pre_hooks = {}\n        self.training = True\n        self.reduction = reduction\n\n    def forward(self, input_tensor, target_tensor):\n        if isinstance(input_tensor, WebGPUTensor) and isinstance(target_tensor, WebGPUTensor):\n            diff = input_tensor._data - target_tensor._data\n            squared_error = diff ** 2\n\n            if self.reduction == \'mean\':\n                loss_value = np.mean(squared_error)\n            elif self.reduction == \'sum\':\n                loss_value = np.sum(squared_error)\n            else:  # \'none\'\n                loss_value = squared_error\n\n            loss_tensor = WebGPUTensor([loss_value] if np.isscalar(loss_value) else loss_value,\n                                     device="webgpu", dtype=input_tensor.dtype, requires_grad=True)\n\n            # Set up backward function for MSE loss\n            def mse_backward(grad_output):\n                # MSE gradient: 2 * (input - target) / N\n                N = input_tensor._data.size if self.reduction == \'mean\' else 1\n                grad_input = 2.0 * diff / N\n\n                if input_tensor.requires_grad:\n                    # Create gradient tensor for the input\n                    grad_input_tensor = WebGPUTensor(grad_input * (grad_output._data if hasattr(grad_output, \'_data\') else grad_output),\n                                                   device="webgpu", dtype=input_tensor.dtype)\n\n                    # Call backward on the input tensor (linear layer output) to propagate gradients\n                    input_tensor.backward(grad_input_tensor)\n\n            loss_tensor._backward_fn = mse_backward\n            loss_tensor._inputs = [input_tensor, target_tensor]\n\n            return loss_tensor\n        else:\n            raise TypeError("Both input and target must be WebGPUTensor")\n\nclass TorchNNCrossEntropyLoss(TorchNNModule):\n    def __init__(self, reduction=\'mean\'):\n        self._parameters = {}\n        self._modules = {}\n        self._buffers = {}\n        self._non_persistent_buffers_set = set()\n        self._backward_hooks = {}\n        self._backward_pre_hooks = {}\n        self._forward_hooks = {}\n        self._forward_pre_hooks = {}\n        self._state_dict_hooks = {}\n        self._load_state_dict_pre_hooks = {}\n        self.training = True\n        self.reduction = reduction\n\n    def forward(self, input_tensor, target_tensor):\n        if isinstance(input_tensor, WebGPUTensor) and isinstance(target_tensor, WebGPUTensor):\n            # Softmax\n            input_data = input_tensor._data\n            exp_data = np.exp(input_data - np.max(input_data, axis=1, keepdims=True))\n            softmax_data = exp_data / np.sum(exp_data, axis=1, keepdims=True)\n\n            # Cross-entropy loss\n            target_indices = target_tensor._data.astype(int)\n            batch_size = input_data.shape[0]\n\n            # Extract probabilities for target classes\n            target_probs = softmax_data[np.arange(batch_size), target_indices]\n\n            # Compute negative log likelihood\n            nll = -np.log(np.clip(target_probs, 1e-8, 1.0))\n\n            if self.reduction == \'mean\':\n                loss_value = np.mean(nll)\n            elif self.reduction == \'sum\':\n                loss_value = np.sum(nll)\n            else:  # \'none\'\n                loss_value = nll\n\n            loss_tensor = WebGPUTensor([loss_value] if np.isscalar(loss_value) else loss_value,\n                                     device="webgpu", dtype=input_tensor.dtype, requires_grad=True)\n\n            # Set up backward function for CrossEntropyLoss\n            def ce_backward(grad_output):\n                if input_tensor.requires_grad:\n                    # Gradient: softmax - one_hot(target)\n                    grad_input = softmax_data.copy()\n                    grad_input[np.arange(batch_size), target_indices] -= 1.0\n\n                    if self.reduction == \'mean\':\n                        grad_input = grad_input / batch_size\n\n                    grad_input_tensor = WebGPUTensor(grad_input * (grad_output._data if hasattr(grad_output, \'_data\') else grad_output),\n                                                   device="webgpu", dtype=input_tensor.dtype)\n\n                    # Call backward on the input tensor to propagate gradients\n                    input_tensor.backward(grad_input_tensor)\n\n            loss_tensor._backward_fn = ce_backward\n            loss_tensor._inputs = [input_tensor, target_tensor]\n\n            return loss_tensor\n        else:\n            raise TypeError("Both input and target must be WebGPUTensor")\n\n# Create torch module with essential functions\nclass TorchModule:\n    def __init__(self):\n        self.tensor = self._tensor\n        self.zeros = self._zeros\n        self.ones = self._ones\n        self.randn = self._randn\n        self.zeros_like = self._zeros_like\n        self.matmul = self._matmul\n        self.sum = self._sum\n        self.as_tensor = self._as_tensor\n        self.arange = self._arange\n        self.randperm = self._randperm\n        self.linspace = self._linspace\n        self.nn = TorchNN()\n\n        # Add Tensor class reference\n        self.Tensor = WebGPUTensor\n\n        # Linear algebra module\n        self.linalg = TorchLinalg()\n\n        # Data utilities module\n        self.utils = TorchUtils()\n\n        # Optimizer module\n        self.optim = TorchOptim()\n\n        # Activation functions\n        self.relu = self._relu\n\n        # Mathematical functions\n        self.round = self.round\n        self.sqrt = self._sqrt\n        self.pow = self._pow\n        self.exp = self._exp\n        self.log = self._log\n        self.mean = self._mean\n        self.max = self._max\n        self.min = self._min\n        self.transpose = self._transpose\n\n        # Advanced mathematical functions\n        self.cat = self._cat\n        self.stack = self._stack\n        self.std = self._std\n        self.abs = self._abs\n        self.sin = self._sin\n        self.cos = self._cos\n        self.clamp = self._clamp\n        self.argmax = self._argmax\n\n        # Context managers\n        self.no_grad = self._no_grad\n\n        # Device and CUDA support\n        self.device = self._device\n        self.cuda = TorchCuda()\n        self.manual_seed = self._manual_seed\n\n        # Data types\n        self.float32 = \'float32\'\n        self.float64 = \'float64\'\n        self.double = \'float64\'\n        self.float = \'float32\'\n        self.int32 = \'int32\'\n        self.int64 = \'int64\'\n        self.long = \'int64\'\n        self.int = \'int32\'\n        self.bool = \'bool\'\n        self.uint8 = \'uint8\'\n        \n        # Device types\n        # self.device = self._device  # DISABLED: Potential recursion source\n        \n    def _tensor(self, data, **kwargs):\n        # Enable WebGPU detection by default for tensor creation\n        if \'device\' not in kwargs:\n            kwargs[\'device\'] = \'webgpu\'  # Default to WebGPU instead of CPU\n        return WebGPUTensor(data, **kwargs)\n    \n    def _zeros(self, *shape, **kwargs):\n        data = np.zeros(shape)\n        return WebGPUTensor(data, **kwargs)\n    \n    def _ones(self, *shape, **kwargs):\n        data = np.ones(shape)\n        return WebGPUTensor(data, **kwargs)\n    \n    def _randn(self, *shape, **kwargs):\n        data = np.random.randn(*shape)\n        return WebGPUTensor(data, **kwargs)\n    \n    def _matmul(self, a, b):\n        if isinstance(a, WebGPUTensor) and isinstance(b, WebGPUTensor):\n            return WebGPUTensor(np.dot(a.data, b.data), device="webgpu")\n        return WebGPUTensor(np.dot(a, b))\n    \n    def _device(self, device_type):\n        """Create a device object"""\n        return WebGPUDevice(device_type)\n    \n    def _sum(self, input_tensor, dim=None, keepdim=False, dtype=None):\n        """Compute sum of tensor elements"""\n        if isinstance(input_tensor, WebGPUTensor):\n            return input_tensor.sum(dim=dim, keepdim=keepdim)\n        else:\n            # Handle numpy arrays or lists\n            if dim is None:\n                result_data = np.sum(input_tensor)\n            else:\n                result_data = np.sum(input_tensor, axis=dim, keepdims=keepdim)\n            return WebGPUTensor(result_data, dtype=dtype or \'float32\')\n    \n    def _as_tensor(self, data, dtype=None, device=None):\n        """Convert data to tensor, similar to torch.as_tensor"""\n        # Determine dtype\n        if dtype is None:\n            if hasattr(data, \'dtype\'):\n                dtype = str(data.dtype)\n            else:\n                dtype = \'float32\'\n        \n        # Determine device - default to WebGPU for better performance\n        if device is None:\n            device = \'webgpu\'\n        \n        # Create tensor\n        return WebGPUTensor(data, dtype=dtype, device=device)\n    \n    def eye(self, n, m=None, dtype=\'float32\', device=\'webgpu\'):\n        """Create identity matrix"""\n        if m is None:\n            m = n\n        data = np.eye(n, m)\n        return WebGPUTensor(data, device=device, dtype=dtype)\n    \n    def round(self, input_tensor, decimals=0):\n        """Round tensor elements to given number of decimals"""\n        if isinstance(input_tensor, WebGPUTensor):\n            rounded_data = np.round(input_tensor._data, decimals=decimals)\n            return WebGPUTensor(rounded_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)\n        else:\n            return WebGPUTensor(np.round(input_tensor, decimals=decimals))\n    \n    def det(self, input_tensor):\n        """Compute determinant of square matrix"""\n        if isinstance(input_tensor, WebGPUTensor):\n            if input_tensor.ndim != 2 or input_tensor.shape[0] != input_tensor.shape[1]:\n                raise RuntimeError("det() expects a 2D square tensor")\n            det_value = np.linalg.det(input_tensor._data.reshape(input_tensor.shape))\n            return WebGPUTensor([det_value], device="webgpu", dtype=input_tensor.dtype)\n        else:\n            return np.linalg.det(input_tensor)\n    \n    def _arange(self, *args, **kwargs):\n        """Create a 1D tensor with evenly spaced values"""\n        if len(args) == 1:\n            # arange(end)\n            start, end, step = 0, args[0], 1\n        elif len(args) == 2:\n            # arange(start, end)\n            start, end, step = args[0], args[1], 1\n        elif len(args) == 3:\n            # arange(start, end, step)\n            start, end, step = args[0], args[1], args[2]\n        else:\n            raise ValueError("arange() takes 1 to 3 positional arguments")\n        \n        data = np.arange(start, end, step)\n        device = kwargs.get(\'device\', \'cpu\')\n        dtype = kwargs.get(\'dtype\', \'int64\' if isinstance(start, int) and isinstance(end, int) and isinstance(step, int) else \'float32\')\n        return WebGPUTensor(data, device=device, dtype=dtype)\n    \n    def _randperm(self, n, **kwargs):\n        """Generate a random permutation of integers from 0 to n-1"""\n        data = np.random.permutation(n)\n        device = kwargs.get(\'device\', \'cpu\')\n        dtype = kwargs.get(\'dtype\', \'int64\')\n        return WebGPUTensor(data, device=device, dtype=dtype)\n\n    def _linspace(self, start, end, steps, **kwargs):\n        """Generate linearly spaced values"""\n        data = np.linspace(start, end, steps)\n        device = kwargs.get(\'device\', \'cpu\')\n        dtype = kwargs.get(\'dtype\', \'float32\')\n        return WebGPUTensor(data, device=device, dtype=dtype)\n\n    def _zeros_like(self, input_tensor, **kwargs):\n        """Create a tensor of zeros with the same shape as input"""\n        if isinstance(input_tensor, WebGPUTensor):\n            data = np.zeros_like(input_tensor._data)\n            device = kwargs.get(\'device\', input_tensor.device)\n            dtype = kwargs.get(\'dtype\', input_tensor.dtype)\n            return WebGPUTensor(data, device=device, dtype=dtype)\n        else:\n            data = np.zeros_like(input_tensor)\n            device = kwargs.get(\'device\', \'cpu\')\n            dtype = kwargs.get(\'dtype\', \'float32\')\n            return WebGPUTensor(data, device=device, dtype=dtype)\n\n    def _relu(self, input_tensor):\n        """ReLU activation function"""\n        if isinstance(input_tensor, WebGPUTensor):\n            result_data = np.maximum(input_tensor._data, 0)\n            result = WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)\n            \n            if input_tensor.requires_grad:\n                def relu_backward(grad_output):\n                    if input_tensor.grad is None:\n                        input_tensor.grad = WebGPUTensor(np.zeros_like(input_tensor._data), device="webgpu", dtype=input_tensor.dtype)\n                    relu_grad = grad_output._data * (input_tensor._data > 0).astype(input_tensor.dtype)\n                    input_tensor.grad._data += relu_grad\n                \n                result._backward_fn = relu_backward\n                result._inputs = [input_tensor]\n            \n            return result\n        else:\n            return np.maximum(input_tensor, 0)\n\n    def _sqrt(self, input_tensor):\n        """Square root function"""\n        if isinstance(input_tensor, WebGPUTensor):\n            result_data = np.sqrt(input_tensor._data)\n            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)\n        else:\n            return np.sqrt(input_tensor)\n\n    def _pow(self, input_tensor, exponent):\n        """Power function"""\n        if isinstance(input_tensor, WebGPUTensor):\n            result_data = np.power(input_tensor._data, exponent)\n            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)\n        else:\n            return np.power(input_tensor, exponent)\n\n    def _exp(self, input_tensor):\n        """Exponential function"""\n        if isinstance(input_tensor, WebGPUTensor):\n            result_data = np.exp(input_tensor._data)\n            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)\n        else:\n            return np.exp(input_tensor)\n\n    def _log(self, input_tensor):\n        """Natural logarithm function"""\n        if isinstance(input_tensor, WebGPUTensor):\n            result_data = np.log(input_tensor._data)\n            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)\n        else:\n            return np.log(input_tensor)\n\n    def _mean(self, input_tensor, dim=None, keepdim=False):\n        """Mean function"""\n        if isinstance(input_tensor, WebGPUTensor):\n            if dim is None:\n                result_data = np.mean(input_tensor._data)\n                return WebGPUTensor([result_data], device="webgpu", dtype=input_tensor.dtype)\n            else:\n                result_data = np.mean(input_tensor._data, axis=dim, keepdims=keepdim)\n                return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype)\n        else:\n            return np.mean(input_tensor, axis=dim, keepdims=keepdim)\n\n    def _max(self, input_tensor, dim=None, keepdim=False):\n        """Maximum function - returns values and indices when dim is specified"""\n        if isinstance(input_tensor, WebGPUTensor):\n            if dim is None:\n                # Global max - return single value\n                result_data = np.max(input_tensor._data)\n                return WebGPUTensor([result_data], device="webgpu", dtype=input_tensor.dtype)\n            else:\n                # Max along dimension - return (values, indices) tuple like PyTorch\n                max_values = np.max(input_tensor._data, axis=dim, keepdims=keepdim)\n                max_indices = np.argmax(input_tensor._data, axis=dim)\n                if keepdim:\n                    max_indices = np.expand_dims(max_indices, axis=dim)\n\n\n                values_tensor = WebGPUTensor(max_values, device="webgpu", dtype=input_tensor.dtype)\n                indices_tensor = WebGPUTensor(max_indices, device="webgpu", dtype=\'int64\')\n                return values_tensor, indices_tensor\n        else:\n            if dim is None:\n                return np.max(input_tensor)\n            else:\n                max_values = np.max(input_tensor, axis=dim, keepdims=keepdim)\n                max_indices = np.argmax(input_tensor, axis=dim)\n                if keepdim:\n                    max_indices = np.expand_dims(max_indices, axis=dim)\n                return max_values, max_indices\n\n    def _min(self, input_tensor, dim=None, keepdim=False):\n        """Minimum function - returns values and indices when dim is specified"""\n        if isinstance(input_tensor, WebGPUTensor):\n            if dim is None:\n                # Global min - return single value\n                result_data = np.min(input_tensor._data)\n                return WebGPUTensor([result_data], device="webgpu", dtype=input_tensor.dtype)\n            else:\n                # Min along dimension - return (values, indices) tuple like PyTorch\n                min_values = np.min(input_tensor._data, axis=dim, keepdims=keepdim)\n                min_indices = np.argmin(input_tensor._data, axis=dim)\n                if keepdim:\n                    min_indices = np.expand_dims(min_indices, axis=dim)\n\n\n                values_tensor = WebGPUTensor(min_values, device="webgpu", dtype=input_tensor.dtype)\n                indices_tensor = WebGPUTensor(min_indices, device="webgpu", dtype=\'int64\')\n                return values_tensor, indices_tensor\n        else:\n            if dim is None:\n                return np.min(input_tensor)\n            else:\n                min_values = np.min(input_tensor, axis=dim, keepdims=keepdim)\n                min_indices = np.argmin(input_tensor, axis=dim)\n                if keepdim:\n                    min_indices = np.expand_dims(min_indices, axis=dim)\n                return min_values, min_indices\n\n    def _transpose(self, input_tensor, dim0, dim1):\n        """Transpose function"""\n        if isinstance(input_tensor, WebGPUTensor):\n            result_data = np.swapaxes(input_tensor._data, dim0, dim1)\n            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)\n        else:\n            return np.swapaxes(input_tensor, dim0, dim1)\n\n    def _cat(self, tensors, dim=0):\n        """Concatenate tensors along specified dimension"""\n        if not isinstance(tensors, (list, tuple)):\n            raise TypeError("tensors must be a list or tuple")\n\n        if len(tensors) == 0:\n            raise RuntimeError("cat expects a non-empty list of tensors")\n\n        # Convert all tensors to WebGPUTensor if needed\n        tensor_list = []\n        for tensor in tensors:\n            if isinstance(tensor, WebGPUTensor):\n                tensor_list.append(tensor.data)\n            else:\n                tensor_list.append(tensor)\n\n        # Use numpy concatenate\n        result_data = np.concatenate(tensor_list, axis=dim)\n        return WebGPUTensor(result_data, device="webgpu", dtype=tensors[0].dtype)\n\n    def _stack(self, tensors, dim=0):\n        """Stack tensors along a new dimension"""\n        if not isinstance(tensors, (list, tuple)):\n            raise TypeError("tensors must be a list or tuple")\n\n        if len(tensors) == 0:\n            raise RuntimeError("stack expects a non-empty list of tensors")\n\n        # Convert all tensors to WebGPUTensor if needed\n        tensor_list = []\n        for tensor in tensors:\n            if isinstance(tensor, WebGPUTensor):\n                tensor_list.append(tensor.data)\n            else:\n                tensor_list.append(tensor)\n\n        # Use numpy stack\n        result_data = np.stack(tensor_list, axis=dim)\n        return WebGPUTensor(result_data, device="webgpu", dtype=tensors[0].dtype)\n\n    def _std(self, input_tensor, dim=None, keepdim=False):\n        """Standard deviation function"""\n        if isinstance(input_tensor, WebGPUTensor):\n            if dim is None:\n                result_data = np.std(input_tensor._data)\n                return WebGPUTensor([result_data], device="webgpu", dtype=input_tensor.dtype)\n            else:\n                result_data = np.std(input_tensor._data, axis=dim, keepdims=keepdim)\n                return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype)\n        else:\n            return np.std(input_tensor, axis=dim, keepdims=keepdim)\n\n    def _abs(self, input_tensor):\n        """Absolute value function"""\n        if isinstance(input_tensor, WebGPUTensor):\n            result_data = np.abs(input_tensor._data)\n            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)\n        else:\n            return np.abs(input_tensor)\n\n    def _sin(self, input_tensor):\n        """Sine function"""\n        if isinstance(input_tensor, WebGPUTensor):\n            result_data = np.sin(input_tensor._data)\n            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)\n        else:\n            return np.sin(input_tensor)\n\n    def _cos(self, input_tensor):\n        """Cosine function"""\n        if isinstance(input_tensor, WebGPUTensor):\n            result_data = np.cos(input_tensor._data)\n            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)\n        else:\n            return np.cos(input_tensor)\n\n    def _clamp(self, input_tensor, min=None, max=None):\n        """Clamp function - constrain values to a range"""\n        if isinstance(input_tensor, WebGPUTensor):\n            result_data = np.clip(input_tensor._data, min, max)\n            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)\n        else:\n            return np.clip(input_tensor, min, max)\n\n    def _argmax(self, input_tensor, dim=None, keepdim=False):\n        """Argmax function - indices of maximum values"""\n        if isinstance(input_tensor, WebGPUTensor):\n            if dim is None:\n                result_data = np.argmax(input_tensor._data)\n                return WebGPUTensor([result_data], device="webgpu", dtype=\'int64\')\n            else:\n                result_data = np.argmax(input_tensor._data, axis=dim)\n                if keepdim:\n                    result_data = np.expand_dims(result_data, axis=dim)\n                return WebGPUTensor(result_data, device="webgpu", dtype=\'int64\')\n        else:\n            return np.argmax(input_tensor, axis=dim)\n\n    def _no_grad(self):\n        """No gradient context manager - simplified implementation"""\n        class NoGradContext:\n            def __enter__(self):\n                return self\n\n            def __exit__(self, exc_type, exc_val, exc_tb):\n                return False\n\n        return NoGradContext()\n\n    def _device(self, device_name):\n        """Create a device object"""\n        if device_name == \'cuda\':\n            device_name = \'webgpu\'  # Map CUDA to WebGPU\n        return WebGPUDevice(device_name)\n\n    def _manual_seed(self, seed):\n        """Set random seed for reproducibility"""\n        np.random.seed(seed)\n        return seed\n\nclass TorchNNL1Loss(TorchNNModule):\n    def __init__(self, reduction=\'mean\'):\n        self._parameters = {}\n        self._modules = {}\n        self._buffers = {}\n        self._non_persistent_buffers_set = set()\n        self._backward_hooks = {}\n        self._backward_pre_hooks = {}\n        self._forward_hooks = {}\n        self._forward_pre_hooks = {}\n        self._state_dict_hooks = {}\n        self._load_state_dict_pre_hooks = {}\n        self.training = True\n        self.reduction = reduction\n\n    def forward(self, input_tensor, target_tensor):\n        if isinstance(input_tensor, WebGPUTensor) and isinstance(target_tensor, WebGPUTensor):\n            diff = np.abs(input_tensor._data - target_tensor._data)\n\n            if self.reduction == \'mean\':\n                loss_value = np.mean(diff)\n            elif self.reduction == \'sum\':\n                loss_value = np.sum(diff)\n            else:  # \'none\'\n                loss_value = diff\n\n            return WebGPUTensor([loss_value] if np.isscalar(loss_value) else loss_value,\n                             device="webgpu", dtype=input_tensor.dtype)\n        else:\n            raise TypeError("Both input and target must be WebGPUTensor")\n\n\n# Data utilities for torch.utils.data\nclass TensorDataset:\n    def __init__(self, *tensors):\n        if not tensors:\n            raise ValueError("At least one tensor should be given")\n        self.tensors = tensors\n\n    def __len__(self):\n        return len(self.tensors[0])\n\n    def __getitem__(self, index):\n        if isinstance(index, slice):\n            return tuple(tensor.data[index] for tensor in self.tensors)\n        return tuple(WebGPUTensor(tensor.data[index]) for tensor in self.tensors)\n\nclass DataLoader:\n    def __init__(self, dataset, batch_size=1, shuffle=False, **kwargs):\n        self._dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self._current_index = 0\n\n    def __iter__(self):\n        self._current_index = 0\n        self.indices = list(range(len(self._dataset)))\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n        return self\n\n    def __next__(self):\n        if self._current_index >= len(self._dataset):\n            raise StopIteration\n\n        batch_indices = self.indices[self._current_index:self._current_index + self.batch_size]\n        batch = []\n\n        # Get first item to determine structure\n        first_item = self._dataset[batch_indices[0]]\n        num_outputs = len(first_item) if isinstance(first_item, tuple) else 1\n\n        # Initialize batch lists\n        if num_outputs > 1:\n            batch = [[] for _ in range(num_outputs)]\n            for idx in batch_indices:\n                items = self._dataset[idx]\n                for i, item in enumerate(items):\n                    batch[i].append(item.data if hasattr(item, \'data\') else item)\n\n            # Convert to tensors\n            result = tuple(WebGPUTensor(np.array(batch_data)) for batch_data in batch)\n        else:\n            batch_data = []\n            for idx in batch_indices:\n                item = self._dataset[idx]\n                batch_data.append(item.data if hasattr(item, \'data\') else item)\n            result = WebGPUTensor(np.array(batch_data))\n\n        self._current_index += self.batch_size\n        return result\n\nclass TorchUtilsData:\n    def __init__(self):\n        self.TensorDataset = TensorDataset\n        self.DataLoader = DataLoader\n\nclass TorchUtils:\n    def __init__(self):\n        self._data = TorchUtilsData()\n\n    @property\n    def data(self):\n        """Return the data module"""\n        return self._data\n\n# SGD Optimizer\nclass SGDOptimizer:\n    def __init__(self, params, lr=0.01, momentum=0, dampening=0, weight_decay=0, nesterov=False):\n        for i, param in enumerate(params):\n            pass  # Parameter validation could go here\n        self.param_groups = [{\'params\': params, \'lr\': lr, \'momentum\': momentum,\n                             \'dampening\': dampening, \'weight_decay\': weight_decay, \'nesterov\': nesterov}]\n        self.state = {}\n\n    def zero_grad(self):\n        """Clear gradients of all parameters"""\n        for group in self.param_groups:\n            for param in group[\'params\']:\n                if hasattr(param, \'grad\') and param.grad is not None:\n                    param.grad._data = np.zeros_like(param.grad._data)\n\n    def step(self):\n        """Perform one optimization step"""\n        for group in self.param_groups:\n            lr = group[\'lr\']\n            momentum = group[\'momentum\']\n            weight_decay = group[\'weight_decay\']\n\n            for param in group[\'params\']:\n                if not hasattr(param, \'grad\') or param.grad is None:\n                    continue\n\n                grad = param.grad._data\n\n                # Add weight decay\n                if weight_decay != 0:\n                    grad = grad + weight_decay * param._data\n\n                # Apply momentum\n                if momentum != 0:\n                    param_state = self.state.setdefault(id(param), {})\n                    if \'momentum_buffer\' not in param_state:\n                        param_state[\'momentum_buffer\'] = np.zeros_like(param._data)\n                    buf = param_state[\'momentum_buffer\']\n                    buf = momentum * buf + grad\n                    grad = buf\n\n                # Update parameters\n                old_data = param._data.copy()\n                np.copyto(param._data, param._data - lr * grad)\n\nclass TorchOptim:\n    def __init__(self):\n        self.SGD = SGDOptimizer\n\n# CUDA support for WebGPU acceleration\nclass TorchCuda:\n    def __init__(self):\n        pass\n\n    def is_available(self):\n        """Check if WebGPU acceleration is available (maps CUDA to WebGPU)"""\n        # In browser, we use WebGPU instead of CUDA\n        try:\n            # Use JavaScript to check if WebGPU is actually available\n            import js\n            if hasattr(js, \'navigator\') and hasattr(js.navigator, \'gpu\'):\n                return js.navigator.gpu is not None\n            else:\n                return False\n        except:\n            return False\n\nclass TorchNN:\n    def __init__(self):\n        self.functional = TorchNNFunctional()\n        self.Linear = TorchNNLinear\n        self.Module = TorchNNModule\n        self.ReLU = TorchNNReLU\n        self.MSELoss = TorchNNMSELoss\n        self.L1Loss = TorchNNL1Loss\n        self.CrossEntropyLoss = TorchNNCrossEntropyLoss\n\n# Global cleanup function to prevent state persistence issues\ndef _cleanup_global_state():\n    """Clean up global tensor state to prevent persistence issues between executions"""\n    import gc\n    # Force garbage collection to clean up circular references\n    gc.collect()\n\nclass TensorDataWrapper:\n    """Wrapper for tensor data that provides PyTorch-like methods"""\n    def __init__(self, numpy_array, parent_tensor=None):\n        self._array = numpy_array\n        self._parent = parent_tensor\n\n    def cpu(self):\n        """Return tensor data on CPU"""\n        return TensorDataWrapper(self._array.copy(), self._parent)\n\n    def numpy(self):\n        """Return the underlying numpy array"""\n        return self._array\n\n    def __getitem__(self, key):\n        """Support indexing operations"""\n        result = self._array[key]\n        return TensorDataWrapper(result, self._parent)\n\n    def __setitem__(self, key, value):\n        """Support item assignment"""\n        if isinstance(value, TensorDataWrapper):\n            self._array[key] = value._array\n        else:\n            self._array[key] = value\n\n    def __len__(self):\n        """Return length of the array"""\n        return len(self._array)\n\n    def __iter__(self):\n        """Support iteration"""\n        for item in self._array:\n            yield TensorDataWrapper(item, self._parent) if hasattr(item, \'shape\') else item\n\n    # Arithmetic operators\n    def __add__(self, other):\n        """Addition operator"""\n        if isinstance(other, TensorDataWrapper):\n            result = self._array + other._array\n        else:\n            result = self._array + other\n        return TensorDataWrapper(result, self._parent)\n\n    def __sub__(self, other):\n        """Subtraction operator"""\n        if isinstance(other, TensorDataWrapper):\n            result = self._array - other._array\n        else:\n            result = self._array - other\n        return TensorDataWrapper(result, self._parent)\n\n    def __mul__(self, other):\n        """Multiplication operator"""\n        if isinstance(other, TensorDataWrapper):\n            result = self._array * other._array\n        else:\n            result = self._array * other\n        return TensorDataWrapper(result, self._parent)\n\n    def __truediv__(self, other):\n        """Division operator"""\n        if isinstance(other, TensorDataWrapper):\n            result = self._array / other._array\n        else:\n            result = self._array / other\n        return TensorDataWrapper(result, self._parent)\n\n    def __pow__(self, other):\n        """Power operator"""\n        if isinstance(other, TensorDataWrapper):\n            result = self._array ** other._array\n        else:\n            result = self._array ** other\n        return TensorDataWrapper(result, self._parent)\n\n    # Reverse arithmetic operators\n    def __radd__(self, other):\n        """Reverse addition"""\n        result = other + self._array\n        return TensorDataWrapper(result, self._parent)\n\n    def __rsub__(self, other):\n        """Reverse subtraction"""\n        result = other - self._array\n        return TensorDataWrapper(result, self._parent)\n\n    def __rmul__(self, other):\n        """Reverse multiplication"""\n        result = other * self._array\n        return TensorDataWrapper(result, self._parent)\n\n    def __rtruediv__(self, other):\n        """Reverse division"""\n        result = other / self._array\n        return TensorDataWrapper(result, self._parent)\n\n    def __getattr__(self, name):\n        """Delegate unknown attributes to the underlying numpy array"""\n        return getattr(self._array, name)\n\n    def __array__(self):\n        """Support numpy array interface"""\n        return self._array\n\n    def __repr__(self):\n        return repr(self._array)\n\n# Install in global namespace\ntorch = TorchModule()\nsys.modules[\'torch\'] = torch\nsys.modules[\'torch.nn\'] = torch.nn\nsys.modules[\'torch.nn.functional\'] = torch.nn.functional\nsys.modules[\'torch.linalg\'] = torch.linalg\nsys.modules[\'torch.utils\'] = torch.utils\nsys.modules[\'torch.utils.data\'] = torch.utils.data\nsys.modules[\'torch.optim\'] = torch.optim\n\n# Clean up any existing state to prevent issues\n_cleanup_global_state()\n'}function r(e){if(!e||"string"!=typeof e)throw new Error("Invalid polyfill code provided");if(e.length>1e6)throw new Error("Polyfill code too large");return!0}t.d(n,{EH:()=>r,pH:()=>s})}}]);