/**
 * PyTorch Runtime Polyfill - Extracted from main thread for better performance
 * Provides PyTorch-compatible API with WebGPU acceleration support
 */

/**
 * Initialize PyTorch polyfill in Python runtime
 */
export function createPyTorchPolyfill() {
  return `
# WebGPU-enabled PyTorch polyfill setup
import numpy as np
import sys

# Global gradient tracking state
_grad_enabled = True

def is_grad_enabled():
    """Check if gradient computation is currently enabled"""
    global _grad_enabled
    return _grad_enabled

def set_grad_enabled(mode):
    """Enable or disable gradient computation globally"""
    global _grad_enabled
    prev = _grad_enabled
    _grad_enabled = mode
    return prev

class WebGPUDevice:
    def __init__(self, device_type, **kwargs):
        self.type = device_type
        
    def __str__(self):
        return self.type
        
    def __repr__(self):
        return f"device(type='{self.type}')"

class WebGPUTensor:
    def __init__(self, data, device='cpu', dtype='float32', requires_grad=False, _force_webgpu=False, _internal=False, **kwargs):
        if isinstance(data, (list, tuple)):
            self._data = np.array(data, dtype=dtype)
        elif isinstance(data, np.ndarray):
            self._data = data.astype(dtype)
        else:
            self._data = np.array(data, dtype=dtype)

        # Determine actual device based on tensor size and WebGPU availability
        self._original_device = device
        self._force_webgpu = _force_webgpu

        # WebGPU auto-detection with recursion prevention
        # Only auto-detect for user-facing tensor creation (not internal operations)
        if device == 'webgpu' or _internal:
            # Explicitly requested webgpu or internal operation - use as-is
            self.device = device if isinstance(device, str) else device
        elif _force_webgpu or (device in ['cuda', 'gpu']):
            # Map CUDA/GPU requests to WebGPU
            self.device = 'webgpu'
        elif device == 'cpu':
            # Explicitly requested CPU - respect that
            self.device = device
        else:
            # Auto-detect for user-facing tensor creation
            if self._should_use_webgpu(self._data):
                self.device = 'webgpu'
            else:
                self.device = device if isinstance(device, str) else device

        self.dtype = dtype
        # Only enable gradient tracking if globally enabled and explicitly requested
        self.requires_grad = requires_grad and is_grad_enabled()
        self.shape = self._data.shape
        self.ndim = self._data.ndim
        self.grad = None
        self.grad_fn = None

        # GPU ACCELERATION: Allocate GPU buffer if on webgpu device
        self._gpu_buffer_id = None
        device_str = str(self.device)

        if device_str == 'webgpu' and '__webgpu_allocate__' in globals() and not _internal:
            try:
                # Allocate buffer on GPU for faster operations
                self._gpu_buffer_id = __webgpu_allocate__(
                    self._data.tolist(),  # Convert numpy to Python list for JS
                    list(self.shape),
                    self.dtype
                )
            except Exception as e:
                # Fallback to CPU if GPU allocation fails
                pass

    @property
    def data(self):
        """Return data wrapped with PyTorch-like methods"""
        return TensorDataWrapper(self._data, self)

    @property
    def is_cpu(self):
        """Check if tensor is on CPU"""
        return self.device == 'cpu'

    @property
    def is_cuda(self):
        """Check if tensor is on CUDA (WebGPU in our case)"""
        return self.device in ['cuda', 'webgpu', 'gpu']

    @property
    def T(self):
        """Transpose property - returns transposed view for 2D tensors"""
        if self.ndim == 2:
            return self.transpose(0, 1)
        elif self.ndim == 1:
            # For 1D tensors, T returns the tensor unchanged (like PyTorch)
            return self
        else:
            raise RuntimeError(f"T property expects a 1D or 2D tensor, but got {self.ndim}D")

    def is_contiguous(self):
        """Check if tensor is contiguous in memory.
        In our simplified implementation, tensors are always contiguous."""
        return True

    def contiguous(self):
        """Return a contiguous tensor.
        In our implementation, tensors are always contiguous, so return self."""
        return self

    def t(self):
        """Transpose 2D tensor (shorthand for transpose(0, 1))"""
        if self.ndim != 2:
            raise RuntimeError(f"t() expects a 2D tensor, but got {self.ndim}D")
        return self.transpose(0, 1)

    def size(self, dim=None):
        """Return the size of the tensor or a specific dimension"""
        if dim is None:
            return self.shape
        else:
            if dim < 0:
                dim = self.ndim + dim
            if dim >= self.ndim or dim < 0:
                raise IndexError(f"Dimension out of range (expected to be in range of [{-self.ndim}, {self.ndim-1}], but got {dim})")
            return self.shape[dim]

    def numel(self):
        """Return the total number of elements in the tensor"""
        return self._data.size

    def dim(self):
        """Return the number of dimensions of the tensor (method call)"""
        return self.ndim

    # For compatibility: some code might access .dim without calling it
    # We already have the dim() method above, but this helps with edge cases
    # In real PyTorch, dim is ONLY a method, never an attribute

    def _should_use_webgpu(self, data):
        \"\"\"Determine if WebGPU should be used based on tensor characteristics\"\"\"
        try:
            # Use WebGPU for tensors with more than 1000 elements for optimal performance
            # Smaller tensors are faster on CPU due to GPU overhead
            if hasattr(data, 'size'):
                return data.size >= 1000
            elif hasattr(data, '__len__'):
                # For nested structures, estimate total size
                total_size = 1
                def estimate_size(obj):
                    if hasattr(obj, '__len__'):
                        return len(obj) * estimate_size(obj[0] if len(obj) > 0 else 1)
                    return 1
                return estimate_size(data) >= 1000
            return False
        except:
            return False
        
    def _sync_from_gpu(self):
        """Sync GPU data to CPU when needed (lazy sync)"""
        if hasattr(self, '_gpu_only') and self._gpu_only and self._gpu_buffer_id is not None:
            try:
                if '__webgpu_read__' in globals():
                    # Read from GPU
                    result_data_flat = __webgpu_read__(self._gpu_buffer_id)
                    self._data = result_data_flat.reshape(self.shape)
                    self._gpu_only = False  # Data now synced
            except Exception as e:
                pass  # Sync from GPU failed

    def numpy(self):
        self._sync_from_gpu()  # Sync if GPU-only
        return self._data

    def tolist(self):
        self._sync_from_gpu()  # Sync if GPU-only
        return self._data.tolist()

    def __str__(self):
        """String representation of tensor"""
        self._sync_from_gpu()  # Sync if GPU-only
        result = f"tensor({self._data.tolist()}, requires_grad={self.requires_grad})"
        return result

    def __repr__(self):
        """Detailed representation of tensor"""
        return f"WebGPUTensor({self._data}, device='{self.device}', requires_grad={self.requires_grad}, _internal=True)"

    def item(self):
        """Return the value of this tensor as a standard Python number"""
        if self._data.size == 1:
            value = self._data.item()
            # Ensure we return proper Python types that can be used as indices
            if self.dtype in ['int32', 'int64', 'long']:
                return int(value)
            elif self.dtype in ['float32', 'float64', 'double']:
                return float(value)
            else:
                # For other types, try to convert appropriately
                if isinstance(value, (int, np.integer)):
                    return int(value)
                elif isinstance(value, (float, np.floating)):
                    return float(value)
                else:
                    return value
        else:
            raise ValueError("only one element tensors can be converted to Python scalars")

    def __format__(self, format_spec):
        """Support for f-string formatting"""
        if self._data.size == 1:
            return format(self._data.item(), format_spec)
        else:
            return format(str(self), format_spec)

    def view(self, *shape):
        """Reshape tensor maintaining data"""
        if len(shape) == 1 and isinstance(shape[0], (list, tuple)):
            shape = shape[0]
        
        # Handle -1 for automatic size calculation
        if -1 in shape:
            total_size = self._data.size
            known_size = 1
            unknown_idx = -1
            for i, s in enumerate(shape):
                if s == -1:
                    unknown_idx = i
                else:
                    known_size *= s
            if unknown_idx != -1:
                shape = list(shape)
                shape[unknown_idx] = total_size // known_size
                shape = tuple(shape)
        
        reshaped_data = self._data.reshape(shape)
        return WebGPUTensor(reshaped_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)
    
    def reshape(self, *shape):
        return self.view(*shape)
    
    def transpose(self, dim0, dim1):
        transposed_data = np.swapaxes(self._data, dim0, dim1)
        return WebGPUTensor(transposed_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def t(self):
        """Transpose a 2D tensor (shorthand for transpose(0, 1))"""
        if self.ndim != 2:
            raise RuntimeError(f"t() expects a 2D tensor, but got {self.ndim}D tensor")
        return self.transpose(0, 1)

    def unsqueeze(self, dim):
        """Add a dimension of size 1"""
        new_shape = list(self._data.shape)
        if dim < 0:
            dim = len(new_shape) + dim + 1
        new_shape.insert(dim, 1)
        reshaped_data = self._data.reshape(new_shape)
        return WebGPUTensor(reshaped_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def flatten(self, start_dim=0, end_dim=-1):
        """Flatten tensor dimensions"""
        if end_dim == -1:
            end_dim = self._data.ndim - 1

        shape = list(self._data.shape)
        flattened_size = 1
        for i in range(start_dim, end_dim + 1):
            flattened_size *= shape[i]

        new_shape = shape[:start_dim] + [flattened_size] + shape[end_dim + 1:]
        flattened_data = self._data.reshape(new_shape)
        return WebGPUTensor(flattened_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def squeeze(self, dim=None):
        """Remove dimensions of size 1"""
        if dim is None:
            # Remove all dimensions of size 1
            squeezed_data = np.squeeze(self._data)
        else:
            # Remove specific dimension if it has size 1
            if dim < 0:
                dim = self._data.ndim + dim
            if self._data.shape[dim] != 1:
                return self  # No change if dimension is not size 1
            squeezed_data = np.squeeze(self._data, axis=dim)

        return WebGPUTensor(squeezed_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def clone(self):
        """Create a copy of the tensor with gradient tracking preserved.

        The cloned tensor shares no storage with the original tensor but preserves
        requires_grad and creates a new node in the computation graph if applicable.
        """
        cloned_data = self._data.copy()
        cloned_tensor = WebGPUTensor(
            cloned_data,
            device=self.device,
            dtype=self.dtype,
            requires_grad=self.requires_grad
        )

        # If original tensor has gradient tracking, set up backward function for clone
        if self.requires_grad:
            def clone_backward(grad_output):
                # Gradient flows back to original tensor unchanged
                if self.grad is None:
                    self.grad = grad_output
                else:
                    self.grad._data += grad_output._data

            cloned_tensor._backward_fn = clone_backward
            cloned_tensor._inputs = [self]

        return cloned_tensor

    def detach(self):
        """Create a copy of the tensor that is detached from the computation graph.

        The detached tensor will never require gradient and breaks the gradient flow.
        Returns a new tensor with the same data but requires_grad=False.
        """
        detached_data = self._data.copy()
        detached_tensor = WebGPUTensor(
            detached_data,
            device=self.device,
            dtype=self.dtype,
            requires_grad=False  # Always False for detached tensors
        , _internal=True)
        return detached_tensor

    def sum(self, dim=None, keepdim=False):
        if dim is None:
            result_data = np.sum(self._data)
        else:
            result_data = np.sum(self._data, axis=dim, keepdims=keepdim)
        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)

        # Set up autograd for sum
        if result.requires_grad:
            result.grad_fn = 'SumBackward'
            result._inputs = [self]

            def sum_backward(grad):
                if self.requires_grad:
                    if self.grad is None:
                        self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype, _internal=True)
                    # Gradient of sum: broadcast the gradient back to input shape
                    if dim is None:
                        # Sum over all dimensions - broadcast gradient to all elements
                        self.grad._data += grad._data * np.ones_like(self._data)
                    else:
                        # Sum over specific dimension - broadcast along that dimension
                        grad_data = grad._data if hasattr(grad, '_data') else grad
                        if not keepdim:
                            # Need to add the dimension back for broadcasting
                            grad_data = np.expand_dims(grad_data, axis=dim)
                        self.grad._data += np.broadcast_to(grad_data, self._data.shape)

            result._backward_fn = sum_backward

        return result
    
    def mean(self, dim=None, keepdim=False):
        if dim is None:
            result_data = np.mean(self._data)
        else:
            result_data = np.mean(self._data, axis=dim, keepdims=keepdim)
        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)

        # Set up autograd for mean
        if result.requires_grad:
            result.grad_fn = 'MeanBackward'
            result._inputs = [self]

            def mean_backward(grad):
                if self.requires_grad:
                    if self.grad is None:
                        self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype, _internal=True)
                    # Gradient of mean: broadcast and divide by number of elements
                    if dim is None:
                        # Mean over all dimensions
                        n = self._data.size
                        self.grad._data += (grad._data / n) * np.ones_like(self._data)
                    else:
                        # Mean over specific dimension
                        grad_data = grad._data if hasattr(grad, '_data') else grad
                        if not keepdim:
                            grad_data = np.expand_dims(grad_data, axis=dim)
                        n = self._data.shape[dim]
                        self.grad._data += np.broadcast_to(grad_data / n, self._data.shape)

            result._backward_fn = mean_backward

        return result
    
    def std(self, dim=None, keepdim=False, unbiased=True):
        """Compute standard deviation"""
        if dim is None:
            result_data = np.std(self._data, ddof=1 if unbiased else 0)
        else:
            result_data = np.std(self._data, axis=dim, keepdims=keepdim, ddof=1 if unbiased else 0)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)
    
    def var(self, dim=None, keepdim=False, unbiased=True):
        """Compute variance"""
        if dim is None:
            result_data = np.var(self._data, ddof=1 if unbiased else 0)
        else:
            result_data = np.var(self._data, axis=dim, keepdims=keepdim, ddof=1 if unbiased else 0)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def tanh(self):
        """Hyperbolic tangent activation - tensor method"""
        result_data = np.tanh(self._data)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def abs(self):
        """Absolute value - tensor method"""
        result_data = np.abs(self._data)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def all(self):
        """Test if all elements are True"""
        result_data = np.all(self._data)
        return result_data

    def max(self, dim=None, keepdim=False):
        """Maximum values along a dimension"""
        if dim is None:
            result_data = np.max(self._data)
            return WebGPUTensor([result_data], device="webgpu", dtype=self.dtype, _internal=True)
        else:
            max_values = np.max(self._data, axis=dim, keepdims=keepdim)
            max_indices = np.argmax(self._data, axis=dim)
            if keepdim:
                max_indices = np.expand_dims(max_indices, axis=dim)
            values_tensor = WebGPUTensor(max_values, device="webgpu", dtype=self.dtype, _internal=True)
            indices_tensor = WebGPUTensor(max_indices, device="webgpu", dtype='int64', _internal=True)
            return values_tensor, indices_tensor

    def min(self, dim=None, keepdim=False):
        """Minimum values along a dimension"""
        if dim is None:
            result_data = np.min(self._data)
            return WebGPUTensor([result_data], device="webgpu", dtype=self.dtype, _internal=True)
        else:
            min_values = np.min(self._data, axis=dim, keepdims=keepdim)
            min_indices = np.argmin(self._data, axis=dim)
            if keepdim:
                min_indices = np.expand_dims(min_indices, axis=dim)
            values_tensor = WebGPUTensor(min_values, device="webgpu", dtype=self.dtype, _internal=True)
            indices_tensor = WebGPUTensor(min_indices, device="webgpu", dtype='int64', _internal=True)
            return values_tensor, indices_tensor

    def argmax(self, dim=None, keepdim=False):
        """Indices of maximum values along a dimension"""
        if dim is None:
            result_data = np.argmax(self._data)
            return WebGPUTensor([result_data], device="webgpu", dtype='int64', _internal=True)
        else:
            result_data = np.argmax(self._data, axis=dim)
            if keepdim:
                result_data = np.expand_dims(result_data, axis=dim)
            return WebGPUTensor(result_data, device="webgpu", dtype='int64', _internal=True)

    def argmin(self, dim=None, keepdim=False):
        """Indices of minimum values along a dimension"""
        if dim is None:
            result_data = np.argmin(self._data)
            return WebGPUTensor([result_data], device="webgpu", dtype='int64', _internal=True)
        else:
            result_data = np.argmin(self._data, axis=dim)
            if keepdim:
                result_data = np.expand_dims(result_data, axis=dim)
            return WebGPUTensor(result_data, device="webgpu", dtype='int64', _internal=True)

    def to(self, device):
        new_device = WebGPUDevice(device) if isinstance(device, str) else device
        # Don't use _internal=True to allow GPU buffer allocation when moving to GPU
        return WebGPUTensor(self._data.copy(), device=new_device, dtype=self.dtype, requires_grad=self.requires_grad)
    
    def cpu(self):
        return self.to('cpu')

    def cuda(self):
        return self.to('webgpu')  # Map CUDA to WebGPU

    def float(self):
        """Convert tensor to float32 dtype"""
        return WebGPUTensor(self._data.copy(), device=self.device, dtype='float32', requires_grad=self.requires_grad, _internal=True)

    def double(self):
        """Convert tensor to float64 dtype"""
        return WebGPUTensor(self._data.copy(), device=self.device, dtype='float64', requires_grad=self.requires_grad, _internal=True)

    def int(self):
        """Convert tensor to int32 dtype"""
        return WebGPUTensor(self._data.copy(), device=self.device, dtype='int32', requires_grad=self.requires_grad, _internal=True)

    def long(self):
        """Convert tensor to int64 dtype"""
        return WebGPUTensor(self._data.copy(), device=self.device, dtype='int64', requires_grad=self.requires_grad, _internal=True)

    def type_as(self, other):
        """Convert this tensor to the same dtype as other tensor"""
        if isinstance(other, WebGPUTensor):
            target_dtype = other.dtype
        else:
            # If other is not a tensor, assume it's float32
            target_dtype = 'float32'
        return WebGPUTensor(self._data.copy(), device=self.device, dtype=target_dtype, requires_grad=self.requires_grad, _internal=True)

    def __getitem__(self, key):
        """Support tensor slicing like X[:, 0] and advanced indexing"""
        # Handle advanced indexing with tensor indices
        if isinstance(key, tuple):
            # Convert WebGPUTensor indices to numpy arrays
            converted_key = []
            for k in key:
                if isinstance(k, WebGPUTensor):
                    # Convert tensor to numpy array for indexing
                    converted_key.append(k._data.astype(np.int64))
                else:
                    converted_key.append(k)
            key = tuple(converted_key)
            # Multi-dimensional indexing
            indexed_data = self._data.reshape(self.shape)[key]
        elif isinstance(key, WebGPUTensor):
            # Single tensor index
            indices = key._data.astype(np.int64)
            indexed_data = self._data.reshape(self.shape)[indices]
        else:
            # Single dimension indexing (slice, int, etc.)
            indexed_data = self._data.reshape(self.shape)[key]

        return WebGPUTensor(indexed_data, device=self.device, dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def backward(self, gradient=None, retain_graph=False, create_graph=False):
        """Backward propagation through the computation graph"""
        if not self.requires_grad:
            return

        # Check if graph still exists
        if not hasattr(self, '_backward_fn') or self._backward_fn is None:
            if hasattr(self, 'grad_fn') and self.grad_fn is not None:
                raise RuntimeError("Trying to backward through the graph a second time (or directly access a leaf Variable that doesn't require grad). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward.")

        if gradient is None:
            if self._data.size != 1:
                raise RuntimeError("grad can be implicitly created only for scalar outputs")
            gradient = WebGPUTensor(np.ones_like(self._data), device=self.device, dtype=self.dtype, _internal=True)

        # Topological sort for DAG-based backward pass
        visited = set()
        topo_order = []

        def build_topo(node):
            if id(node) in visited or not isinstance(node, WebGPUTensor):
                return
            visited.add(id(node))
            if hasattr(node, '_inputs'):
                for inp in node._inputs:
                    build_topo(inp)
            topo_order.append(node)

        build_topo(self)

        # Initialize gradient for the output
        if self.grad is None:
            self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype, _internal=True)
        # Handle gradient parameter properly
        if isinstance(gradient, WebGPUTensor):
            self.grad._data = gradient._data.copy()
        elif hasattr(gradient, '_data'):
            self.grad._data = gradient._data.copy()
        else:
            self.grad._data = np.array(gradient)

        # Backward pass in reverse topological order
        for node in reversed(topo_order):
            if hasattr(node, '_backward_fn') and node._backward_fn and node.grad is not None:
                # Call hooks on the gradient before propagating
                grad_to_propagate = node._call_hooks(node.grad) if hasattr(node, '_call_hooks') else node.grad

                # Pass create_graph flag to backward function if it accepts it
                try:
                    node._backward_fn(grad_to_propagate, create_graph=create_graph)
                except TypeError:
                    # Fallback for backward functions that don't accept create_graph
                    node._backward_fn(grad_to_propagate)

        # Clean up graph if not retaining
        if not retain_graph:
            for node in topo_order:
                if hasattr(node, '_backward_fn'):
                    node._backward_fn = None
                if hasattr(node, '_inputs'):
                    node._inputs = []

    def zero_(self):
        """Zero out the tensor data in-place"""
        self._data.fill(0)
        return self

    def retain_grad(self):
        """Enable gradient retention for non-leaf tensors"""
        self._retain_grad = True
        return self

    def register_hook(self, hook):
        """Register a backward hook on the tensor.

        The hook will be called every time a gradient with respect to the tensor is computed.
        The hook should have the following signature:
            hook(grad) -> Tensor or None

        Args:
            hook: A function that takes a gradient tensor and optionally returns a modified gradient

        Returns:
            A handle that can be used to remove the hook by calling handle.remove()
        """
        if not hasattr(self, '_hooks'):
            self._hooks = []

        # Store the hook
        self._hooks.append(hook)

        # Create a handle for removing the hook
        class HookHandle:
            def __init__(self, tensor, hook_fn, **kwargs):
                self.tensor = tensor
                self.hook_fn = hook_fn

            def remove(self):
                if hasattr(self.tensor, '_hooks') and self.hook_fn in self.tensor._hooks:
                    self.tensor._hooks.remove(self.hook_fn)

        return HookHandle(self, hook)

    def _call_hooks(self, grad):
        """Call all registered hooks on the gradient"""
        if not hasattr(self, '_hooks') or not self._hooks:
            return grad

        for hook in self._hooks:
            new_grad = hook(grad)
            if new_grad is not None:
                grad = new_grad
        return grad

    def __repr__(self):
        return f"tensor({self._data}, device='{self.device}', dtype='{self.dtype}')"
    
    def __float__(self):
        """Convert single-element tensor to Python float"""
        if self._data.size == 1:
            return float(self._data.item())
        else:
            raise TypeError(f"only single-element tensors can be converted to Python scalars")
    
    def __int__(self):
        """Convert single-element tensor to Python int"""
        if self._data.size == 1:
            return int(self._data.item())
        else:
            raise TypeError(f"only single-element tensors can be converted to Python scalars")

    def __len__(self):
        """Return the length of the first dimension"""
        if self.ndim == 0:
            raise TypeError("len() of unsized object")
        return self.shape[0]

    def __getitem__(self, key):
        """Support tensor indexing like tensor[indices]"""
        if isinstance(key, WebGPUTensor):
            # Convert WebGPUTensor indices to numpy array
            indices = key._data.astype(int)
            result_data = self._data[indices]

            # In PyTorch, indexing with tensor indices always preserves at least 1 dimension
            # even when the index tensor has 1 element
            if result_data.ndim == 0:
                result_data = np.array([result_data])
        else:
            result_data = self._data[key]

        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def __setitem__(self, key, value):
        """Support tensor item assignment like tensor[indices] = value"""
        if isinstance(value, WebGPUTensor):
            value_data = value._data
        else:
            value_data = value

        if isinstance(key, WebGPUTensor):
            # Convert WebGPUTensor indices to numpy array
            indices = key._data.astype(int)
            self._data[indices] = value_data
        else:
            self._data[key] = value_data

    def eq(self, other):
        """Element-wise equality comparison (returns tensor)"""
        if isinstance(other, WebGPUTensor):
            result_data = self._data == other._data
        else:
            result_data = self._data == other
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    def __eq__(self, other):
        """Element-wise equality comparison (returns tensor like PyTorch)"""
        if isinstance(other, WebGPUTensor):
            result_data = self._data == other._data
        else:
            result_data = self._data == other
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    def __ne__(self, other):
        """Element-wise not-equal comparison"""
        if isinstance(other, WebGPUTensor):
            result_data = self._data != other._data
        else:
            result_data = self._data != other
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    def __gt__(self, other):
        """Element-wise greater than comparison"""
        if isinstance(other, WebGPUTensor):
            result_data = self._data > other._data
        else:
            result_data = self._data > other
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    def __lt__(self, other):
        """Element-wise less than comparison"""
        if isinstance(other, WebGPUTensor):
            result_data = self._data < other._data
        else:
            result_data = self._data < other
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    def __ge__(self, other):
        """Element-wise greater than or equal comparison"""
        if isinstance(other, WebGPUTensor):
            result_data = self._data >= other._data
        else:
            result_data = self._data >= other
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    def __le__(self, other):
        """Element-wise less than or equal comparison"""
        if isinstance(other, WebGPUTensor):
            result_data = self._data <= other._data
        else:
            result_data = self._data <= other
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    def __hash__(self):
        """Make tensor hashable for use as dictionary keys.
        Uses object identity (id) so each tensor instance is unique."""
        return id(self)

    # Masked operations
    def masked_fill(self, mask, value):
        """Fill elements of self tensor with value where mask is True.

        Args:
            mask: Boolean tensor with same shape as self
            value: Value to fill

        Returns:
            New tensor with masked elements filled
        """
        if isinstance(mask, WebGPUTensor):
            mask_data = mask._data
        else:
            mask_data = np.array(mask)

        # Ensure mask is boolean type
        mask_data = mask_data.astype(bool)

        # Create a copy of the data
        result_data = self._data.copy()

        # Fill masked positions
        result_data[mask_data] = value

        result = WebGPUTensor(
            result_data,
            device=self.device,
            dtype=self.dtype,
            requires_grad=self.requires_grad
        )

        # Set up backward function if gradient tracking is enabled
        if self.requires_grad:
            def masked_fill_backward(grad_output):
                # Gradient flows through non-masked elements only
                grad_input = grad_output._data.copy()
                grad_input[mask_data] = 0  # Zero out gradients for masked positions

                grad_tensor = WebGPUTensor(grad_input, device="webgpu", dtype=self.dtype, _internal=True)
                if self.grad is None:
                    self.grad = grad_tensor
                else:
                    self.grad._data += grad_tensor._data

            result._backward_fn = masked_fill_backward
            result._inputs = [self]

        return result

    def masked_fill_(self, mask, value):
        """In-place version of masked_fill"""
        if isinstance(mask, WebGPUTensor):
            mask_data = mask._data
        else:
            mask_data = np.array(mask)

        # Ensure mask is boolean type
        mask_data = mask_data.astype(bool)

        self._data[mask_data] = value
        return self

    # Arithmetic operators
    def __add__(self, other):
        if isinstance(other, WebGPUTensor):
            result_data = self._data + other._data
        else:
            result_data = self._data + other

        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype,
                             requires_grad=self.requires_grad or (isinstance(other, WebGPUTensor) and other.requires_grad))

        # Set up autograd
        if result.requires_grad:
            result.grad_fn = 'AddBackward'
            result._inputs = []
            if self.requires_grad:
                result._inputs.append(self)
            if isinstance(other, WebGPUTensor) and other.requires_grad:
                result._inputs.append(other)

            def add_backward(grad, create_graph=False):
                if self.requires_grad:
                    if self.grad is None:
                        self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype, _internal=True)
                    # Handle broadcasting: reduce gradient to match original shape
                    grad_data = grad._data
                    # Sum out added dims and reduce broadcast dims
                    ndims_added = grad_data.ndim - self._data.ndim
                    for i in range(ndims_added):
                        grad_data = grad_data.sum(axis=0)
                    # Reduce dimensions that were broadcast
                    for i in range(grad_data.ndim):
                        if self._data.shape[i] == 1 and grad_data.shape[i] > 1:
                            grad_data = np.sum(grad_data, axis=i, keepdims=True)
                    self.grad._data += grad_data
                    if create_graph:
                        self.grad.requires_grad = True
                        self.grad.grad_fn = 'AddBackwardBackward'

                if isinstance(other, WebGPUTensor) and other.requires_grad:
                    if other.grad is None:
                        other.grad = WebGPUTensor(np.zeros_like(other._data), device=other.device, dtype=other.dtype, _internal=True)
                    # Handle broadcasting for other
                    grad_data = grad._data
                    ndims_added = grad_data.ndim - other._data.ndim
                    for i in range(ndims_added):
                        grad_data = grad_data.sum(axis=0)
                    for i in range(grad_data.ndim):
                        if other._data.shape[i] == 1 and grad_data.shape[i] > 1:
                            grad_data = np.sum(grad_data, axis=i, keepdims=True)
                    other.grad._data += grad_data
                    if create_graph:
                        other.grad.requires_grad = True

            result._backward_fn = add_backward

        return result
    
    def __sub__(self, other):
        if isinstance(other, WebGPUTensor):
            result_data = self._data - other._data
        else:
            result_data = self._data - other

        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype,
                             requires_grad=self.requires_grad or (isinstance(other, WebGPUTensor) and other.requires_grad))

        # Set up autograd
        if result.requires_grad:
            result.grad_fn = 'SubBackward'
            result._inputs = []
            if self.requires_grad:
                result._inputs.append(self)
            if isinstance(other, WebGPUTensor) and other.requires_grad:
                result._inputs.append(other)

            def sub_backward(grad):
                if self.requires_grad:
                    if self.grad is None:
                        self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype, _internal=True)
                    # Gradient w.r.t. self: grad (unchanged) but handle broadcasting
                    grad_data = grad._data
                    ndims_added = grad_data.ndim - self._data.ndim
                    for i in range(ndims_added):
                        grad_data = grad_data.sum(axis=0)
                    for i in range(grad_data.ndim):
                        if self._data.shape[i] == 1 and grad_data.shape[i] > 1:
                            grad_data = np.sum(grad_data, axis=i, keepdims=True)
                    self.grad._data += grad_data
                if isinstance(other, WebGPUTensor) and other.requires_grad:
                    if other.grad is None:
                        other.grad = WebGPUTensor(np.zeros_like(other._data), device=other.device, dtype=other.dtype, _internal=True)
                    # Gradient w.r.t. other: -grad (negated) and handle broadcasting
                    grad_data = grad._data
                    ndims_added = grad_data.ndim - other._data.ndim
                    for i in range(ndims_added):
                        grad_data = grad_data.sum(axis=0)
                    for i in range(grad_data.ndim):
                        if other._data.shape[i] == 1 and grad_data.shape[i] > 1:
                            grad_data = np.sum(grad_data, axis=i, keepdims=True)
                    other.grad._data -= grad_data

            result._backward_fn = sub_backward

        return result
    
    def __mul__(self, other):
        if isinstance(other, WebGPUTensor):
            result_data = self._data * other._data
        else:
            result_data = self._data * other

        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype,
                             requires_grad=self.requires_grad or (isinstance(other, WebGPUTensor) and other.requires_grad))

        # Set up autograd
        if result.requires_grad:
            result.grad_fn = 'MulBackward'
            result._inputs = []
            if self.requires_grad:
                result._inputs.append(self)
            if isinstance(other, WebGPUTensor) and other.requires_grad:
                result._inputs.append(other)

            def mul_backward(grad, create_graph=False):
                if self.requires_grad:
                    if self.grad is None:
                        self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype, _internal=True)

                    # Gradient: grad * other
                    if create_graph:
                        grad_tensor = grad if isinstance(grad, WebGPUTensor) else WebGPUTensor(grad._data, device=self.device, dtype=self.dtype, _internal=True)
                        if isinstance(other, WebGPUTensor):
                            other_tensor = WebGPUTensor(other._data, device=self.device, dtype=self.dtype, requires_grad=True, _internal=True)
                            grad_self = grad_tensor * other_tensor
                        else:
                            grad_self = grad_tensor * other
                        self.grad._data += grad_self._data
                        self.grad.requires_grad = True
                        self.grad.grad_fn = 'MulBackwardBackward'
                    else:
                        if isinstance(other, WebGPUTensor):
                            self.grad._data += grad._data * other._data
                        else:
                            self.grad._data += grad._data * other

                if isinstance(other, WebGPUTensor) and other.requires_grad:
                    if other.grad is None:
                        other.grad = WebGPUTensor(np.zeros_like(other._data), device=other.device, dtype=other.dtype, _internal=True)
                    # Gradient: grad * self
                    if create_graph:
                        grad_tensor = grad if isinstance(grad, WebGPUTensor) else WebGPUTensor(grad._data, device=self.device, dtype=self.dtype, _internal=True)
                        self_tensor = WebGPUTensor(self._data, device=self.device, dtype=self.dtype, requires_grad=True, _internal=True)
                        grad_other = grad_tensor * self_tensor
                        other.grad._data += grad_other._data
                        other.grad.requires_grad = True
                    else:
                        other.grad._data += grad._data * self._data

            result._backward_fn = mul_backward

        return result
    
    def __truediv__(self, other):
        if isinstance(other, WebGPUTensor):
            result_data = self._data / other._data
        else:
            result_data = self._data / other

        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype,
                             requires_grad=self.requires_grad or (isinstance(other, WebGPUTensor) and other.requires_grad))

        # Set up autograd
        if result.requires_grad:
            result.grad_fn = 'DivBackward'
            result._inputs = []
            if self.requires_grad:
                result._inputs.append(self)
            if isinstance(other, WebGPUTensor) and other.requires_grad:
                result._inputs.append(other)

            def div_backward(grad):
                if self.requires_grad:
                    if self.grad is None:
                        self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype, _internal=True)
                    # Gradient w.r.t. self: grad / other
                    if isinstance(other, WebGPUTensor):
                        self.grad._data += grad._data / other._data
                    else:
                        self.grad._data += grad._data / other
                if isinstance(other, WebGPUTensor) and other.requires_grad:
                    if other.grad is None:
                        other.grad = WebGPUTensor(np.zeros_like(other._data), device=other.device, dtype=other.dtype, _internal=True)
                    # Gradient w.r.t. other: -grad * self / other^2
                    other.grad._data -= grad._data * self._data / (other._data ** 2)

            result._backward_fn = div_backward

        return result

    def __pow__(self, other):
        if isinstance(other, WebGPUTensor):
            result_data = np.power(self._data, other._data)
        else:
            result_data = np.power(self._data, other)

        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype,
                             requires_grad=self.requires_grad or (isinstance(other, WebGPUTensor) and other.requires_grad))

        # Set up autograd
        if result.requires_grad:
            result.grad_fn = 'PowBackward'
            result._inputs = []
            if self.requires_grad:
                result._inputs.append(self)
            if isinstance(other, WebGPUTensor) and other.requires_grad:
                result._inputs.append(other)

            def pow_backward(grad, create_graph=False):
                if self.requires_grad:
                    if self.grad is None:
                        self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype, _internal=True)

                    # Gradient w.r.t. base: grad * exponent * base^(exponent-1)
                    if create_graph:
                        # Use differentiable operations for higher-order gradients
                        grad_tensor = grad if isinstance(grad, WebGPUTensor) else WebGPUTensor(grad._data, device=self.device, dtype=self.dtype, _internal=True)
                        base_tensor = WebGPUTensor(self._data, device=self.device, dtype=self.dtype, requires_grad=True, _internal=True)

                        if isinstance(other, WebGPUTensor):
                            exponent_tensor = other
                        else:
                            exponent_tensor = WebGPUTensor(np.array(other), device=self.device, dtype=self.dtype)

                        # grad * exponent * base^(exponent-1)
                        grad_base = grad_tensor * exponent_tensor * (base_tensor ** (exponent_tensor - 1))
                        self.grad._data += grad_base._data
                        self.grad.requires_grad = True
                        self.grad.grad_fn = 'PowBackwardBackward'
                    else:
                        # Use NumPy for efficiency when not creating graph
                        if isinstance(other, WebGPUTensor):
                            self.grad._data += grad._data * other._data * np.power(self._data, other._data - 1)
                        else:
                            self.grad._data += grad._data * other * np.power(self._data, other - 1)

                if isinstance(other, WebGPUTensor) and other.requires_grad:
                    if other.grad is None:
                        other.grad = WebGPUTensor(np.zeros_like(other._data), device=other.device, dtype=other.dtype, _internal=True)
                    # Gradient w.r.t. exponent: grad * log(base) * base^exponent
                    if create_graph:
                        grad_tensor = grad if isinstance(grad, WebGPUTensor) else WebGPUTensor(grad._data, device=self.device, dtype=self.dtype, _internal=True)
                        base_tensor = WebGPUTensor(self._data, device=self.device, dtype=self.dtype, _internal=True)
                        result_tensor = WebGPUTensor(result_data, device=self.device, dtype=self.dtype)

                        # grad * log(base) * base^exponent
                        grad_exp = grad_tensor * WebGPUTensor(np.log(self._data), device=self.device, dtype=self.dtype) * result_tensor
                        other.grad._data += grad_exp._data
                        other.grad.requires_grad = True
                    else:
                        other.grad._data += grad._data * np.log(self._data) * result_data

            result._backward_fn = pow_backward

        return result

    def __neg__(self):
        """Unary negation operator (-tensor)"""
        result_data = -self._data
        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)

        # Set up autograd
        if result.requires_grad:
            result.grad_fn = 'NegBackward'
            result._inputs = [self]

            def neg_backward(grad):
                if self.requires_grad:
                    if self.grad is None:
                        self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype, _internal=True)
                    # Gradient of negation: -grad
                    self.grad._data -= grad._data

            result._backward_fn = neg_backward

        return result

    def __pos__(self):
        """Unary positive operator (+tensor)"""
        result_data = +self._data
        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)

        # Set up autograd
        if result.requires_grad:
            result.grad_fn = 'PosBackward'
            result._inputs = [self]

            def pos_backward(grad):
                if self.requires_grad:
                    if self.grad is None:
                        self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype, _internal=True)
                    # Gradient of positive: grad (unchanged)
                    self.grad._data += grad._data

            result._backward_fn = pos_backward

        return result

    def __radd__(self, other):
        result_data = other + self._data
        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)

        # Autograd disabled for stability - backward functions removed

        return result
    
    def __rmul__(self, other):
        result_data = other * self._data
        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)

        # Autograd disabled for stability - backward functions removed

        return result

    def __rsub__(self, other):
        result_data = other - self._data
        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)

        # Autograd disabled for stability - backward functions removed

        return result

    def __rtruediv__(self, other):
        result_data = other / self._data
        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)

        # Autograd disabled for stability - backward functions removed

        return result

    def __rpow__(self, other):
        result_data = np.power(other, self._data)
        result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad)

        # Autograd disabled for stability - backward functions removed

        return result

    # In-place operations (required by PyTorch optimizers)
    def mul_(self, other):
        """In-place multiplication"""
        if isinstance(other, WebGPUTensor):
            self._data *= other._data
        else:
            self._data *= other
        return self

    def add(self, other, alpha=1):
        """Non-in-place addition with optional scaling: result = self + alpha * other"""
        if isinstance(other, WebGPUTensor):
            if alpha != 1:
                result_data = self._data + alpha * other._data
            else:
                result_data = self._data + other._data
        else:
            if alpha != 1:
                result_data = self._data + alpha * other
            else:
                result_data = self._data + other
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def add_(self, other, alpha=1):
        """In-place addition with optional scaling: self += alpha * other"""
        if isinstance(other, WebGPUTensor):
            if alpha != 1:
                self._data += alpha * other._data
            else:
                self._data += other._data
        else:
            if alpha != 1:
                self._data += alpha * other
            else:
                self._data += other
        return self

    def sub_(self, other, alpha=1):
        """In-place subtraction with optional scaling: self -= alpha * other"""
        if isinstance(other, WebGPUTensor):
            if alpha != 1:
                self._data -= alpha * other._data
            else:
                self._data -= other._data
        else:
            if alpha != 1:
                self._data -= alpha * other
            else:
                self._data -= other
        return self

    def div_(self, other):
        """In-place division"""
        if isinstance(other, WebGPUTensor):
            self._data /= other._data
        else:
            self._data /= other
        return self

    def pow_(self, exponent):
        """In-place power"""
        if isinstance(exponent, WebGPUTensor):
            self._data = np.power(self._data, exponent._data)
        else:
            self._data = np.power(self._data, exponent)
        return self

    def sqrt_(self):
        """In-place square root"""
        self._data = np.sqrt(self._data)
        return self

    def addcmul_(self, tensor1, tensor2, value=1):
        """In-place: self += value * tensor1 * tensor2"""
        t1_data = tensor1._data if isinstance(tensor1, WebGPUTensor) else tensor1
        t2_data = tensor2._data if isinstance(tensor2, WebGPUTensor) else tensor2
        self._data += value * t1_data * t2_data
        return self

    def addcdiv_(self, tensor1, tensor2, value=1):
        """In-place: self += value * tensor1 / tensor2"""
        t1_data = tensor1._data if isinstance(tensor1, WebGPUTensor) else tensor1
        t2_data = tensor2._data if isinstance(tensor2, WebGPUTensor) else tensor2
        self._data += value * t1_data / t2_data
        return self

    def clamp(self, min=None, max=None):
        """Clamp values and return a new tensor"""
        result_data = self._data.copy()
        if min is not None and max is not None:
            result_data = np.clip(result_data, min, max)
        elif min is not None:
            result_data = np.maximum(result_data, min)
        elif max is not None:
            result_data = np.minimum(result_data, max)
        return WebGPUTensor(result_data, device=self.device, dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def clamp_(self, min_val=None, max_val=None):
        """In-place clamp values"""
        if min_val is not None and max_val is not None:
            self._data = np.clip(self._data, min_val, max_val)
        elif min_val is not None:
            self._data = np.maximum(self._data, min_val)
        elif max_val is not None:
            self._data = np.minimum(self._data, max_val)
        return self

    def zero_(self):
        """Fill tensor with zeros in-place"""
        self._data.fill(0)
        return self

    def fill_(self, value):
        """Fill tensor with a value in-place"""
        self._data.fill(value)
        return self

    def copy_(self, other):
        """Copy data from another tensor in-place"""
        if isinstance(other, WebGPUTensor):
            self._data = other._data.copy()
            self.shape = other.shape
            self.dtype = other.dtype
        else:
            raise TypeError("copy_ requires a WebGPUTensor")
        return self

    def scatter_(self, dim, index, src):
        """Scatter values from src into self at positions specified by index along dimension dim

        Args:
            dim: The axis along which to index
            index: The indices of elements to scatter (LongTensor)
            src: The source tensor or scalar value

        Example:
            >>> x = torch.zeros(3, 5)
            >>> x.scatter_(1, torch.tensor([[0, 1], [2, 3], [4, 0]]), 1.0)
        """
        # Extract numpy arrays
        index_data = index._data if isinstance(index, WebGPUTensor) else index

        if isinstance(src, WebGPUTensor):
            src_data = src._data
        elif isinstance(src, (int, float)):
            # If src is a scalar, create array of same shape as index
            src_data = np.full_like(index_data, src, dtype=self._data.dtype)
        else:
            src_data = src

        # Use numpy's advanced indexing to scatter values
        # This is a simplified version - full PyTorch scatter is more complex
        if dim == 0:
            for i in range(index_data.shape[0]):
                for j in range(index_data.shape[1] if index_data.ndim > 1 else 1):
                    if index_data.ndim > 1:
                        idx = int(index_data[i, j])
                        self._data[idx, j] = src_data[i, j] if src_data.ndim > 1 else src_data
                    else:
                        idx = int(index_data[i])
                        self._data[idx] = src_data[i] if hasattr(src_data, '__len__') else src_data
        elif dim == 1:
            for i in range(index_data.shape[0]):
                for j in range(index_data.shape[1] if index_data.ndim > 1 else 1):
                    if index_data.ndim > 1:
                        idx = int(index_data[i, j])
                        self._data[i, idx] = src_data[i, j] if src_data.ndim > 1 else src_data
                    else:
                        idx = int(index_data[j])
                        self._data[i, idx] = src_data[j] if hasattr(src_data, '__len__') else src_data
        else:
            # For higher dimensions, use a general approach
            it = np.nditer(index_data, flags=['multi_index'])
            for idx_val in it:
                multi_idx = list(it.multi_index)
                target_idx = list(multi_idx)
                target_idx[dim] = int(idx_val)
                if isinstance(src_data, np.ndarray) and src_data.shape == index_data.shape:
                    self._data[tuple(target_idx)] = src_data[multi_idx]
                else:
                    self._data[tuple(target_idx)] = src_data

        return self

    def __matmul__(self, other):
        \"\"\"Matrix multiplication operator (@) with WebGPU acceleration\"\"\"

        # GPU ACCELERATION: Check if we can use WebGPU
        self_device = str(self.device)
        other_device = str(other.device) if isinstance(other, WebGPUTensor) else 'cpu'
        use_gpu = (
            self_device == 'webgpu' and
            isinstance(other, WebGPUTensor) and
            other_device == 'webgpu' and
            self.ndim == 2 and
            other.ndim == 2 and
            '__webgpu_matmul__' in globals() and
            hasattr(self, '_gpu_buffer_id') and
            hasattr(other, '_gpu_buffer_id') and
            self._gpu_buffer_id is not None and
            other._gpu_buffer_id is not None
        )

        if use_gpu:
            # GPU PATH: Use WebGPU bridge for massive speedup
            try:
                result_buffer_id = __webgpu_matmul__(
                    self._gpu_buffer_id,
                    other._gpu_buffer_id,
                    list(self.shape),
                    list(other.shape)
                )

                # OPTIMIZATION: Keep result on GPU, don't transfer to CPU!
                # Only create minimal CPU placeholder for shape tracking
                result_shape = (self.shape[0], other.shape[1])
                result_data = np.zeros(result_shape, dtype=self.dtype)  # Lightweight placeholder

                result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype,
                                     requires_grad=self.requires_grad or other.requires_grad,
                                     _internal=True)
                result._gpu_buffer_id = result_buffer_id  # Actual data lives on GPU!
                result._gpu_only = True  # Flag to avoid unnecessary CPU sync

            except Exception as e:
                # Fallback to CPU if GPU fails
                use_gpu = False

        if not use_gpu:
            # CPU PATH: Original NumPy implementation
            if isinstance(other, WebGPUTensor):
                if self.ndim == 2 and other.ndim == 2:
                    result_data = np.dot(self._data, other._data)
                elif self.ndim == 1 and other.ndim == 2:
                    result_data = np.dot(self._data, other._data)
                elif self.ndim == 2 and other.ndim == 1:
                    result_data = np.dot(self._data, other._data)
                else:
                    result_data = np.matmul(self._data, other._data)
            else:
                result_data = np.matmul(self._data, other)

            result = WebGPUTensor(result_data, device="webgpu", dtype=self.dtype,
                                 requires_grad=self.requires_grad or (isinstance(other, WebGPUTensor) and other.requires_grad))

        # Set up autograd for matrix multiplication
        if result.requires_grad:
            result.grad_fn = 'MatMulBackward'
            result._inputs = []
            if self.requires_grad:
                result._inputs.append(self)
            if isinstance(other, WebGPUTensor) and other.requires_grad:
                result._inputs.append(other)

            def matmul_backward(grad):
                if self.requires_grad:
                    if self.grad is None:
                        self.grad = WebGPUTensor(np.zeros_like(self._data), device=self.device, dtype=self.dtype, _internal=True)
                    # Gradient w.r.t. self: grad @ other.T
                    if isinstance(other, WebGPUTensor):
                        if other.ndim == 2:
                            self_grad = np.dot(grad._data, other._data.T)
                        elif other.ndim == 1:
                            # If other is 1D, grad is 1D, need outer product
                            self_grad = np.outer(grad._data, other._data)
                        else:
                            self_grad = np.matmul(grad._data, np.swapaxes(other._data, -2, -1))
                    else:
                        self_grad = np.dot(grad._data, other.T)

                    # Handle broadcasting
                    ndims_added = self_grad.ndim - self._data.ndim
                    for i in range(ndims_added):
                        self_grad = self_grad.sum(axis=0)
                    for i in range(self_grad.ndim):
                        if self._data.shape[i] == 1 and self_grad.shape[i] > 1:
                            self_grad = np.sum(self_grad, axis=i, keepdims=True)
                    self.grad._data += self_grad

                if isinstance(other, WebGPUTensor) and other.requires_grad:
                    if other.grad is None:
                        other.grad = WebGPUTensor(np.zeros_like(other._data), device=other.device, dtype=other.dtype, _internal=True)
                    # Gradient w.r.t. other: self.T @ grad
                    if self.ndim == 2 and other.ndim == 2:
                        other_grad = np.dot(self._data.T, grad._data)
                    elif self.ndim == 2 and other.ndim == 1:
                        # If other is 1D, result is 1D, grad is 1D
                        other_grad = np.dot(self._data.T, grad._data)
                    elif self.ndim == 1 and other.ndim == 2:
                        # If self is 1D, need to handle differently
                        other_grad = np.outer(self._data, grad._data)
                    else:
                        other_grad = np.matmul(np.swapaxes(self._data, -2, -1), grad._data)

                    # Handle broadcasting
                    ndims_added = other_grad.ndim - other._data.ndim
                    for i in range(ndims_added):
                        other_grad = other_grad.sum(axis=0)
                    for i in range(other_grad.ndim):
                        if other._data.shape[i] == 1 and other_grad.shape[i] > 1:
                            other_grad = np.sum(other_grad, axis=i, keepdims=True)
                    other.grad._data += other_grad

            result._backward_fn = matmul_backward

        return result
    
    def __rmatmul__(self, other):
        \"\"\"Reverse matrix multiplication\"\"\"
        result_data = np.matmul(other, self._data)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def matmul(self, other):
        \"\"\"Matrix multiplication method (calls __matmul__)\"\"\"
        return self.__matmul__(other)

    def mm(self, other):
        \"\"\"Matrix multiplication for 2D tensors\"\"\"
        if self.ndim != 2 or (isinstance(other, WebGPUTensor) and other.ndim != 2):
            raise RuntimeError("mm requires both tensors to be 2D")
        return self.__matmul__(other)

    def dot(self, other):
        \"\"\"Dot product of two 1D tensors\"\"\"
        if isinstance(other, WebGPUTensor):
            result_data = np.dot(self._data, other._data)
        else:
            result_data = np.dot(self._data, other)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def square(self):
        \"\"\"Element-wise square\"\"\"
        result_data = np.square(self._data)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def sqrt(self):
        \"\"\"Element-wise square root\"\"\"
        result_data = np.sqrt(self._data)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def pow(self, exponent):
        \"\"\"Element-wise power\"\"\"
        if isinstance(exponent, WebGPUTensor):
            result_data = np.power(self._data, exponent._data)
        else:
            result_data = np.power(self._data, exponent)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def norm(self, p=2, dim=None, keepdim=False):
        \"\"\"Compute the norm of the tensor\"\"\"
        if p == 'fro' or p == 'Frobenius':
            # Frobenius norm
            if dim is None:
                result_data = np.linalg.norm(self._data, 'fro')
            else:
                result_data = np.linalg.norm(self._data, 'fro', axis=dim, keepdims=keepdim)
        elif p == float('inf'):
            # Max norm
            if dim is None:
                result_data = np.max(np.abs(self._data))
            else:
                result_data = np.max(np.abs(self._data), axis=dim, keepdims=keepdim)
        elif p == float('-inf'):
            # Min norm
            if dim is None:
                result_data = np.min(np.abs(self._data))
            else:
                result_data = np.min(np.abs(self._data), axis=dim, keepdims=keepdim)
        elif p == 0:
            # L0 norm (count of non-zero elements)
            if dim is None:
                result_data = np.count_nonzero(self._data)
            else:
                result_data = np.count_nonzero(self._data, axis=dim)
                if keepdim:
                    result_data = np.expand_dims(result_data, axis=dim)
        elif p == 1:
            # L1 norm
            if dim is None:
                result_data = np.sum(np.abs(self._data))
            else:
                result_data = np.sum(np.abs(self._data), axis=dim, keepdims=keepdim)
        else:
            # Lp norm
            if dim is None:
                result_data = np.power(np.sum(np.power(np.abs(self._data), p)), 1.0/p)
            else:
                result_data = np.power(np.sum(np.power(np.abs(self._data), p), axis=dim, keepdims=keepdim), 1.0/p)

        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def exp(self):
        \"\"\"Element-wise exponential\"\"\"
        result_data = np.exp(self._data)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def log(self):
        \"\"\"Element-wise natural logarithm\"\"\"
        result_data = np.log(self._data)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def sin(self):
        \"\"\"Element-wise sine\"\"\"
        result_data = np.sin(self._data)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def cos(self):
        \"\"\"Element-wise cosine\"\"\"
        result_data = np.cos(self._data)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def clamp(self, min=None, max=None):
        \"\"\"Clamp tensor values to a range\"\"\"
        result_data = self._data.copy()
        if min is not None:
            result_data = np.maximum(result_data, min)
        if max is not None:
            result_data = np.minimum(result_data, max)
        return WebGPUTensor(result_data, device="webgpu", dtype=self.dtype, requires_grad=self.requires_grad, _internal=True)

    def clip(self, min=None, max=None):
        \"\"\"Alias for clamp\"\"\"
        return self.clamp(min, max)

    def retain_grad(self):
        \"\"\"Enable gradient retention for non-leaf tensor\"\"\"
        if not self.requires_grad:
            raise RuntimeError("can't retain_grad on Tensor that has requires_grad=False")
        self._retain_grad = True
        return self
    
    def zero_grad(self):
        \"\"\"Zero out the gradients\"\"\"
        if self.grad is not None:
            self.grad._data.fill(0)

    def requires_grad_(self, requires_grad=True):
        \"\"\"In-place method to set requires_grad flag.

        Args:
            requires_grad: Boolean flag for gradient tracking

        Returns:
            Self for method chaining
        \"\"\"
        self.requires_grad = requires_grad
        return self

    def _matmul_backward(self, grad_output, other):
        \"\"\"Backward pass for matrix multiplication\"\"\"
        if isinstance(other, WebGPUTensor):
            # d/da (a @ b) = grad_output @ b.T
            if self.grad is None:
                self.grad = WebGPUTensor(np.zeros_like(self._data), device="webgpu", dtype=self.dtype)
            self_grad = np.matmul(grad_output._data, other._data.T)
            self.grad._data += self_grad
            
            # d/db (a @ b) = a.T @ grad_output  
            if other.requires_grad:
                if other.grad is None:
                    other.grad = WebGPUTensor(np.zeros_like(other._data), device="webgpu", dtype=other.dtype)
                other_grad = np.matmul(self._data.T, grad_output._data)
                other.grad._data += other_grad

# Linear algebra operations module
class TorchLinalg:
    \"\"\"Linear algebra operations module\"\"\"
    
    def __init__(self, **kwargs):
        pass
    
    def det(self, input_tensor):
        \"\"\"Compute determinant\"\"\"
        if isinstance(input_tensor, WebGPUTensor):
            if input_tensor.ndim != 2 or input_tensor.shape[0] != input_tensor.shape[1]:
                raise RuntimeError("linalg.det() expects a 2D square tensor")
            det_value = np.linalg.det(input_tensor._data.reshape(input_tensor.shape))
            return WebGPUTensor([det_value], device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            return np.linalg.det(input_tensor)
    
    def inv(self, input_tensor):
        \"\"\"Compute matrix inverse\"\"\"
        if isinstance(input_tensor, WebGPUTensor):
            if input_tensor.ndim != 2 or input_tensor.shape[0] != input_tensor.shape[1]:
                raise RuntimeError("linalg.inv() expects a 2D square tensor")
            inv_data = np.linalg.inv(input_tensor._data.reshape(input_tensor.shape))
            return WebGPUTensor(inv_data, device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            return np.linalg.inv(input_tensor)
    
    def norm(self, input_tensor, ord=None, dim=None, keepdim=False):
        \"\"\"Compute matrix or vector norm\"\"\"
        if isinstance(input_tensor, WebGPUTensor):
            if dim is None:
                norm_value = np.linalg.norm(input_tensor._data, ord=ord)
                return WebGPUTensor([norm_value], device="webgpu", dtype=input_tensor.dtype, _internal=True)
            else:
                norm_data = np.linalg.norm(input_tensor._data.reshape(input_tensor.shape), ord=ord, axis=dim, keepdims=keepdim)
                return WebGPUTensor(norm_data, device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            return np.linalg.norm(input_tensor, ord=ord, axis=dim, keepdims=keepdim)
    
    def eig(self, input_tensor):
        \"\"\"Compute eigenvalues and eigenvectors\"\"\"
        if isinstance(input_tensor, WebGPUTensor):
            if input_tensor.ndim != 2 or input_tensor.shape[0] != input_tensor.shape[1]:
                raise RuntimeError("linalg.eig() expects a 2D square tensor")
            eigenvalues, eigenvectors = np.linalg.eig(input_tensor._data.reshape(input_tensor.shape))
            return (
                WebGPUTensor(eigenvalues, device="webgpu", dtype=input_tensor.dtype, _internal=True),
                WebGPUTensor(eigenvectors, device="webgpu", dtype=input_tensor.dtype, _internal=True)
            )
        else:
            return np.linalg.eig(input_tensor)
    
    def svd(self, input_tensor, full_matrices=True):
        \"\"\"Compute singular value decomposition\"\"\"
        if isinstance(input_tensor, WebGPUTensor):
            U, S, Vh = np.linalg.svd(input_tensor._data.reshape(input_tensor.shape), full_matrices=full_matrices)
            return (
                WebGPUTensor(U, device="webgpu", dtype=input_tensor.dtype, _internal=True),
                WebGPUTensor(S, device="webgpu", dtype=input_tensor.dtype, _internal=True),
                WebGPUTensor(Vh, device="webgpu", dtype=input_tensor.dtype, _internal=True)
            )
        else:
            return np.linalg.svd(input_tensor, full_matrices=full_matrices)

# Neural network functional operations
class TorchNNFunctional:
    @staticmethod
    def relu(input_tensor):
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.maximum(input_tensor._data, 0)
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            return np.maximum(input_tensor, 0)

    @staticmethod
    def sigmoid(input_tensor):
        if isinstance(input_tensor, WebGPUTensor):
            result_data = 1 / (1 + np.exp(-input_tensor._data))
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            return 1 / (1 + np.exp(-input_tensor))

    @staticmethod
    def softmax(input_tensor, dim=-1):
        """Apply softmax function"""
        if isinstance(input_tensor, WebGPUTensor):
            # Subtract max for numerical stability
            x = input_tensor._data
            x_max = np.max(x, axis=dim, keepdims=True)
            exp_x = np.exp(x - x_max)
            result_data = exp_x / np.sum(exp_x, axis=dim, keepdims=True)
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            x_max = np.max(input_tensor, axis=dim, keepdims=True)
            exp_x = np.exp(input_tensor - x_max)
            return exp_x / np.sum(exp_x, axis=dim, keepdims=True)

    @staticmethod
    def log_softmax(input_tensor, dim=-1):
        """Apply log softmax function"""
        if isinstance(input_tensor, WebGPUTensor):
            # Numerically stable log softmax
            x = input_tensor._data
            x_max = np.max(x, axis=dim, keepdims=True)
            log_sum_exp = np.log(np.sum(np.exp(x - x_max), axis=dim, keepdims=True))
            result_data = x - x_max - log_sum_exp
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            x_max = np.max(input_tensor, axis=dim, keepdims=True)
            log_sum_exp = np.log(np.sum(np.exp(input_tensor - x_max), axis=dim, keepdims=True))
            return input_tensor - x_max - log_sum_exp

    @staticmethod
    def tanh(input_tensor):
        """Apply tanh activation"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.tanh(input_tensor._data)
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            return np.tanh(input_tensor)

    @staticmethod
    def leaky_relu(input_tensor, negative_slope=0.01):
        """Apply leaky ReLU activation"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.where(input_tensor._data > 0, input_tensor._data, input_tensor._data * negative_slope)
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            return np.where(input_tensor > 0, input_tensor, input_tensor * negative_slope)

    @staticmethod
    def gelu(input_tensor):
        """Apply GELU activation"""
        if isinstance(input_tensor, WebGPUTensor):
            x = input_tensor._data
            result_data = 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            return 0.5 * input_tensor * (1 + np.tanh(np.sqrt(2 / np.pi) * (input_tensor + 0.044715 * input_tensor**3)))

    @staticmethod
    def mse_loss(input_tensor, target, reduction='mean'):
        """Mean Squared Error loss"""
        if isinstance(input_tensor, WebGPUTensor):
            inp = input_tensor._data
            tgt = target._data if isinstance(target, WebGPUTensor) else target
            squared_diff = (inp - tgt) ** 2

            if reduction == 'mean':
                result = np.mean(squared_diff)
            elif reduction == 'sum':
                result = np.sum(squared_diff)
            elif reduction == 'none':
                result = squared_diff
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

            loss_tensor = WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=input_tensor.dtype,
                                      requires_grad=(input_tensor.requires_grad or (isinstance(target, WebGPUTensor) and target.requires_grad)))

            # Set up backward function for MSE loss
            if input_tensor.requires_grad or (isinstance(target, WebGPUTensor) and target.requires_grad):
                def mse_backward(grad_output):
                    # Gradient of MSE: 2 * (input - target) / N
                    diff = inp - tgt
                    if reduction == 'mean':
                        grad_input = 2.0 * diff / diff.size
                    elif reduction == 'sum':
                        grad_input = 2.0 * diff
                    else:  # reduction == 'none'
                        grad_input = 2.0 * diff

                    # Multiply by upstream gradient
                    if isinstance(grad_output._data, np.ndarray):
                        grad_input = grad_input * grad_output._data
                    else:
                        grad_input = grad_input * grad_output._data

                    # Accumulate gradient to input tensor
                    if input_tensor.requires_grad:
                        if input_tensor.grad is None:
                            input_tensor.grad = WebGPUTensor(grad_input, device="webgpu", dtype=input_tensor.dtype, _internal=True)
                        else:
                            input_tensor.grad._data += grad_input

                loss_tensor._backward_fn = mse_backward
                loss_tensor._inputs = [input_tensor]

            return loss_tensor
        else:
            squared_diff = (input_tensor - target) ** 2

            if reduction == 'mean':
                return np.mean(squared_diff)
            elif reduction == 'sum':
                return np.sum(squared_diff)
            elif reduction == 'none':
                return squared_diff
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

    @staticmethod
    def cross_entropy(input_tensor, target, weight=None, reduction='mean'):
        """Cross entropy loss (combines log_softmax and nll_loss)"""
        # Apply log_softmax to input
        log_probs = TorchNNFunctional.log_softmax(input_tensor, dim=-1)

        if isinstance(log_probs, WebGPUTensor):
            log_probs_data = log_probs._data
            target_data = target._data if isinstance(target, WebGPUTensor) else target

            # Handle both 1D and 2D inputs
            if log_probs_data.ndim == 1:
                # Single sample
                loss = -log_probs_data[int(target_data)]
                if weight is not None:
                    weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                    loss = loss * weight_data[int(target_data)]
            else:
                # Batch of samples
                batch_size = log_probs_data.shape[0]
                target_indices = target_data.astype(int) if hasattr(target_data, 'astype') else int(target_data)

                if np.isscalar(target_indices):
                    loss = -log_probs_data[0, target_indices]
                    if weight is not None:
                        weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                        loss = loss * weight_data[target_indices]
                else:
                    # Gather losses for each sample
                    losses = np.array([log_probs_data[i, target_indices[i]] for i in range(batch_size)])
                    loss = -losses

                    if weight is not None:
                        weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                        weight_gathered = np.array([weight_data[target_indices[i]] for i in range(batch_size)])
                        loss = loss * weight_gathered

            if reduction == 'mean':
                result = np.mean(loss)
            elif reduction == 'sum':
                result = np.sum(loss)
            elif reduction == 'none':
                result = loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

            loss_tensor = WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=log_probs.dtype,
                                      requires_grad=input_tensor.requires_grad)

            # Set up backward function for cross entropy loss
            if input_tensor.requires_grad:
                def cross_entropy_backward(grad_output):
                    # Gradient: softmax(input) - one_hot(target)
                    # First compute softmax
                    if input_tensor._data.ndim == 1:
                        max_val = np.max(input_tensor._data)
                        exp_vals = np.exp(input_tensor._data - max_val)
                        softmax = exp_vals / np.sum(exp_vals)

                        # Create one-hot encoding
                        one_hot = np.zeros_like(softmax)
                        one_hot[int(target_data)] = 1.0

                        grad_input = softmax - one_hot

                        # Apply weight if provided
                        if weight is not None:
                            weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                            grad_input = grad_input * weight_data[int(target_data)]
                    else:
                        # Batch processing
                        max_vals = np.max(input_tensor._data, axis=-1, keepdims=True)
                        exp_vals = np.exp(input_tensor._data - max_vals)
                        softmax = exp_vals / np.sum(exp_vals, axis=-1, keepdims=True)

                        # Create one-hot encoding
                        batch_size, num_classes = input_tensor._data.shape
                        one_hot = np.zeros_like(softmax)
                        target_indices_int = target_data.astype(int) if hasattr(target_data, 'astype') else int(target_data)
                        if np.isscalar(target_indices_int):
                            one_hot[0, target_indices_int] = 1.0
                        else:
                            one_hot[np.arange(batch_size), target_indices_int] = 1.0

                        grad_input = softmax - one_hot

                        # Apply weight if provided
                        if weight is not None:
                            weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                            weight_gathered = np.array([weight_data[target_indices_int[i]] for i in range(batch_size)])
                            grad_input = grad_input * weight_gathered[:, np.newaxis]

                    # Apply reduction factor
                    if reduction == 'mean':
                        grad_input = grad_input / (grad_input.shape[0] if grad_input.ndim > 1 else 1)

                    # Multiply by upstream gradient
                    if isinstance(grad_output._data, np.ndarray):
                        grad_input = grad_input * grad_output._data
                    else:
                        grad_input = grad_input * float(grad_output._data)

                    # Accumulate gradient to input tensor
                    if input_tensor.grad is None:
                        input_tensor.grad = WebGPUTensor(grad_input, device="webgpu", dtype=input_tensor.dtype, _internal=True)
                    else:
                        input_tensor.grad._data += grad_input

                loss_tensor._backward_fn = cross_entropy_backward
                loss_tensor._inputs = [input_tensor]

            return loss_tensor
        else:
            # NumPy path
            if log_probs.ndim == 1:
                loss = -log_probs[int(target)]
            else:
                batch_size = log_probs.shape[0]
                target_indices = target.astype(int) if hasattr(target, 'astype') else int(target)

                if np.isscalar(target_indices):
                    loss = -log_probs[0, target_indices]
                else:
                    losses = np.array([log_probs[i, target_indices[i]] for i in range(batch_size)])
                    loss = -losses

            if reduction == 'mean':
                return np.mean(loss)
            elif reduction == 'sum':
                return np.sum(loss)
            elif reduction == 'none':
                return loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

    @staticmethod
    def nll_loss(input_tensor, target, weight=None, reduction='mean'):
        """Negative Log Likelihood loss

        Args:
            input_tensor: log-probabilities, shape (N, C) or (C,)
            target: class indices, shape (N,) or scalar
            weight: optional class weights, shape (C,)
            reduction: 'mean', 'sum', or 'none'
        """
        if isinstance(input_tensor, WebGPUTensor):
            log_probs_data = input_tensor._data
            target_data = target._data if isinstance(target, WebGPUTensor) else target

            # Handle both 1D and 2D inputs
            if log_probs_data.ndim == 1:
                # Single sample
                loss = -log_probs_data[int(target_data)]
                if weight is not None:
                    weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                    loss = loss * weight_data[int(target_data)]
            else:
                # Batch of samples
                batch_size = log_probs_data.shape[0]
                target_indices = target_data.astype(int) if hasattr(target_data, 'astype') else int(target_data)

                if np.isscalar(target_indices):
                    loss = -log_probs_data[0, target_indices]
                    if weight is not None:
                        weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                        loss = loss * weight_data[target_indices]
                else:
                    # Gather losses for each sample
                    losses = np.array([log_probs_data[i, target_indices[i]] for i in range(batch_size)])
                    loss = -losses

                    if weight is not None:
                        weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                        weight_gathered = np.array([weight_data[target_indices[i]] for i in range(batch_size)])
                        loss = loss * weight_gathered

            if reduction == 'mean':
                result = np.mean(loss)
            elif reduction == 'sum':
                result = np.sum(loss)
            elif reduction == 'none':
                result = loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

            loss_tensor = WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=input_tensor.dtype,
                                      requires_grad=input_tensor.requires_grad)

            # Set up backward function for NLL loss
            if input_tensor.requires_grad:
                def nll_backward(grad_output):
                    # Gradient: -1 at the target indices (with optional weighting)
                    grad_input = np.zeros_like(log_probs_data)

                    if log_probs_data.ndim == 1:
                        grad_input[int(target_data)] = -1.0
                        if weight is not None:
                            weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                            grad_input[int(target_data)] *= weight_data[int(target_data)]
                    else:
                        batch_size = log_probs_data.shape[0]
                        target_indices = target_data.astype(int) if hasattr(target_data, 'astype') else int(target_data)

                        if np.isscalar(target_indices):
                            grad_input[0, target_indices] = -1.0
                            if weight is not None:
                                weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                                grad_input[0, target_indices] *= weight_data[target_indices]
                        else:
                            for i in range(batch_size):
                                grad_input[i, target_indices[i]] = -1.0
                                if weight is not None:
                                    weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                                    grad_input[i, target_indices[i]] *= weight_data[target_indices[i]]

                    # Apply reduction factor
                    if reduction == 'mean':
                        grad_input = grad_input / (batch_size if log_probs_data.ndim > 1 else 1)

                    # Multiply by upstream gradient
                    if isinstance(grad_output._data, np.ndarray):
                        grad_input = grad_input * grad_output._data
                    else:
                        grad_input = grad_input * float(grad_output._data)

                    # Accumulate gradient to input tensor
                    if input_tensor.grad is None:
                        input_tensor.grad = WebGPUTensor(grad_input, device="webgpu", dtype=input_tensor.dtype, _internal=True)
                    else:
                        input_tensor.grad._data += grad_input

                loss_tensor._backward_fn = nll_backward
                loss_tensor._inputs = [input_tensor]

            return loss_tensor
        else:
            # NumPy path
            log_probs = input_tensor
            if log_probs.ndim == 1:
                loss = -log_probs[int(target)]
                if weight is not None:
                    loss = loss * weight[int(target)]
            else:
                batch_size = log_probs.shape[0]
                target_indices = target.astype(int) if hasattr(target, 'astype') else int(target)

                if np.isscalar(target_indices):
                    loss = -log_probs[0, target_indices]
                    if weight is not None:
                        loss = loss * weight[target_indices]
                else:
                    losses = np.array([log_probs[i, target_indices[i]] for i in range(batch_size)])
                    loss = -losses
                    if weight is not None:
                        weight_gathered = np.array([weight[target_indices[i]] for i in range(batch_size)])
                        loss = loss * weight_gathered

            if reduction == 'mean':
                return np.mean(loss)
            elif reduction == 'sum':
                return np.sum(loss)
            elif reduction == 'none':
                return loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

    @staticmethod
    def binary_cross_entropy(input_tensor, target, weight=None, reduction='mean'):
        """Binary Cross Entropy loss

        Args:
            input_tensor: predictions (must be in [0, 1])
            target: target values (0 or 1)
            weight: optional element-wise weights
            reduction: 'mean', 'sum', or 'none'
        """
        if isinstance(input_tensor, WebGPUTensor):
            inp = input_tensor._data
            tgt = target._data if isinstance(target, WebGPUTensor) else target

            # Clamp to avoid log(0)
            inp = np.clip(inp, 1e-7, 1 - 1e-7)

            # BCE formula: -[y*log(x) + (1-y)*log(1-x)]
            loss = -(tgt * np.log(inp) + (1 - tgt) * np.log(1 - inp))

            if weight is not None:
                weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                loss = loss * weight_data

            if reduction == 'mean':
                result = np.mean(loss)
            elif reduction == 'sum':
                result = np.sum(loss)
            elif reduction == 'none':
                result = loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

            return WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=input_tensor.dtype)
        else:
            inp = np.clip(input_tensor, 1e-7, 1 - 1e-7)
            loss = -(target * np.log(inp) + (1 - target) * np.log(1 - inp))

            if weight is not None:
                loss = loss * weight

            if reduction == 'mean':
                return np.mean(loss)
            elif reduction == 'sum':
                return np.sum(loss)
            elif reduction == 'none':
                return loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

    @staticmethod
    def binary_cross_entropy_with_logits(input_tensor, target, weight=None, reduction='mean'):
        """Binary Cross Entropy with Logits loss (more numerically stable)

        Args:
            input_tensor: raw logits (before sigmoid)
            target: target values (0 or 1)
            weight: optional element-wise weights
            reduction: 'mean', 'sum', or 'none'
        """
        if isinstance(input_tensor, WebGPUTensor):
            inp = input_tensor._data
            tgt = target._data if isinstance(target, WebGPUTensor) else target

            # Numerically stable formula: max(x,0) - x*y + log(1 + exp(-|x|))
            max_val = np.maximum(inp, 0)
            loss = max_val - inp * tgt + np.log(1 + np.exp(-np.abs(inp)))

            if weight is not None:
                weight_data = weight._data if isinstance(weight, WebGPUTensor) else weight
                loss = loss * weight_data

            if reduction == 'mean':
                result = np.mean(loss)
            elif reduction == 'sum':
                result = np.sum(loss)
            elif reduction == 'none':
                result = loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

            return WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=input_tensor.dtype)
        else:
            max_val = np.maximum(input_tensor, 0)
            loss = max_val - input_tensor * target + np.log(1 + np.exp(-np.abs(input_tensor)))

            if weight is not None:
                loss = loss * weight

            if reduction == 'mean':
                return np.mean(loss)
            elif reduction == 'sum':
                return np.sum(loss)
            elif reduction == 'none':
                return loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

    @staticmethod
    def l1_loss(input_tensor, target, reduction='mean'):
        """L1 Loss (Mean Absolute Error)

        Args:
            input_tensor: predictions
            target: target values
            reduction: 'mean', 'sum', or 'none'
        """
        if isinstance(input_tensor, WebGPUTensor):
            inp = input_tensor._data
            tgt = target._data if isinstance(target, WebGPUTensor) else target
            loss = np.abs(inp - tgt)

            if reduction == 'mean':
                result = np.mean(loss)
            elif reduction == 'sum':
                result = np.sum(loss)
            elif reduction == 'none':
                result = loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

            return WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=input_tensor.dtype)
        else:
            loss = np.abs(input_tensor - target)

            if reduction == 'mean':
                return np.mean(loss)
            elif reduction == 'sum':
                return np.sum(loss)
            elif reduction == 'none':
                return loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

    @staticmethod
    def smooth_l1_loss(input_tensor, target, beta=1.0, reduction='mean'):
        """Smooth L1 Loss (Huber Loss)

        Args:
            input_tensor: predictions
            target: target values
            beta: threshold for switching between L1 and L2
            reduction: 'mean', 'sum', or 'none'
        """
        if isinstance(input_tensor, WebGPUTensor):
            inp = input_tensor._data
            tgt = target._data if isinstance(target, WebGPUTensor) else target
            diff = np.abs(inp - tgt)

            # Smooth L1: 0.5 * diff^2 / beta if |diff| < beta, else |diff| - 0.5 * beta
            loss = np.where(diff < beta, 0.5 * diff ** 2 / beta, diff - 0.5 * beta)

            if reduction == 'mean':
                result = np.mean(loss)
            elif reduction == 'sum':
                result = np.sum(loss)
            elif reduction == 'none':
                result = loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

            return WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=input_tensor.dtype)
        else:
            diff = np.abs(input_tensor - target)
            loss = np.where(diff < beta, 0.5 * diff ** 2 / beta, diff - 0.5 * beta)

            if reduction == 'mean':
                return np.mean(loss)
            elif reduction == 'sum':
                return np.sum(loss)
            elif reduction == 'none':
                return loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

    @staticmethod
    def kl_div(input_tensor, target, reduction='mean'):
        """KL Divergence loss

        Args:
            input_tensor: log probabilities
            target: target probabilities
            reduction: 'mean', 'sum', 'batchmean', or 'none'
        """
        if isinstance(input_tensor, WebGPUTensor):
            inp = input_tensor._data  # log probabilities
            tgt = target._data if isinstance(target, WebGPUTensor) else target  # probabilities

            # KL(P||Q) = sum(P * (log(P) - log(Q)))
            # Since input is already log(Q), we have: sum(P * (log(P) - input))
            loss = tgt * (np.log(np.clip(tgt, 1e-10, None)) - inp)

            if reduction == 'mean':
                result = np.mean(loss)
            elif reduction == 'sum':
                result = np.sum(loss)
            elif reduction == 'batchmean':
                # Average over batch dimension
                result = np.sum(loss) / (loss.shape[0] if loss.ndim > 1 else 1)
            elif reduction == 'none':
                result = loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

            return WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=input_tensor.dtype)
        else:
            loss = target * (np.log(np.clip(target, 1e-10, None)) - input_tensor)

            if reduction == 'mean':
                return np.mean(loss)
            elif reduction == 'sum':
                return np.sum(loss)
            elif reduction == 'batchmean':
                return np.sum(loss) / (loss.shape[0] if loss.ndim > 1 else 1)
            elif reduction == 'none':
                return loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

    @staticmethod
    def cosine_embedding_loss(input1, input2, target, margin=0.0, reduction='mean'):
        """Cosine Embedding Loss

        Args:
            input1: first input tensor
            input2: second input tensor
            target: 1 for similar pairs, -1 for dissimilar pairs
            margin: margin for dissimilar pairs
            reduction: 'mean', 'sum', or 'none'
        """
        if isinstance(input1, WebGPUTensor):
            inp1 = input1._data
            inp2 = input2._data if isinstance(input2, WebGPUTensor) else input2
            tgt = target._data if isinstance(target, WebGPUTensor) else target

            # Compute cosine similarity
            dot_product = np.sum(inp1 * inp2, axis=-1)
            norm1 = np.sqrt(np.sum(inp1 ** 2, axis=-1))
            norm2 = np.sqrt(np.sum(inp2 ** 2, axis=-1))
            cos_sim = dot_product / (norm1 * norm2 + 1e-8)

            # Loss: 1 - cos_sim if target=1, max(0, cos_sim - margin) if target=-1
            loss = np.where(tgt == 1, 1 - cos_sim, np.maximum(0, cos_sim - margin))

            if reduction == 'mean':
                result = np.mean(loss)
            elif reduction == 'sum':
                result = np.sum(loss)
            elif reduction == 'none':
                result = loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

            return WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=input1.dtype)
        else:
            dot_product = np.sum(input1 * input2, axis=-1)
            norm1 = np.sqrt(np.sum(input1 ** 2, axis=-1))
            norm2 = np.sqrt(np.sum(input2 ** 2, axis=-1))
            cos_sim = dot_product / (norm1 * norm2 + 1e-8)

            loss = np.where(target == 1, 1 - cos_sim, np.maximum(0, cos_sim - margin))

            if reduction == 'mean':
                return np.mean(loss)
            elif reduction == 'sum':
                return np.sum(loss)
            elif reduction == 'none':
                return loss
            else:
                raise ValueError(f"Invalid reduction mode: {reduction}")

# Parameter wrapper for tensors
class TorchNNParameter:
    """A kind of Tensor that is to be considered a module parameter.

    Parameters are Tensor subclasses that have a very special property when used with
    Module s - when they're assigned as Module attributes they are automatically added
    to the list of its parameters, and will appear in parameters() iterator.
    """
    def __init__(self, data=None, requires_grad=True, **kwargs):
        if data is None:
            raise ValueError("Parameter data cannot be None")

        # If data is already a tensor, use it; otherwise create a tensor
        if isinstance(data, WebGPUTensor):
            self.data = data
            # Override requires_grad if specified
            if hasattr(self.data, 'requires_grad'):
                self.data.requires_grad = requires_grad
        else:
            # Convert to tensor
            import torch as torch_module
            self.data = torch_module.tensor(data, requires_grad=requires_grad)

        # Mark this as a parameter
        self.is_parameter = True
        self.requires_grad = requires_grad

    def __repr__(self):
        return f"Parameter containing:\\n{self.data.__repr__()}"

    # Note: No __hash__ or __eq__ override - uses default identity-based
    # This allows parameters to be used as dict keys in optimizer state

    def __getattr__(self, name):
        # Delegate attribute access to the underlying tensor
        if name in ['data', 'is_parameter', 'requires_grad']:
            return object.__getattribute__(self, name)
        return getattr(self.data, name)

    @property
    def shape(self):
        return self.data.shape

    @property
    def dtype(self):
        return self.data.dtype

    @property
    def device(self):
        return self.data.device

    @property
    def grad(self):
        return getattr(self.data, 'grad', None)

    @grad.setter
    def grad(self, value):
        self.data.grad = value

    def backward(self, gradient=None):
        """Backward pass for parameter"""
        if hasattr(self.data, 'backward'):
            self.data.backward(gradient)

    def zero_grad(self):
        """Zero out the gradient"""
        if hasattr(self.data, 'grad') and self.data.grad is not None:
            self.data.grad = None

    def numpy(self):
        """Convert to numpy array"""
        return self.data.numpy() if hasattr(self.data, 'numpy') else self.data._data

    def tolist(self):
        """Convert to Python list"""
        return self.data.tolist() if hasattr(self.data, 'tolist') else self.data._data.tolist()

    def item(self):
        """Get scalar value"""
        return self.data.item() if hasattr(self.data, 'item') else self.data._data.item()

    def to(self, device):
        """Move parameter to device"""
        if hasattr(self.data, 'to'):
            self.data = self.data.to(device)
        return self

    def clone(self):
        """Clone the parameter"""
        cloned_data = self.data.clone() if hasattr(self.data, 'clone') else self.data
        return TorchNNParameter(cloned_data, requires_grad=self.requires_grad)

    def detach(self):
        """Detach from computation graph"""
        if hasattr(self.data, 'detach'):
            return self.data.detach()
        return self.data

# Neural network modules
class TorchNNModule:
    def __init__(self, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True
        
    def parameters(self):
        params = []
        debug_id = getattr(self, '_debug_id', 'UNKNOWN')
        class_name = self.__class__.__name__
        for param in self._parameters.values():
            params.append(param)
        for module_name, module in self._modules.items():
            if hasattr(module, 'parameters'):
                subparams = module.parameters()
                params.extend(subparams)
        return params

    def named_parameters(self, prefix=''):
        """Returns an iterator over module parameters, yielding (name, parameter) tuples.

        Args:
            prefix: Prefix to prepend to all parameter names

        Yields:
            (str, WebGPUTensor): Tuples of parameter name and parameter tensor
        """
        # Yield parameters from this module
        for name, param in self._parameters.items():
            if param is not None:
                full_name = prefix + name if prefix else name
                yield (full_name, param)

        # Recursively yield parameters from submodules
        for module_name, module in self._modules.items():
            if module is not None and hasattr(module, 'named_parameters'):
                submodule_prefix = prefix + module_name + '.' if prefix else module_name + '.'
                for name, param in module.named_parameters(prefix=submodule_prefix):
                    yield (name, param)

    def modules(self):
        \"\"\"Returns an iterator over all modules in the network.

        Yields:
            TorchNNModule: Module in the network (including self and all descendants)
        \"\"\"
        # Yield self first
        yield self

        # Recursively yield all submodules
        for name, module in self._modules.items():
            if module is not None:
                if hasattr(module, 'modules'):
                    # If it's a TorchNNModule, recursively get its modules
                    for submodule in module.modules():
                        yield submodule
                else:
                    # If it's a simple module without modules() method, just yield it
                    yield module

    def __setattr__(self, name, value):
        """Override setattr to automatically register modules and parameters"""
        # Check if the value is a TorchNNModule (submodule)
        if isinstance(value, TorchNNModule):
            if hasattr(self, '_modules'):
                self._modules[name] = value

        # Check if the value is a WebGPUTensor with requires_grad (parameter)
        elif isinstance(value, WebGPUTensor) and getattr(value, 'requires_grad', False):
            if hasattr(self, '_parameters'):
                self._parameters[name] = value

        # Default setattr behavior
        object.__setattr__(self, name, value)

    def __call__(self, *args, **kwargs):
        # Call forward pre-hooks
        for hook in self._forward_pre_hooks.values():
            hook(self, args)

        # Call forward method
        result = self.forward(*args, **kwargs)

        # Call forward hooks
        for hook in self._forward_hooks.values():
            hook_result = hook(self, args, result)
            if hook_result is not None:
                result = hook_result

        return result

    def forward(self, x):
        raise NotImplementedError

    def register_forward_hook(self, hook):
        """Register a forward hook on the module.

        The hook will be called every time after forward() has computed an output.
        It should have the following signature:
            hook(module, input, output) -> None or modified output

        Args:
            hook: A function that takes (module, input, output) as arguments

        Returns:
            A handle that can be used to remove the hook by calling handle.remove()
        """
        handle_id = len(self._forward_hooks)
        self._forward_hooks[handle_id] = hook

        # Create a handle for removing the hook
        class HookHandle:
            def __init__(self, hooks_dict, hook_id, **kwargs):
                self.hooks_dict = hooks_dict
                self.hook_id = hook_id

            def remove(self):
                if self.hook_id in self.hooks_dict:
                    del self.hooks_dict[self.hook_id]

        return HookHandle(self._forward_hooks, handle_id)

    def to(self, device):
        """Move module to specified device"""
        # Convert device string to device object if needed
        if isinstance(device, str):
            if device == 'cuda':
                device = 'webgpu'  # Map CUDA to WebGPU
            target_device = device
        else:
            target_device = device.type if hasattr(device, 'type') else str(device)

        # Move all parameters to the new device
        for param in self.parameters():
            # CRITICAL: Actually allocate GPU buffers, not just metadata!
            if target_device == 'webgpu' and '__webgpu_allocate__' in globals():
                # Allocate GPU buffer for this parameter
                param.device = WebGPUDevice(target_device)
                if not hasattr(param, '_gpu_buffer_id') or param._gpu_buffer_id is None:
                    try:
                        param._gpu_buffer_id = __webgpu_allocate__(
                            param._data.tolist(),
                            list(param.shape),
                            param.dtype
                        )
                    except Exception as e:
                        pass  # Failed to allocate GPU buffer
            else:
                param.device = target_device

        # Recursively move submodules
        for module in self._modules.values():
            if hasattr(module, 'to'):
                module.to(target_device)

        return self

    def cpu(self):
        """Move module to CPU"""
        return self.to('cpu')

    def cuda(self):
        """Move module to WebGPU (CUDA equivalent)"""
        return self.to('webgpu')

    def train(self, mode=True):
        """Set the module in training mode"""
        self.training = mode
        # Recursively set training mode for all submodules
        for module in self._modules.values():
            if hasattr(module, 'train'):
                module.train(mode)
        return self

    def eval(self):
        """Set the module in evaluation mode"""
        return self.train(False)

    def state_dict(self, destination=None, prefix=''):
        """Returns a dictionary containing all parameters and buffers.

        Args:
            destination: Optional dict to store state into
            prefix: Prefix for parameter names (used for nested modules)

        Returns:
            Dictionary mapping parameter names to their tensor data
        """
        if destination is None:
            destination = {}

        # Add parameters from this module
        for name, param in self._parameters.items():
            if param is not None:
                key = prefix + name
                # Store as numpy array for serialization
                destination[key] = param._data.copy()

        # Add buffers from this module
        for name, buf in self._buffers.items():
            if buf is not None and name not in self._non_persistent_buffers_set:
                key = prefix + name
                destination[key] = buf._data.copy() if hasattr(buf, '_data') else buf.copy()

        # Recursively add state from submodules
        for name, module in self._modules.items():
            if module is not None and hasattr(module, 'state_dict'):
                module.state_dict(destination, prefix + name + '.')

        return destination

    def load_state_dict(self, state_dict, strict=True):
        """Loads parameters and buffers from state_dict.

        Args:
            state_dict: Dictionary containing parameters and buffers
            strict: If True, requires exact match of keys. If False, allows partial loading.

        Returns:
            NamedTuple with missing_keys and unexpected_keys lists
        """
        missing_keys = []
        unexpected_keys = list(state_dict.keys())

        # Helper function to load state for a module with prefix
        def load_module_state(module, prefix=''):
            # Load parameters
            for name, param in module._parameters.items():
                if param is not None:
                    key = prefix + name
                    if key in state_dict:
                        # Update parameter data
                        param._data = state_dict[key].copy()
                        if key in unexpected_keys:
                            unexpected_keys.remove(key)
                    elif strict:
                        missing_keys.append(key)

            # Load buffers
            for name, buf in module._buffers.items():
                if buf is not None and name not in module._non_persistent_buffers_set:
                    key = prefix + name
                    if key in state_dict:
                        if hasattr(buf, '_data'):
                            buf._data = state_dict[key].copy()
                        else:
                            module._buffers[name] = state_dict[key].copy()
                        if key in unexpected_keys:
                            unexpected_keys.remove(key)
                    elif strict:
                        missing_keys.append(key)

            # Recursively load submodules
            for name, submodule in module._modules.items():
                if submodule is not None:
                    load_module_state(submodule, prefix + name + '.')

        # Load state starting from this module
        load_module_state(self, '')

        # Return information about missing and unexpected keys
        class LoadResult:
            def __init__(self, missing, unexpected, **kwargs):
                self.missing_keys = missing
                self.unexpected_keys = unexpected

        if strict and (missing_keys or unexpected_keys):
            error_msgs = []
            if missing_keys:
                error_msgs.append(f"Missing keys in state_dict: {missing_keys}")
            if unexpected_keys:
                error_msgs.append(f"Unexpected keys in state_dict: {unexpected_keys}")
            raise RuntimeError("Error(s) loading state_dict: " + " ".join(error_msgs))

        return LoadResult(missing_keys, unexpected_keys)

class TorchNNLinear(TorchNNModule):
    def __init__(self, in_features, out_features, bias=True, **kwargs):
        import random
        self._debug_id = random.randint(1000, 9999)
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True
        self.in_features = in_features
        self.out_features = out_features

        weight_data = np.random.randn(out_features, in_features) * np.sqrt(2.0 / in_features)
        self.weight = WebGPUTensor(weight_data, requires_grad=True)
        self._parameters['weight'] = self.weight

        if bias:
            bias_data = np.zeros(out_features)
            self.bias = WebGPUTensor(bias_data, requires_grad=True)
            self._parameters['bias'] = self.bias
        else:
            self.bias = None

        # Cache for transposed weight GPU buffer (avoid re-uploading every forward pass)
        self._weight_t_gpu_buffer_id = None
        self._weight_t_data = None

    def forward(self, x):
        if isinstance(x, WebGPUTensor):
            # Linear transformation: y = xW^T + b
            # GPU ACCELERATED with cached transpose: Only upload W^T once, reuse for all forward passes

            # Check if we can use GPU path
            weight_on_gpu = hasattr(self.weight, '_gpu_buffer_id') and self.weight._gpu_buffer_id is not None
            input_on_gpu = hasattr(x, '_gpu_buffer_id') and x._gpu_buffer_id is not None

            if weight_on_gpu and input_on_gpu and '__webgpu_allocate__' in globals():
                # GPU path with cached transpose
                # Check if we need to create/update cached transpose
                if self._weight_t_gpu_buffer_id is None:
                    # First time: create and upload transposed weight
                    weight_t_data = self.weight._data.T
                    self._weight_t_data = weight_t_data
                    self._weight_t_gpu_buffer_id = __webgpu_allocate__(
                        weight_t_data.tolist(),
                        list(weight_t_data.shape),
                        self.weight.dtype
                    )

                # Create temporary WebGPUTensor for transposed weight
                weight_t = WebGPUTensor(self._weight_t_data, device="webgpu", dtype=self.weight.dtype, _internal=True)
                weight_t._gpu_buffer_id = self._weight_t_gpu_buffer_id

                # GPU matmul: x @ W^T
                result = x @ weight_t

                if self.bias is not None:
                    # Add bias (CPU for now)
                    result_data = result._data + self.bias._data
                    result = WebGPUTensor(result_data, device="webgpu", dtype=x.dtype,
                                        requires_grad=result.requires_grad, _internal=True)
            else:
                # Fallback: CPU path
                result_data = np.dot(x._data, self.weight._data.T)
                if self.bias is not None:
                    result_data = result_data + self.bias._data
                result = WebGPUTensor(result_data, device="webgpu", dtype=x.dtype,
                                    requires_grad=(x.requires_grad or self.weight.requires_grad or
                                                 (self.bias is not None and self.bias.requires_grad)), _internal=True)

            # Set up backward function for linear layer
            def linear_backward(grad_output):

                if x.requires_grad:
                    # Gradient w.r.t input: grad_output @ weight
                    if x.grad is None:
                        x.grad = WebGPUTensor(np.zeros_like(x._data), device="webgpu", dtype=x.dtype)
                    x.grad._data += np.dot(grad_output._data, self.weight._data)

                if self.weight.requires_grad:
                    # Gradient w.r.t weight: x^T @ grad_output
                    if self.weight.grad is None:
                        self.weight.grad = WebGPUTensor(np.zeros_like(self.weight._data), device="webgpu", dtype=self.weight.dtype)
                    weight_grad = np.dot(grad_output._data.T, x._data)
                    self.weight.grad._data += weight_grad

                if self.bias is not None and self.bias.requires_grad:
                    # Gradient w.r.t bias: sum(grad_output, axis=0)
                    if self.bias.grad is None:
                        self.bias.grad = WebGPUTensor(np.zeros_like(self.bias._data), device="webgpu", dtype=self.bias.dtype)
                    bias_grad = np.sum(grad_output._data, axis=0)
                    self.bias.grad._data += bias_grad

            result._backward_fn = linear_backward
            result._inputs = [x, self.weight] + ([self.bias] if self.bias else [])

            return result
        else:
            raise TypeError("Input must be WebGPUTensor")

class TorchNNReLU(TorchNNModule):
    def __init__(self, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

    def forward(self, x):
        if isinstance(x, WebGPUTensor):
            result_data = np.maximum(x._data, 0)
            result = WebGPUTensor(result_data, device="webgpu", dtype=x.dtype, requires_grad=x.requires_grad, _internal=True)

            # Set up backward function for ReLU
            if x.requires_grad:
                def relu_backward(grad_output):
                    # Gradient: 1 if x > 0, else 0
                    grad_input = grad_output._data * (x._data > 0)
                    if x.grad is None:
                        x.grad = WebGPUTensor(np.zeros_like(x._data), device="webgpu", dtype=x.dtype, _internal=True)
                    x.grad._data += grad_input

                result._backward_fn = relu_backward
                result._inputs = [x]

            return result
        else:
            return np.maximum(x, 0)

class TorchNNSigmoid(TorchNNModule):
    """Sigmoid activation layer"""
    def __init__(self, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

    def forward(self, x):
        """Apply sigmoid: (x) = 1 / (1 + exp(-x))"""
        if isinstance(x, WebGPUTensor):
            result_data = 1 / (1 + np.exp(-x._data))
            result = WebGPUTensor(result_data, device="webgpu", dtype=x.dtype, requires_grad=x.requires_grad)

            # Set up backward function for autograd
            if x.requires_grad:
                def sigmoid_backward(grad_output):
                    # Gradient: '(x) = (x) * (1 - (x))
                    sigmoid_val = result_data
                    grad_input = grad_output._data * sigmoid_val * (1 - sigmoid_val)
                    grad_tensor = WebGPUTensor(grad_input, device="webgpu", dtype=x.dtype, _internal=True)
                    x.backward(grad_tensor)

                result._backward_fn = sigmoid_backward
                result._inputs = [x]

            return result
        else:
            return 1 / (1 + np.exp(-x))

class TorchNNTanh(TorchNNModule):
    """Tanh activation layer"""
    def __init__(self, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

    def forward(self, x):
        """Apply tanh: tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))"""
        if isinstance(x, WebGPUTensor):
            result_data = np.tanh(x._data)
            result = WebGPUTensor(result_data, device="webgpu", dtype=x.dtype, requires_grad=x.requires_grad)

            # Set up backward function for autograd
            if x.requires_grad:
                def tanh_backward(grad_output):
                    # Gradient: tanh'(x) = 1 - tanh(x)
                    tanh_val = result_data
                    grad_input = grad_output._data * (1 - tanh_val ** 2)
                    grad_tensor = WebGPUTensor(grad_input, device="webgpu", dtype=x.dtype, _internal=True)
                    x.backward(grad_tensor)

                result._backward_fn = tanh_backward
                result._inputs = [x]

            return result
        else:
            return np.tanh(x)

class TorchNNLeakyReLU(TorchNNModule):
    """Leaky ReLU activation layer

    LeakyReLU(x) = max(0, x) + negative_slope * min(0, x)
    """
    def __init__(self, negative_slope=0.01, inplace=False, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True
        self.negative_slope = negative_slope
        self.inplace = inplace

    def forward(self, x):
        """Apply Leaky ReLU"""
        if isinstance(x, WebGPUTensor):
            result_data = np.where(x._data > 0, x._data, self.negative_slope * x._data)
            result = WebGPUTensor(result_data, device="webgpu", dtype=x.dtype, requires_grad=x.requires_grad)

            # Set up backward function for autograd
            if x.requires_grad:
                def leaky_relu_backward(grad_output):
                    # Gradient: 1 if x > 0, negative_slope otherwise
                    grad_input = np.where(x._data > 0, grad_output._data, self.negative_slope * grad_output._data)
                    grad_tensor = WebGPUTensor(grad_input, device="webgpu", dtype=x.dtype, _internal=True)
                    x.backward(grad_tensor)

                result._backward_fn = leaky_relu_backward
                result._inputs = [x]

            return result
        else:
            return np.where(x > 0, x, self.negative_slope * x)

class TorchNNGELU(TorchNNModule):
    """Gaussian Error Linear Units (GELU) activation

    GELU(x) = x * (x) where (x) is the Gaussian CDF
    Approximation: GELU(x)  0.5 * x * (1 + tanh((2/) * (x + 0.044715 * x)))
    """
    def __init__(self, approximate='none', **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True
        self.approximate = approximate

    def forward(self, x):
        """Apply GELU activation"""
        if isinstance(x, WebGPUTensor):
            if self.approximate == 'tanh':
                # Fast approximation using tanh
                # GELU(x)  0.5 * x * (1 + tanh((2/) * (x + 0.044715 * x)))
                sqrt_2_over_pi = np.sqrt(2.0 / np.pi)
                inner = sqrt_2_over_pi * (x._data + 0.044715 * x._data ** 3)
                result_data = 0.5 * x._data * (1.0 + np.tanh(inner))
            else:
                # Accurate version using erf (error function)
                # GELU(x) = 0.5 * x * (1 + erf(x / 2))
                try:
                    from scipy.special import erf
                    result_data = 0.5 * x._data * (1.0 + erf(x._data / np.sqrt(2.0)))
                except ImportError:
                    # Fallback to tanh approximation if scipy not available
                    sqrt_2_over_pi = np.sqrt(2.0 / np.pi)
                    inner = sqrt_2_over_pi * (x._data + 0.044715 * x._data ** 3)
                    result_data = 0.5 * x._data * (1.0 + np.tanh(inner))

            result = WebGPUTensor(result_data, device="webgpu", dtype=x.dtype, requires_grad=x.requires_grad)

            # Set up backward function for autograd
            if x.requires_grad:
                def gelu_backward(grad_output):
                    # Simplified gradient (using tanh approximation)
                    sqrt_2_over_pi = np.sqrt(2.0 / np.pi)
                    x_cubed = x._data ** 3
                    inner = sqrt_2_over_pi * (x._data + 0.044715 * x_cubed)
                    tanh_inner = np.tanh(inner)
                    sech_squared = 1 - tanh_inner ** 2

                    grad_input = grad_output._data * (
                        0.5 * (1.0 + tanh_inner) +
                        0.5 * x._data * sech_squared * sqrt_2_over_pi * (1 + 3 * 0.044715 * x._data ** 2)
                    )
                    grad_tensor = WebGPUTensor(grad_input, device="webgpu", dtype=x.dtype, _internal=True)
                    x.backward(grad_tensor)

                result._backward_fn = gelu_backward
                result._inputs = [x]

            return result
        else:
            # NumPy fallback
            sqrt_2_over_pi = np.sqrt(2.0 / np.pi)
            inner = sqrt_2_over_pi * (x + 0.044715 * x ** 3)
            return 0.5 * x * (1.0 + np.tanh(inner))

class TorchNNSiLU(TorchNNModule):
    """Sigmoid-Weighted Linear Unit (SiLU) / Swish activation

    SiLU(x) = x * sigmoid(x) = x / (1 + exp(-x))
    Also known as Swish activation
    """
    def __init__(self, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

    def forward(self, x):
        """Apply SiLU/Swish: x * sigmoid(x)"""
        if isinstance(x, WebGPUTensor):
            sigmoid_x = 1.0 / (1.0 + np.exp(-x._data))
            result_data = x._data * sigmoid_x
            result = WebGPUTensor(result_data, device="webgpu", dtype=x.dtype, requires_grad=x.requires_grad)

            # Set up backward function for autograd
            if x.requires_grad:
                def silu_backward(grad_output):
                    # Gradient: sigmoid(x) + x * sigmoid(x) * (1 - sigmoid(x))
                    # Simplifies to: sigmoid(x) * (1 + x * (1 - sigmoid(x)))
                    grad_input = grad_output._data * (sigmoid_x + x._data * sigmoid_x * (1 - sigmoid_x))
                    grad_tensor = WebGPUTensor(grad_input, device="webgpu", dtype=x.dtype, _internal=True)
                    x.backward(grad_tensor)

                result._backward_fn = silu_backward
                result._inputs = [x]

            return result
        else:
            sigmoid_x = 1.0 / (1.0 + np.exp(-x))
            return x * sigmoid_x

class TorchNNDropout(TorchNNModule):
    """Dropout layer for regularization"""
    def __init__(self, p=0.5, inplace=False, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True
        self.p = p  # Dropout probability
        self.inplace = inplace

    def forward(self, x):
        """Apply dropout: randomly zero elements during training"""
        if isinstance(x, WebGPUTensor):
            # During evaluation, dropout is a no-op
            if not self.training:
                return x

            # During training, randomly drop elements
            if self.p == 0:
                return x

            # Generate dropout mask
            keep_prob = 1 - self.p
            mask = np.random.binomial(1, keep_prob, size=x.shape) / keep_prob

            # Apply mask and scale
            result_data = x._data * mask
            result = WebGPUTensor(result_data, device="webgpu", dtype=x.dtype, requires_grad=x.requires_grad)

            # Set up backward function for autograd
            if x.requires_grad:
                def dropout_backward(grad_output):
                    # Gradient flows through the same mask
                    grad_input = grad_output._data * mask
                    grad_tensor = WebGPUTensor(grad_input, device="webgpu", dtype=x.dtype, _internal=True)
                    x.backward(grad_tensor)

                result._backward_fn = dropout_backward
                result._inputs = [x]

            return result
        else:
            # NumPy fallback
            if not self.training or self.p == 0:
                return x
            keep_prob = 1 - self.p
            mask = np.random.binomial(1, keep_prob, size=x.shape) / keep_prob
            return x * mask

class TorchNNBatchNorm2d(TorchNNModule):
    """Batch Normalization for 2D spatial data (4D input: N x C x H x W)"""
    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats

        # Learnable affine parameters (gamma and beta)
        if self.affine:
            self.weight = WebGPUTensor(np.ones(num_features), requires_grad=True)
            self.bias = WebGPUTensor(np.zeros(num_features), requires_grad=True)
            self._parameters['weight'] = self.weight
            self._parameters['bias'] = self.bias
        else:
            self.weight = None
            self.bias = None

        # Running statistics (buffers, not parameters)
        if self.track_running_stats:
            self.running_mean = WebGPUTensor(np.zeros(num_features), requires_grad=False)
            self.running_var = WebGPUTensor(np.ones(num_features), requires_grad=False)
            self.num_batches_tracked = 0
            self._buffers['running_mean'] = self.running_mean
            self._buffers['running_var'] = self.running_var
        else:
            self.running_mean = None
            self.running_var = None
            self.num_batches_tracked = None

    def forward(self, x):
        """Apply batch normalization.

        Input shape: (N, C, H, W) where N=batch, C=channels, H=height, W=width
        """
        if not isinstance(x, WebGPUTensor):
            raise TypeError("Input must be WebGPUTensor")

        # Input validation: expect 4D tensor (N, C, H, W)
        if x.ndim != 4:
            raise ValueError(f"Expected 4D input (got {x.ndim}D input)")

        N, C, H, W = x.shape

        if C != self.num_features:
            raise ValueError(f"Expected {self.num_features} channels (got {C})")

        # Training mode: use batch statistics
        if self.training:
            # Compute mean and variance over batch and spatial dimensions (N, H, W)
            # Keep channel dimension separate
            # Reshape to (N*H*W, C) for easier computation
            x_reshaped = x._data.transpose(0, 2, 3, 1).reshape(-1, C)  # (N*H*W, C)

            # Compute batch statistics
            batch_mean = x_reshaped.mean(axis=0)  # (C,)
            batch_var = x_reshaped.var(axis=0)    # (C,)

            # Update running statistics
            if self.track_running_stats:
                self.num_batches_tracked += 1
                # Exponential moving average
                self.running_mean._data = (1 - self.momentum) * self.running_mean._data + self.momentum * batch_mean
                self.running_var._data = (1 - self.momentum) * self.running_var._data + self.momentum * batch_var

            # Use batch statistics for normalization
            mean = batch_mean
            var = batch_var

        # Evaluation mode: use running statistics
        else:
            if not self.track_running_stats:
                raise RuntimeError("Running statistics not tracked, cannot use eval mode")
            mean = self.running_mean._data
            var = self.running_var._data

        # Normalize: (x - mean) / sqrt(var + eps)
        # Reshape mean and var for broadcasting: (1, C, 1, 1)
        mean_broadcast = mean.reshape(1, C, 1, 1)
        var_broadcast = var.reshape(1, C, 1, 1)

        x_normalized = (x._data - mean_broadcast) / np.sqrt(var_broadcast + self.eps)

        # Apply affine transformation: gamma * x_norm + beta
        if self.affine:
            weight_broadcast = self.weight._data.reshape(1, C, 1, 1)
            bias_broadcast = self.bias._data.reshape(1, C, 1, 1)
            output_data = weight_broadcast * x_normalized + bias_broadcast
        else:
            output_data = x_normalized

        # Create output tensor
        result = WebGPUTensor(output_data, device="webgpu", dtype=x.dtype, requires_grad=x.requires_grad)

        # Set up backward function for autograd (simplified)
        if x.requires_grad and self.training:
            def batchnorm_backward(grad_output):
                # Simplified backward pass
                # Full implementation would compute gradients w.r.t input, weight, and bias
                # For now, just pass gradient through
                if x.grad is None:
                    x.grad = WebGPUTensor(np.zeros_like(x._data), device="webgpu", dtype=x.dtype)
                x.grad._data += grad_output._data

                # Gradient w.r.t weight and bias (if affine)
                if self.affine:
                    if self.weight.grad is None:
                        self.weight.grad = WebGPUTensor(np.zeros_like(self.weight._data), device="webgpu", dtype=self.weight.dtype)
                    if self.bias.grad is None:
                        self.bias.grad = WebGPUTensor(np.zeros_like(self.bias._data), device="webgpu", dtype=self.bias.dtype)

                    # Gradient w.r.t weight: sum over (N, H, W) of grad * x_normalized
                    grad_weight = (grad_output._data * x_normalized).transpose(0, 2, 3, 1).reshape(-1, C).sum(axis=0)
                    self.weight.grad._data += grad_weight

                    # Gradient w.r.t bias: sum over (N, H, W) of grad
                    grad_bias = grad_output._data.transpose(0, 2, 3, 1).reshape(-1, C).sum(axis=0)
                    self.bias.grad._data += grad_bias

            result._backward_fn = batchnorm_backward
            result._inputs = [x]

        return result

    def reset_running_stats(self):
        """Reset running statistics to initial values"""
        if self.track_running_stats:
            self.running_mean._data = np.zeros(self.num_features)
            self.running_var._data = np.ones(self.num_features)
            self.num_batches_tracked = 0

    def reset_parameters(self):
        """Reset learnable parameters to initial values"""
        self.reset_running_stats()
        if self.affine:
            self.weight._data = np.ones(self.num_features)
            self.bias._data = np.zeros(self.num_features)

class TorchNNConv2d(TorchNNModule):
    """2D Convolution Layer

    Args:
        in_channels: Number of input channels
        out_channels: Number of output channels (filters)
        kernel_size: Size of the convolution kernel (int or tuple)
        stride: Stride of the convolution (default: 1)
        padding: Zero-padding added to both sides (default: 0)
        dilation: Spacing between kernel elements (default: 1)
        groups: Number of blocked connections (default: 1)
        bias: If True, adds learnable bias (default: True)
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        self.in_channels = in_channels
        self.out_channels = out_channels

        # Handle kernel_size as int or tuple
        if isinstance(kernel_size, int):
            self.kernel_size = (kernel_size, kernel_size)
        else:
            self.kernel_size = kernel_size

        # Handle stride as int or tuple
        if isinstance(stride, int):
            self.stride = (stride, stride)
        else:
            self.stride = stride

        # Handle padding as int or tuple
        if isinstance(padding, int):
            self.padding = (padding, padding)
        else:
            self.padding = padding

        # Handle dilation as int or tuple
        if isinstance(dilation, int):
            self.dilation = (dilation, dilation)
        else:
            self.dilation = dilation

        self.groups = groups

        # Initialize weights with Kaiming/He initialization
        # Shape: (out_channels, in_channels, kernel_height, kernel_width)
        k = np.sqrt(1.0 / (in_channels * self.kernel_size[0] * self.kernel_size[1]))
        weight_data = np.random.uniform(-k, k, (out_channels, in_channels, self.kernel_size[0], self.kernel_size[1]))
        self.weight = WebGPUTensor(weight_data, requires_grad=True)
        self._parameters['weight'] = self.weight

        # Initialize bias
        if bias:
            bias_data = np.random.uniform(-k, k, (out_channels,))
            self.bias = WebGPUTensor(bias_data, requires_grad=True)
            self._parameters['bias'] = self.bias
        else:
            self.bias = None

    def forward(self, x):
        """Apply 2D convolution

        Input shape: (N, C_in, H_in, W_in)
        Output shape: (N, C_out, H_out, W_out)

        where:
            H_out = floor((H_in + 2*padding[0] - dilation[0]*(kernel_size[0]-1) - 1) / stride[0] + 1)
            W_out = floor((W_in + 2*padding[1] - dilation[1]*(kernel_size[1]-1) - 1) / stride[1] + 1)
        """
        if not isinstance(x, WebGPUTensor):
            raise TypeError("Input must be WebGPUTensor")

        if x.ndim != 4:
            raise ValueError(f"Expected 4D input (got {x.ndim}D input)")

        N, C_in, H_in, W_in = x.shape

        if C_in != self.in_channels:
            raise ValueError(f"Expected {self.in_channels} input channels (got {C_in})")

        # Apply padding
        if self.padding[0] > 0 or self.padding[1] > 0:
            x_padded = np.pad(x._data,
                            ((0, 0), (0, 0), (self.padding[0], self.padding[0]), (self.padding[1], self.padding[1])),
                            mode='constant', constant_values=0)
        else:
            x_padded = x._data

        # Compute output dimensions
        H_out = (x_padded.shape[2] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) // self.stride[0] + 1
        W_out = (x_padded.shape[3] - self.dilation[1] * (self.kernel_size[1] - 1) - 1) // self.stride[1] + 1

        # Initialize output
        output = np.zeros((N, self.out_channels, H_out, W_out))

        # Perform convolution
        for n in range(N):  # batch
            for c_out in range(self.out_channels):  # output channels
                for h_out in range(H_out):  # output height
                    for w_out in range(W_out):  # output width
                        # Calculate input region
                        h_start = h_out * self.stride[0]
                        w_start = w_out * self.stride[1]

                        # Convolve kernel with input region
                        sum_val = 0.0
                        for c_in in range(C_in):  # input channels
                            for kh in range(self.kernel_size[0]):  # kernel height
                                for kw in range(self.kernel_size[1]):  # kernel width
                                    h_idx = h_start + kh * self.dilation[0]
                                    w_idx = w_start + kw * self.dilation[1]

                                    sum_val += (x_padded[n, c_in, h_idx, w_idx] *
                                              self.weight._data[c_out, c_in, kh, kw])

                        # Add bias if present
                        if self.bias is not None:
                            sum_val += self.bias._data[c_out]

                        output[n, c_out, h_out, w_out] = sum_val

        # Create output tensor
        result = WebGPUTensor(output, device="webgpu", dtype=x.dtype,
                            requires_grad=(x.requires_grad or self.weight.requires_grad), _internal=True)

        # Set up backward function for autograd (simplified)
        if x.requires_grad or self.weight.requires_grad:
            def conv2d_backward(grad_output):
                # Gradient w.r.t input
                if x.requires_grad:
                    if x.grad is None:
                        x.grad = WebGPUTensor(np.zeros_like(x._data), device="webgpu", dtype=x.dtype)
                    # Simplified: just accumulate gradients (full implementation would do transposed convolution)
                    x.grad._data += grad_output._data.sum() * 0.001  # Placeholder

                # Gradient w.r.t weight
                if self.weight.requires_grad:
                    if self.weight.grad is None:
                        self.weight.grad = WebGPUTensor(np.zeros_like(self.weight._data), device="webgpu", dtype=self.weight.dtype)
                    # Simplified gradient accumulation
                    self.weight.grad._data += grad_output._data.sum() * 0.001  # Placeholder

                # Gradient w.r.t bias
                if self.bias is not None and self.bias.requires_grad:
                    if self.bias.grad is None:
                        self.bias.grad = WebGPUTensor(np.zeros_like(self.bias._data), device="webgpu", dtype=self.bias.dtype)
                    # Sum gradients over batch, height, width
                    grad_bias = grad_output._data.sum(axis=(0, 2, 3))
                    self.bias.grad._data += grad_bias

            result._backward_fn = conv2d_backward
            result._inputs = [x, self.weight]

        return result

class TorchNNMaxPool2d(TorchNNModule):
    """2D Max Pooling Layer

    Args:
        kernel_size: Size of the pooling window (int or tuple)
        stride: Stride of the pooling (default: kernel_size)
        padding: Zero-padding added to both sides (default: 0)
        dilation: Spacing between kernel elements (default: 1)
        return_indices: If True, returns indices along with outputs (default: False)
        ceil_mode: If True, use ceil instead of floor for output shape (default: False)
    """
    def __init__(self, kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        # Handle kernel_size as int or tuple
        if isinstance(kernel_size, int):
            self.kernel_size = (kernel_size, kernel_size)
        else:
            self.kernel_size = kernel_size

        # Handle stride (default to kernel_size if not specified)
        if stride is None:
            self.stride = self.kernel_size
        elif isinstance(stride, int):
            self.stride = (stride, stride)
        else:
            self.stride = stride

        # Handle padding as int or tuple
        if isinstance(padding, int):
            self.padding = (padding, padding)
        else:
            self.padding = padding

        # Handle dilation as int or tuple
        if isinstance(dilation, int):
            self.dilation = (dilation, dilation)
        else:
            self.dilation = dilation

        self.return_indices = return_indices
        self.ceil_mode = ceil_mode

    def forward(self, x):
        """Apply 2D max pooling

        Input shape: (N, C, H_in, W_in)
        Output shape: (N, C, H_out, W_out)
        """
        if not isinstance(x, WebGPUTensor):
            raise TypeError("Input must be WebGPUTensor")

        if x.ndim != 4:
            raise ValueError(f"Expected 4D input (got {x.ndim}D input)")

        N, C, H_in, W_in = x.shape

        # Apply padding
        if self.padding[0] > 0 or self.padding[1] > 0:
            x_padded = np.pad(x._data,
                            ((0, 0), (0, 0), (self.padding[0], self.padding[0]), (self.padding[1], self.padding[1])),
                            mode='constant', constant_values=-np.inf)
        else:
            x_padded = x._data

        # Compute output dimensions
        if self.ceil_mode:
            H_out = int(np.ceil((x_padded.shape[2] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) / self.stride[0] + 1))
            W_out = int(np.ceil((x_padded.shape[3] - self.dilation[1] * (self.kernel_size[1] - 1) - 1) / self.stride[1] + 1))
        else:
            H_out = (x_padded.shape[2] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) // self.stride[0] + 1
            W_out = (x_padded.shape[3] - self.dilation[1] * (self.kernel_size[1] - 1) - 1) // self.stride[1] + 1

        # Initialize output
        output = np.zeros((N, C, H_out, W_out))
        indices = np.zeros((N, C, H_out, W_out), dtype=np.int64) if self.return_indices else None

        # Perform max pooling
        for n in range(N):  # batch
            for c in range(C):  # channels
                for h_out in range(H_out):  # output height
                    for w_out in range(W_out):  # output width
                        # Calculate input region
                        h_start = h_out * self.stride[0]
                        w_start = w_out * self.stride[1]

                        # Find max value in pooling window
                        max_val = -np.inf
                        max_idx = 0

                        for kh in range(self.kernel_size[0]):  # kernel height
                            for kw in range(self.kernel_size[1]):  # kernel width
                                h_idx = h_start + kh * self.dilation[0]
                                w_idx = w_start + kw * self.dilation[1]

                                if h_idx < x_padded.shape[2] and w_idx < x_padded.shape[3]:
                                    val = x_padded[n, c, h_idx, w_idx]
                                    if val > max_val:
                                        max_val = val
                                        max_idx = h_idx * x_padded.shape[3] + w_idx

                        output[n, c, h_out, w_out] = max_val
                        if self.return_indices:
                            indices[n, c, h_out, w_out] = max_idx

        # Create output tensor
        result = WebGPUTensor(output, device="webgpu", dtype=x.dtype, requires_grad=x.requires_grad)

        # Set up backward function for autograd
        if x.requires_grad:
            def maxpool2d_backward(grad_output):
                if x.grad is None:
                    x.grad = WebGPUTensor(np.zeros_like(x._data), device="webgpu", dtype=x.dtype)
                # Simplified backward (full implementation would route gradients to max positions)
                x.grad._data += grad_output._data.sum() * 0.001  # Placeholder

            result._backward_fn = maxpool2d_backward
            result._inputs = [x]

        if self.return_indices:
            indices_tensor = WebGPUTensor(indices, device="webgpu", dtype='int64', _internal=True)
            return result, indices_tensor
        else:
            return result

class TorchNNAvgPool2d(TorchNNModule):
    """2D Average Pooling Layer

    Args:
        kernel_size: Size of the pooling window (int or tuple)
        stride: Stride of the pooling (default: kernel_size)
        padding: Zero-padding added to both sides (default: 0)
        ceil_mode: If True, use ceil instead of floor for output shape (default: False)
        count_include_pad: If True, include padding in average calculation (default: True)
    """
    def __init__(self, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        # Handle kernel_size as int or tuple
        if isinstance(kernel_size, int):
            self.kernel_size = (kernel_size, kernel_size)
        else:
            self.kernel_size = kernel_size

        # Handle stride (default to kernel_size if not specified)
        if stride is None:
            self.stride = self.kernel_size
        elif isinstance(stride, int):
            self.stride = (stride, stride)
        else:
            self.stride = stride

        # Handle padding as int or tuple
        if isinstance(padding, int):
            self.padding = (padding, padding)
        else:
            self.padding = padding

        self.ceil_mode = ceil_mode
        self.count_include_pad = count_include_pad

    def forward(self, x):
        """Apply 2D average pooling

        Input shape: (N, C, H_in, W_in)
        Output shape: (N, C, H_out, W_out)
        """
        if not isinstance(x, WebGPUTensor):
            raise TypeError("Input must be WebGPUTensor")

        if x.ndim != 4:
            raise ValueError(f"Expected 4D input (got {x.ndim}D input)")

        N, C, H_in, W_in = x.shape

        # Apply padding
        if self.padding[0] > 0 or self.padding[1] > 0:
            x_padded = np.pad(x._data,
                            ((0, 0), (0, 0), (self.padding[0], self.padding[0]), (self.padding[1], self.padding[1])),
                            mode='constant', constant_values=0)
        else:
            x_padded = x._data

        # Compute output dimensions
        if self.ceil_mode:
            H_out = int(np.ceil((x_padded.shape[2] - self.kernel_size[0]) / self.stride[0] + 1))
            W_out = int(np.ceil((x_padded.shape[3] - self.kernel_size[1]) / self.stride[1] + 1))
        else:
            H_out = (x_padded.shape[2] - self.kernel_size[0]) // self.stride[0] + 1
            W_out = (x_padded.shape[3] - self.kernel_size[1]) // self.stride[1] + 1

        # Initialize output
        output = np.zeros((N, C, H_out, W_out))

        # Perform average pooling
        for n in range(N):  # batch
            for c in range(C):  # channels
                for h_out in range(H_out):  # output height
                    for w_out in range(W_out):  # output width
                        # Calculate input region
                        h_start = h_out * self.stride[0]
                        w_start = w_out * self.stride[1]
                        h_end = min(h_start + self.kernel_size[0], x_padded.shape[2])
                        w_end = min(w_start + self.kernel_size[1], x_padded.shape[3])

                        # Extract pooling window
                        window = x_padded[n, c, h_start:h_end, w_start:w_end]

                        # Compute average
                        if self.count_include_pad:
                            # Include padding in count
                            divisor = self.kernel_size[0] * self.kernel_size[1]
                        else:
                            # Only count actual values
                            divisor = window.size

                        output[n, c, h_out, w_out] = window.sum() / divisor

        # Create output tensor
        result = WebGPUTensor(output, device="webgpu", dtype=x.dtype, requires_grad=x.requires_grad)

        # Set up backward function for autograd
        if x.requires_grad:
            def avgpool2d_backward(grad_output):
                if x.grad is None:
                    x.grad = WebGPUTensor(np.zeros_like(x._data), device="webgpu", dtype=x.dtype)
                # Simplified backward (full implementation would distribute gradients evenly)
                x.grad._data += grad_output._data.sum() * 0.001  # Placeholder

            result._backward_fn = avgpool2d_backward
            result._inputs = [x]

        return result

class TorchNNMSELoss(TorchNNModule):
    def __init__(self, reduction='mean', **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True
        self.reduction = reduction

    def forward(self, input_tensor, target_tensor):
        if isinstance(input_tensor, WebGPUTensor) and isinstance(target_tensor, WebGPUTensor):
            diff = input_tensor._data - target_tensor._data
            squared_error = diff ** 2

            if self.reduction == 'mean':
                loss_value = np.mean(squared_error)
            elif self.reduction == 'sum':
                loss_value = np.sum(squared_error)
            else:  # 'none'
                loss_value = squared_error

            # Create loss tensor - ensure it's always a numpy array
            if np.isscalar(loss_value):
                loss_data = np.array([loss_value])
            else:
                loss_data = loss_value

            loss_tensor = WebGPUTensor(loss_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=True, _internal=True)

            # Set up backward function for MSE loss
            def mse_backward(grad_output):
                # MSE gradient: 2 * (input - target) / N
                N = input_tensor._data.size if self.reduction == 'mean' else 1
                grad_input = 2.0 * diff / N

                if input_tensor.requires_grad:
                    # Accumulate gradient for the input
                    if input_tensor.grad is None:
                        input_tensor.grad = WebGPUTensor(np.zeros_like(input_tensor._data), device="webgpu", dtype=input_tensor.dtype, _internal=True)

                    # Multiply by incoming gradient from loss.backward()
                    grad_output_val = grad_output._data if hasattr(grad_output, '_data') else grad_output
                    input_tensor.grad._data += grad_input * grad_output_val

            loss_tensor._backward_fn = mse_backward
            loss_tensor._inputs = [input_tensor, target_tensor]

            return loss_tensor
        else:
            raise TypeError("Both input and target must be WebGPUTensor")

class TorchNNCrossEntropyLoss(TorchNNModule):
    def __init__(self, reduction='mean', **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True
        self.reduction = reduction

    def forward(self, input_tensor, target_tensor):
        if isinstance(input_tensor, WebGPUTensor) and isinstance(target_tensor, WebGPUTensor):
            # Softmax
            input_data = input_tensor._data
            exp_data = np.exp(input_data - np.max(input_data, axis=1, keepdims=True))
            softmax_data = exp_data / np.sum(exp_data, axis=1, keepdims=True)

            # Cross-entropy loss
            target_indices = target_tensor._data.astype(int)
            batch_size = input_data.shape[0]

            # Extract probabilities for target classes
            target_probs = softmax_data[np.arange(batch_size), target_indices]

            # Compute negative log likelihood
            nll = -np.log(np.clip(target_probs, 1e-8, 1.0))

            if self.reduction == 'mean':
                loss_value = np.mean(nll)
            elif self.reduction == 'sum':
                loss_value = np.sum(nll)
            else:  # 'none'
                loss_value = nll

            loss_tensor = WebGPUTensor([loss_value] if np.isscalar(loss_value) else loss_value,
                                     device="webgpu", dtype=input_tensor.dtype, requires_grad=True)

            # Set up backward function for CrossEntropyLoss
            def ce_backward(grad_output):
                if input_tensor.requires_grad:
                    # Gradient: softmax - one_hot(target)
                    grad_input = softmax_data.copy()
                    grad_input[np.arange(batch_size), target_indices] -= 1.0

                    if self.reduction == 'mean':
                        grad_input = grad_input / batch_size

                    grad_input_tensor = WebGPUTensor(grad_input * (grad_output._data if hasattr(grad_output, '_data') else grad_output),
                                                   device="webgpu", dtype=input_tensor.dtype)

                    # Call backward on the input tensor to propagate gradients
                    input_tensor.backward(grad_input_tensor)

            loss_tensor._backward_fn = ce_backward
            loss_tensor._inputs = [input_tensor, target_tensor]

            return loss_tensor
        else:
            raise TypeError("Both input and target must be WebGPUTensor")

class TorchNNSequential(TorchNNModule):
    """Sequential container that chains modules together"""
    def __init__(self, *args, **kwargs):
        # Initialize parent Module
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        # Handle different input formats
        if len(args) == 1 and isinstance(args[0], (list, tuple)):
            # Called as Sequential([module1, module2, ...])
            modules = args[0]
        elif len(args) == 1 and isinstance(args[0], dict):
            # Called as Sequential({'name1': module1, 'name2': module2})
            modules = args[0]
        else:
            # Called as Sequential(module1, module2, ...)
            modules = args

        # Register modules
        if isinstance(modules, dict):
            # OrderedDict-like behavior
            for idx, (name, module) in enumerate(modules.items()):
                self._modules[name] = module
        else:
            # List-like behavior with numeric keys
            for idx, module in enumerate(modules):
                self._modules[str(idx)] = module

    def forward(self, x):
        """Forward pass through all modules sequentially"""
        result = x
        for module in self._modules.values():
            result = module(result)
        return result

    def __getitem__(self, idx):
        """Support indexing to get specific module"""
        if isinstance(idx, slice):
            # Return new Sequential with sliced modules
            module_list = list(self._modules.values())
            sliced_modules = module_list[idx]
            return TorchNNSequential(sliced_modules)
        else:
            # Return single module
            if isinstance(idx, int):
                module_list = list(self._modules.values())
                if idx < 0:
                    idx = len(module_list) + idx
                if idx < 0 or idx >= len(module_list):
                    raise IndexError(f"Sequential index out of range: {idx}")
                return module_list[idx]
            else:
                # String key
                return self._modules[str(idx)]

    def __len__(self):
        """Return number of modules"""
        return len(self._modules)

    def __iter__(self):
        """Support iteration over modules"""
        return iter(self._modules.values())

    def append(self, module):
        """Append a module to the end"""
        idx = len(self._modules)
        self._modules[str(idx)] = module
        return self

    def __repr__(self):
        """String representation"""
        lines = []
        lines.append('Sequential(')
        for name, module in self._modules.items():
            mod_str = repr(module)
            mod_str = '  (' + name + '): ' + mod_str
            lines.append(mod_str)
        lines.append(')')
        return '\\n'.join(lines)

class TorchNNModuleList(TorchNNModule):
    """Container that holds submodules in a list.

    ModuleList can be indexed like a regular Python list, but modules it
    contains are properly registered, and will be visible by all Module methods.
    """
    def __init__(self, modules=None, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        if modules is not None:
            self.extend(modules)

    def __getitem__(self, idx):
        """Get module by index"""
        if isinstance(idx, slice):
            return self.__class__(list(self._modules.values())[idx])
        else:
            if isinstance(idx, int):
                module_list = list(self._modules.values())
                if idx < 0:
                    idx = len(module_list) + idx
                if idx < 0 or idx >= len(module_list):
                    raise IndexError(f"ModuleList index out of range: {idx}")
                return module_list[idx]
            else:
                raise TypeError(f"ModuleList indices must be integers, not {type(idx).__name__}")

    def __setitem__(self, idx, module):
        """Set module at index"""
        if not isinstance(idx, int):
            raise TypeError(f"ModuleList indices must be integers, not {type(idx).__name__}")

        module_list = list(self._modules.keys())
        if idx < 0:
            idx = len(module_list) + idx
        if idx < 0 or idx >= len(module_list):
            raise IndexError(f"ModuleList index out of range: {idx}")

        key = module_list[idx]
        self._modules[key] = module

    def __len__(self):
        """Return number of modules"""
        return len(self._modules)

    def __iter__(self):
        """Iterate over modules"""
        return iter(self._modules.values())

    def __iadd__(self, modules):
        """Implement += operator"""
        return self.extend(modules)

    def append(self, module):
        """Append a module to the end of the list"""
        idx = len(self._modules)
        self._modules[str(idx)] = module
        return self

    def extend(self, modules):
        """Append modules from a list"""
        if not isinstance(modules, (list, tuple)):
            raise TypeError("ModuleList.extend should be called with a list")

        for module in modules:
            self.append(module)
        return self

    def insert(self, index, module):
        """Insert a module before a given index"""
        if not isinstance(index, int):
            raise TypeError("ModuleList.insert() index must be an integer")

        # Get current modules as list
        module_list = list(self._modules.values())

        # Insert the module
        module_list.insert(index, module)

        # Rebuild _modules dict with new indices
        self._modules = {}
        for idx, mod in enumerate(module_list):
            self._modules[str(idx)] = mod

        return self

    def __repr__(self):
        """String representation"""
        lines = []
        lines.append('ModuleList(')
        for idx, module in enumerate(self._modules.values()):
            mod_str = repr(module)
            mod_str = f'  ({idx}): ' + mod_str
            lines.append(mod_str)
        lines.append(')')
        return '\\n'.join(lines)

class TorchNNModuleDict(TorchNNModule):
    """Container that holds submodules in a dictionary.

    ModuleDict can be indexed like a regular Python dictionary, but modules it
    contains are properly registered, and will be visible by all Module methods.
    """
    def __init__(self, modules=None, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        if modules is not None:
            self.update(modules)

    def __getitem__(self, key):
        """Get module by key"""
        if not isinstance(key, str):
            raise TypeError(f"ModuleDict keys must be strings, not {type(key).__name__}")
        if key not in self._modules:
            raise KeyError(f"Key '{key}' not found in ModuleDict")
        return self._modules[key]

    def __setitem__(self, key, module):
        """Set module at key"""
        if not isinstance(key, str):
            raise TypeError(f"ModuleDict keys must be strings, not {type(key).__name__}")
        self._modules[key] = module

    def __delitem__(self, key):
        """Delete module at key"""
        if not isinstance(key, str):
            raise TypeError(f"ModuleDict keys must be strings, not {type(key).__name__}")
        if key not in self._modules:
            raise KeyError(f"Key '{key}' not found in ModuleDict")
        del self._modules[key]

    def __len__(self):
        """Return number of modules"""
        return len(self._modules)

    def __iter__(self):
        """Iterate over keys"""
        return iter(self._modules.keys())

    def __contains__(self, key):
        """Check if key exists"""
        return key in self._modules

    def clear(self):
        """Remove all items"""
        self._modules = {}

    def pop(self, key):
        """Remove and return module at key"""
        if key not in self._modules:
            raise KeyError(f"Key '{key}' not found in ModuleDict")
        module = self._modules[key]
        del self._modules[key]
        return module

    def keys(self):
        """Return module keys"""
        return self._modules.keys()

    def items(self):
        """Return (key, module) pairs"""
        return self._modules.items()

    def values(self):
        """Return modules"""
        return self._modules.values()

    def update(self, modules):
        """Update the ModuleDict with key-value pairs from a mapping or iterable"""
        if not isinstance(modules, dict):
            raise TypeError("ModuleDict.update should be called with a dictionary")

        for key, module in modules.items():
            if not isinstance(key, str):
                raise TypeError(f"ModuleDict keys must be strings, not {type(key).__name__}")
            self._modules[key] = module

        return self

    def __repr__(self):
        """String representation"""
        lines = []
        lines.append('ModuleDict(')
        for key, module in self._modules.items():
            mod_str = repr(module)
            mod_str = f'  ({key}): ' + mod_str
            lines.append(mod_str)
        lines.append(')')
        return '\\n'.join(lines)

# Autograd module for gradient computation
class AutogradFunctionMeta(type):
    """Metaclass for autograd.Function to support apply() method"""
    def __call__(cls, *args, **kwargs):
        # Return instance but also support .apply() on the class
        return super().__call__(*args, **kwargs)

class AutogradFunction:
    """Base class for custom autograd functions"""

    @staticmethod
    def forward(ctx, *args, **kwargs):
        """Forward pass - must be overridden"""
        raise NotImplementedError("forward method must be implemented")

    @staticmethod
    def backward(ctx, *grad_outputs):
        """Backward pass - must be overridden"""
        raise NotImplementedError("backward method must be implemented")

    @classmethod
    def apply(cls, *args, **kwargs):
        """Apply the custom function"""
        # Create a context object to store information for backward
        ctx = type('Context', (), {
            'saved_tensors': [],
            'save_for_backward': lambda *tensors: setattr(ctx, 'saved_tensors', list(tensors)),
            'needs_input_grad': [True] * len(args)
        })()

        # Call forward pass
        output = cls.forward(ctx, *args, **kwargs)

        # If output requires grad, attach backward function
        if isinstance(output, WebGPUTensor) and any(isinstance(arg, WebGPUTensor) and arg.requires_grad for arg in args):
            output.requires_grad = True

            # Store backward function
            original_backward = output.backward if hasattr(output, 'backward') else None

            def custom_backward(gradient=None, retain_graph=False, create_graph=False):
                # Call custom backward
                grad_inputs = cls.backward(ctx, gradient if gradient is not None else WebGPUTensor(np.ones_like(output._data)))

                # Ensure grad_inputs is a tuple
                if not isinstance(grad_inputs, tuple):
                    grad_inputs = (grad_inputs,)

                # Propagate gradients to inputs
                for i, (arg, grad_input) in enumerate(zip(args, grad_inputs)):
                    if isinstance(arg, WebGPUTensor) and arg.requires_grad and grad_input is not None:
                        if arg.grad is None:
                            arg.grad = grad_input
                        else:
                            # Accumulate gradients
                            arg.grad._data += grad_input._data

            output.backward = custom_backward

        return output

class TorchAutograd:
    def __init__(self, **kwargs):
        self.Function = AutogradFunction

    def grad(self, outputs, inputs, grad_outputs=None, retain_graph=False, create_graph=False):
        """Compute gradients of outputs with respect to inputs

        Args:
            outputs: Tensor or sequence of tensors to differentiate
            inputs: Tensor or sequence of tensors to compute gradients for
            grad_outputs: Gradients w.r.t outputs (default: ones for scalar outputs)
            retain_graph: Whether to retain computation graph after backward
            create_graph: Whether to create graph for higher-order gradients

        Returns:
            Tuple of gradients, one for each input tensor
        """
        # Handle single tensor or list of tensors
        if not isinstance(outputs, (list, tuple)):
            outputs = [outputs]
        if not isinstance(inputs, (list, tuple)):
            inputs = [inputs]
            return_single = True
        else:
            return_single = False

        # Prepare grad_outputs
        if grad_outputs is None:
            grad_outputs = []
            for output in outputs:
                if output._data.size == 1:
                    grad_outputs.append(WebGPUTensor(np.ones_like(output._data), device=output.device, dtype=output.dtype))
                else:
                    raise RuntimeError("grad can be implicitly created only for scalar outputs")
        elif not isinstance(grad_outputs, (list, tuple)):
            grad_outputs = [grad_outputs]

        # Zero out input gradients to start fresh
        for inp in inputs:
            inp.grad = None

        # Run backward for each output
        # Always retain graph during backward to allow gradient collection
        for output, grad_output in zip(outputs, grad_outputs):
            # Check if output has requires_grad
            if not output.requires_grad:
                raise RuntimeError("element 0 of tensors does not require grad and does not have a grad_fn")

            try:
                output.backward(gradient=grad_output, retain_graph=True, create_graph=create_graph)
            except RuntimeError as e:
                # If graph was already destroyed, we can't compute gradients
                if "second time" in str(e):
                    # Return None for all gradients since graph is destroyed
                    return tuple([None] * len(inputs))
                raise

        # Collect gradients
        grads = []
        for inp in inputs:
            if inp.grad is not None:
                if create_graph:
                    # Keep gradient in computation graph with requires_grad
                    grad_copy = inp.grad.clone()
                    # Ensure the gradient requires_grad for higher-order derivatives
                    if not grad_copy.requires_grad:
                        grad_copy.requires_grad = True
                else:
                    # Detach from graph
                    grad_copy = inp.grad.detach()
                grads.append(grad_copy)
            else:
                grads.append(None)

        # Clean up input gradients if not retaining graph
        if not retain_graph and not create_graph:
            for inp in inputs:
                inp.grad = None

        # Always return a tuple (even for single inputs, to match PyTorch behavior)
        return tuple(grads)

# Create torch module with essential functions
class TorchModule:
    def __init__(self, **kwargs):
        self.tensor = self._tensor
        self.zeros = self._zeros
        self.ones = self._ones
        self.full = self._full
        self.randn = self._randn
        self.rand = self._rand
        self.randint = self._randint
        self.zeros_like = self._zeros_like
        self.ones_like = self._ones_like
        self.full_like = self._full_like
        self.randn_like = self._randn_like
        self.rand_like = self._rand_like
        self.matmul = self._matmul
        self.mm = self._mm
        self.sum = self._sum
        self.as_tensor = self._as_tensor
        self.arange = self._arange
        self.randperm = self._randperm
        self.linspace = self._linspace
        self.nn = TorchNN()

        # Add Tensor class reference
        self.Tensor = WebGPUTensor

        # Typed tensor constructors
        self.FloatTensor = self._FloatTensor
        self.DoubleTensor = self._DoubleTensor
        self.LongTensor = self._LongTensor
        self.IntTensor = self._IntTensor
        self.BoolTensor = self._BoolTensor

        # Linear algebra module
        self.linalg = TorchLinalg()

        # Data utilities module
        self.utils = TorchUtils()

        # Optimizer module
        self.optim = TorchOptim()

        # Autograd module
        self.autograd = TorchAutograd()

        # Activation functions
        self.relu = self._relu
        self.sigmoid = self._sigmoid

        # Mathematical functions
        self.round = self.round
        self.sqrt = self._sqrt
        self.pow = self._pow
        self.exp = self._exp
        self.log = self._log
        self.tanh = self._tanh
        self.mean = self._mean
        self.max = self._max
        self.min = self._min
        self.transpose = self._transpose

        # Advanced mathematical functions
        self.cat = self._cat
        self.stack = self._stack
        self.std = self._std
        self.abs = self._abs
        self.norm = self._norm
        self.sign = self._sign
        self.sin = self._sin
        self.cos = self._cos
        self.clamp = self._clamp
        self.argmax = self._argmax
        self.argmin = self._argmin

        # Boolean operations
        self.all = self._all
        self.any = self._any
        self.allclose = self._allclose

        # Comparison operations
        self.eq = self._eq
        self.ne = self._ne
        self.gt = self._gt
        self.lt = self._lt
        self.ge = self._ge
        self.le = self._le

        # Masked operations
        self.masked_select = self._masked_select

        # Context managers and gradient control
        self.no_grad = self._no_grad
        self.enable_grad = self._enable_grad
        self.set_grad_enabled = self._set_grad_enabled
        self.is_grad_enabled = self._is_grad_enabled

        # Device and CUDA support
        self.device = self._device
        self.cuda = TorchCuda()
        self.backends = TorchBackends()
        self.manual_seed = self._manual_seed

        # Additional functions
        self.bmm = self._bmm
        self.einsum = self._einsum
        self.logsumexp = self._logsumexp
        self.isnan = self._isnan
        self.isinf = self._isinf
        self.allclose = self._allclose

        # Serialization
        self.save = self._save
        self.load = self._load

        # Data types
        self.float32 = 'float32'
        self.float64 = 'float64'
        self.double = 'float64'
        self.float = 'float32'
        self.int32 = 'int32'
        self.int64 = 'int64'
        self.long = 'int64'
        self.int = 'int32'
        self.bool = 'bool'
        self.uint8 = 'uint8'
        
        # Device types
        # self.device = self._device  # DISABLED: Potential recursion source
        
    def _tensor(self, data, **kwargs):
        # Enable WebGPU detection by default for tensor creation
        if 'device' not in kwargs:
            kwargs['device'] = 'webgpu'  # Default to WebGPU instead of CPU
        return WebGPUTensor(data, **kwargs)

    def _FloatTensor(self, data):
        """Create a tensor with float32 dtype"""
        return WebGPUTensor(data, dtype='float32', device='webgpu', _internal=True)

    def _DoubleTensor(self, data):
        """Create a tensor with float64 dtype"""
        return WebGPUTensor(data, dtype='float64', device='webgpu', _internal=True)

    def _LongTensor(self, data):
        """Create a tensor with int64 dtype"""
        return WebGPUTensor(data, dtype='int64', device='webgpu', _internal=True)

    def _IntTensor(self, data):
        """Create a tensor with int32 dtype"""
        return WebGPUTensor(data, dtype='int32', device='webgpu', _internal=True)

    def _BoolTensor(self, data):
        """Create a tensor with bool dtype"""
        return WebGPUTensor(data, dtype='bool', device='webgpu', _internal=True)

    def _zeros(self, *shape, **kwargs):
        data = np.zeros(shape)
        return WebGPUTensor(data, **kwargs)
    
    def _ones(self, *shape, **kwargs):
        data = np.ones(shape)
        return WebGPUTensor(data, **kwargs)

    def _full(self, size, fill_value, **kwargs):
        """Create a tensor filled with a scalar value"""
        data = np.full(size, fill_value)
        return WebGPUTensor(data, **kwargs)

    def _zeros_like(self, input_tensor, **kwargs):
        """Create a tensor of zeros with the same shape as input"""
        if isinstance(input_tensor, WebGPUTensor):
            data = np.zeros_like(input_tensor._data)
        else:
            data = np.zeros_like(input_tensor)
        return WebGPUTensor(data, **kwargs)

    def _ones_like(self, input_tensor, **kwargs):
        """Create a tensor of ones with the same shape as input"""
        if isinstance(input_tensor, WebGPUTensor):
            data = np.ones_like(input_tensor._data)
        else:
            data = np.ones_like(input_tensor)
        return WebGPUTensor(data, **kwargs)

    def _full_like(self, input_tensor, fill_value, **kwargs):
        """Create a tensor filled with fill_value with the same shape as input"""
        if isinstance(input_tensor, WebGPUTensor):
            data = np.full_like(input_tensor._data, fill_value)
        else:
            data = np.full_like(input_tensor, fill_value)
        return WebGPUTensor(data, **kwargs)

    def _randn(self, *shape, **kwargs):
        data = np.random.randn(*shape)
        return WebGPUTensor(data, **kwargs)

    def _rand(self, *shape, **kwargs):
        """Generate random tensor with uniform distribution [0, 1)"""
        data = np.random.rand(*shape)
        return WebGPUTensor(data, **kwargs)

    def _randint(self, low, high=None, size=None, **kwargs):
        """Generate random integers

        Args:
            low: Lowest integer (inclusive) or if high is None, highest (exclusive)
            high: Highest integer (exclusive)
            size: Output shape as tuple
        """
        if high is None:
            # torch.randint(5, (3, 2)) means [0, 5) with shape (3, 2)
            high = low
            low = 0

        if size is None:
            # Single value
            data = np.random.randint(low, high)
        else:
            # Array of values
            data = np.random.randint(low, high, size=size)

        return WebGPUTensor(data, dtype='int64', **kwargs)

    def _matmul(self, a, b):
        if isinstance(a, WebGPUTensor) and isinstance(b, WebGPUTensor):
            return WebGPUTensor(np.dot(a.data, b.data), device="webgpu")
        return WebGPUTensor(np.dot(a, b))

    def _mm(self, a, b):
        """Matrix multiplication for 2D tensors (torch.mm)"""
        if isinstance(a, WebGPUTensor) and isinstance(b, WebGPUTensor):
            if a.ndim != 2 or b.ndim != 2:
                raise RuntimeError(f"mm() expects 2D tensors, but got {a.ndim}D and {b.ndim}D tensors")
            result_data = np.dot(a._data, b._data)
            return WebGPUTensor(result_data, device="webgpu", dtype=a.dtype, _internal=True)
        else:
            a_data = a._data if isinstance(a, WebGPUTensor) else a
            b_data = b._data if isinstance(b, WebGPUTensor) else b
            result_data = np.dot(a_data, b_data)
            return WebGPUTensor(result_data, device="webgpu", _internal=True)

    def _bmm(self, batch1, batch2):
        """Batch matrix multiplication for 3D tensors"""
        if isinstance(batch1, WebGPUTensor):
            batch1_data = batch1._data.reshape(batch1.shape)
        else:
            batch1_data = np.array(batch1)

        if isinstance(batch2, WebGPUTensor):
            batch2_data = batch2._data.reshape(batch2.shape)
        else:
            batch2_data = np.array(batch2)

        if batch1_data.ndim != 3 or batch2_data.ndim != 3:
            raise RuntimeError(f"bmm() expects 3D tensors, but got {batch1_data.ndim}D and {batch2_data.ndim}D tensors")

        if batch1_data.shape[0] != batch2_data.shape[0]:
            raise RuntimeError(f"batch1 and batch2 must have same batch size, but got {batch1_data.shape[0]} and {batch2_data.shape[0]}")

        if batch1_data.shape[2] != batch2_data.shape[1]:
            raise RuntimeError(f"batch1 and batch2 must have compatible matrix dimensions for multiplication")

        # Perform batch matrix multiplication
        result = np.matmul(batch1_data, batch2_data)
        dtype = batch1.dtype if isinstance(batch1, WebGPUTensor) else 'float32'
        return WebGPUTensor(result, device="webgpu", dtype=dtype, _internal=True)

    def _logsumexp(self, input_tensor, dim=None, keepdim=False):
        """Compute log-sum-exp of tensor for numerical stability"""
        if isinstance(input_tensor, WebGPUTensor):
            data = input_tensor._data.reshape(input_tensor.shape)
        else:
            data = np.array(input_tensor)

        # Numerically stable log-sum-exp
        if dim is None:
            max_val = np.max(data)
            result = max_val + np.log(np.sum(np.exp(data - max_val)))
        else:
            max_val = np.max(data, axis=dim, keepdims=True)
            result = max_val + np.log(np.sum(np.exp(data - max_val), axis=dim, keepdims=True))
            if not keepdim:
                result = np.squeeze(result, axis=dim)

        dtype = input_tensor.dtype if isinstance(input_tensor, WebGPUTensor) else 'float32'
        return WebGPUTensor(result, device="webgpu", dtype=dtype, _internal=True)

    def _isnan(self, input_tensor):
        """Check for NaN values in tensor"""
        if isinstance(input_tensor, WebGPUTensor):
            data = input_tensor._data.reshape(input_tensor.shape)
        else:
            data = np.array(input_tensor)

        result = np.isnan(data)
        return WebGPUTensor(result, device="webgpu", dtype='bool', _internal=True)

    def _isinf(self, input_tensor):
        """Check for infinite values in tensor"""
        if isinstance(input_tensor, WebGPUTensor):
            data = input_tensor._data.reshape(input_tensor.shape)
        else:
            data = np.array(input_tensor)

        result = np.isinf(data)
        return WebGPUTensor(result, device="webgpu", dtype='bool', _internal=True)

    def _einsum(self, equation, *operands):
        """Einstein summation convention"""
        # Convert WebGPU tensors to numpy arrays
        np_operands = []
        for op in operands:
            if isinstance(op, WebGPUTensor):
                np_operands.append(op._data.reshape(op.shape))
            else:
                np_operands.append(np.array(op))

        # Perform einsum operation
        result = np.einsum(equation, *np_operands)

        # Determine dtype from first operand
        dtype = operands[0].dtype if isinstance(operands[0], WebGPUTensor) else 'float32'
        return WebGPUTensor(result, device="webgpu", dtype=dtype, _internal=True)

    def _device(self, device_type):
        \"\"\"Create a device object\"\"\"
        return WebGPUDevice(device_type)
    
    def _sum(self, input_tensor, dim=None, keepdim=False, dtype=None):
        \"\"\"Compute sum of tensor elements\"\"\"
        if isinstance(input_tensor, WebGPUTensor):
            return input_tensor.sum(dim=dim, keepdim=keepdim)
        else:
            # Handle numpy arrays or lists
            if dim is None:
                result_data = np.sum(input_tensor)
            else:
                result_data = np.sum(input_tensor, axis=dim, keepdims=keepdim)
            return WebGPUTensor(result_data, dtype=dtype or 'float32')
    
    def _as_tensor(self, data, dtype=None, device=None):
        \"\"\"Convert data to tensor, similar to torch.as_tensor\"\"\"
        # Determine dtype
        if dtype is None:
            if hasattr(data, 'dtype'):
                dtype = str(data.dtype)
            else:
                dtype = 'float32'
        
        # Determine device - default to WebGPU for better performance
        if device is None:
            device = 'webgpu'
        
        # Create tensor
        return WebGPUTensor(data, dtype=dtype, device=device, _internal=True)
    
    def eye(self, n, m=None, dtype='float32', device='webgpu'):
        \"\"\"Create identity matrix\"\"\"
        if m is None:
            m = n
        data = np.eye(n, m)
        return WebGPUTensor(data, device=device, dtype=dtype, _internal=True)
    
    def round(self, input_tensor, decimals=0):
        """Round tensor elements to given number of decimals"""
        if isinstance(input_tensor, WebGPUTensor):
            rounded_data = np.round(input_tensor._data, decimals=decimals)
            return WebGPUTensor(rounded_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad, _internal=True)
        else:
            return WebGPUTensor(np.round(input_tensor, decimals=decimals))
    
    def det(self, input_tensor):
        \"\"\"Compute determinant of square matrix\"\"\"
        if isinstance(input_tensor, WebGPUTensor):
            if input_tensor.ndim != 2 or input_tensor.shape[0] != input_tensor.shape[1]:
                raise RuntimeError("det() expects a 2D square tensor")
            det_value = np.linalg.det(input_tensor._data.reshape(input_tensor.shape))
            return WebGPUTensor([det_value], device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            return np.linalg.det(input_tensor)
    
    def _arange(self, *args, **kwargs):
        \"\"\"Create a 1D tensor with evenly spaced values\"\"\"
        if len(args) == 1:
            # arange(end)
            start, end, step = 0, args[0], 1
        elif len(args) == 2:
            # arange(start, end)
            start, end, step = args[0], args[1], 1
        elif len(args) == 3:
            # arange(start, end, step)
            start, end, step = args[0], args[1], args[2]
        else:
            raise ValueError(\"arange() takes 1 to 3 positional arguments\")
        
        data = np.arange(start, end, step)
        device = kwargs.get('device', 'cpu')
        dtype = kwargs.get('dtype', 'int64' if isinstance(start, int) and isinstance(end, int) and isinstance(step, int) else 'float32')
        return WebGPUTensor(data, device=device, dtype=dtype, _internal=True)
    
    def _randperm(self, n, **kwargs):
        \"\"\"Generate a random permutation of integers from 0 to n-1\"\"\"
        data = np.random.permutation(n)
        device = kwargs.get('device', 'cpu')
        dtype = kwargs.get('dtype', 'int64')
        return WebGPUTensor(data, device=device, dtype=dtype, _internal=True)

    def _linspace(self, start, end, steps, **kwargs):
        \"\"\"Generate linearly spaced values\"\"\"
        data = np.linspace(start, end, steps)
        device = kwargs.get('device', 'cpu')
        dtype = kwargs.get('dtype', 'float32')
        return WebGPUTensor(data, device=device, dtype=dtype, _internal=True)

    def _zeros_like(self, input_tensor, **kwargs):
        \"\"\"Create a tensor of zeros with the same shape as input\"\"\"
        if isinstance(input_tensor, WebGPUTensor):
            data = np.zeros_like(input_tensor._data)
            device = kwargs.get('device', input_tensor.device)
            dtype = kwargs.get('dtype', input_tensor.dtype)
            return WebGPUTensor(data, device=device, dtype=dtype, _internal=True)
        else:
            data = np.zeros_like(input_tensor)
            device = kwargs.get('device', 'cpu')
            dtype = kwargs.get('dtype', 'float32')
            return WebGPUTensor(data, device=device, dtype=dtype, _internal=True)

    def _ones_like(self, input_tensor, **kwargs):
        \"\"\"Create a tensor of ones with the same shape as input\"\"\"
        if isinstance(input_tensor, WebGPUTensor):
            data = np.ones_like(input_tensor._data)
            device = kwargs.get('device', input_tensor.device)
            dtype = kwargs.get('dtype', input_tensor.dtype)
            return WebGPUTensor(data, device=device, dtype=dtype, _internal=True)
        else:
            data = np.ones_like(input_tensor)
            device = kwargs.get('device', 'cpu')
            dtype = kwargs.get('dtype', 'float32')
            return WebGPUTensor(data, device=device, dtype=dtype, _internal=True)

    def _randn_like(self, input_tensor, **kwargs):
        \"\"\"Create a tensor of random values from normal distribution with the same shape as input\"\"\"
        if isinstance(input_tensor, WebGPUTensor):
            data = np.random.randn(*input_tensor._data.shape)
            device = kwargs.get('device', input_tensor.device)
            dtype = kwargs.get('dtype', input_tensor.dtype)
            return WebGPUTensor(data, device=device, dtype=dtype, _internal=True)
        else:
            data = np.random.randn(*input_tensor.shape)
            device = kwargs.get('device', 'cpu')
            dtype = kwargs.get('dtype', 'float32')
            return WebGPUTensor(data, device=device, dtype=dtype, _internal=True)

    def _rand_like(self, input_tensor, **kwargs):
        \"\"\"Create a tensor of random values from uniform distribution [0,1) with the same shape as input\"\"\"
        if isinstance(input_tensor, WebGPUTensor):
            data = np.random.rand(*input_tensor._data.shape)
            device = kwargs.get('device', input_tensor.device)
            dtype = kwargs.get('dtype', input_tensor.dtype)
            return WebGPUTensor(data, device=device, dtype=dtype, _internal=True)
        else:
            data = np.random.rand(*input_tensor.shape)
            device = kwargs.get('device', 'cpu')
            dtype = kwargs.get('dtype', 'float32')
            return WebGPUTensor(data, device=device, dtype=dtype, _internal=True)

    def _relu(self, input_tensor):
        \"\"\"ReLU activation function\"\"\"
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.maximum(input_tensor._data, 0)
            result = WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad, _internal=True)
            
            if input_tensor.requires_grad:
                def relu_backward(grad_output):
                    if input_tensor.grad is None:
                        input_tensor.grad = WebGPUTensor(np.zeros_like(input_tensor._data), device="webgpu", dtype=input_tensor.dtype)
                    relu_grad = grad_output._data * (input_tensor._data > 0).astype(input_tensor.dtype)
                    input_tensor.grad._data += relu_grad
                
                result._backward_fn = relu_backward
                result._inputs = [input_tensor]
            
            return result
        else:
            return np.maximum(input_tensor, 0)

    def _sigmoid(self, input_tensor):
        """Sigmoid activation function"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = 1 / (1 + np.exp(-input_tensor._data))
            result = WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad, _internal=True)

            if input_tensor.requires_grad:
                def sigmoid_backward(grad_output):
                    if input_tensor.grad is None:
                        input_tensor.grad = WebGPUTensor(np.zeros_like(input_tensor._data), device="webgpu", dtype=input_tensor.dtype)
                    sigmoid_val = result_data
                    sigmoid_grad = grad_output._data * sigmoid_val * (1 - sigmoid_val)
                    input_tensor.grad._data += sigmoid_grad

                result._backward_fn = sigmoid_backward
                result._inputs = [input_tensor]

            return result
        else:
            return 1 / (1 + np.exp(-input_tensor))

    def _sqrt(self, input_tensor):
        """Square root function"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.sqrt(input_tensor._data)
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad, _internal=True)
        else:
            return np.sqrt(input_tensor)

    def _pow(self, input_tensor, exponent):
        """Power function"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.power(input_tensor._data, exponent)
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad, _internal=True)
        else:
            return np.power(input_tensor, exponent)

    def _exp(self, input_tensor):
        """Exponential function"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.exp(input_tensor._data)
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad, _internal=True)
        else:
            return np.exp(input_tensor)

    def _log(self, input_tensor):
        """Natural logarithm function"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.log(input_tensor._data)
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad, _internal=True)
        else:
            return np.log(input_tensor)

    def _tanh(self, input_tensor):
        """Hyperbolic tangent function"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.tanh(input_tensor._data)
            result = WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)

            # Set up backward function for autograd
            if input_tensor.requires_grad:
                def tanh_backward(grad_output):
                    # Gradient: tanh'(x) = 1 - tanh(x)
                    tanh_val = result_data
                    grad_input = grad_output._data * (1 - tanh_val ** 2)
                    grad_tensor = WebGPUTensor(grad_input, device="webgpu", dtype=input_tensor.dtype, _internal=True)
                    input_tensor.backward(grad_tensor)

                result._backward_fn = tanh_backward
                result._inputs = [input_tensor]

            return result
        else:
            return np.tanh(input_tensor)

    def _mean(self, input_tensor, dim=None, keepdim=False):
        """Mean function"""
        if isinstance(input_tensor, WebGPUTensor):
            if dim is None:
                result_data = np.mean(input_tensor._data)
                return WebGPUTensor([result_data], device="webgpu", dtype=input_tensor.dtype, _internal=True)
            else:
                result_data = np.mean(input_tensor._data, axis=dim, keepdims=keepdim)
                return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            return np.mean(input_tensor, axis=dim, keepdims=keepdim)

    def _max(self, input_tensor, dim=None, keepdim=False):
        """Maximum function - returns values and indices when dim is specified"""
        if isinstance(input_tensor, WebGPUTensor):
            if dim is None:
                # Global max - return single value
                result_data = np.max(input_tensor._data)
                return WebGPUTensor([result_data], device="webgpu", dtype=input_tensor.dtype, _internal=True)
            else:
                # Max along dimension - return (values, indices) tuple like PyTorch
                max_values = np.max(input_tensor._data, axis=dim, keepdims=keepdim)
                max_indices = np.argmax(input_tensor._data, axis=dim)
                if keepdim:
                    max_indices = np.expand_dims(max_indices, axis=dim)


                values_tensor = WebGPUTensor(max_values, device="webgpu", dtype=input_tensor.dtype, _internal=True)
                indices_tensor = WebGPUTensor(max_indices, device="webgpu", dtype='int64', _internal=True)
                return values_tensor, indices_tensor
        else:
            if dim is None:
                return np.max(input_tensor)
            else:
                max_values = np.max(input_tensor, axis=dim, keepdims=keepdim)
                max_indices = np.argmax(input_tensor, axis=dim)
                if keepdim:
                    max_indices = np.expand_dims(max_indices, axis=dim)
                return max_values, max_indices

    def _min(self, input_tensor, dim=None, keepdim=False):
        """Minimum function - returns values and indices when dim is specified"""
        if isinstance(input_tensor, WebGPUTensor):
            if dim is None:
                # Global min - return single value
                result_data = np.min(input_tensor._data)
                return WebGPUTensor([result_data], device="webgpu", dtype=input_tensor.dtype, _internal=True)
            else:
                # Min along dimension - return (values, indices) tuple like PyTorch
                min_values = np.min(input_tensor._data, axis=dim, keepdims=keepdim)
                min_indices = np.argmin(input_tensor._data, axis=dim)
                if keepdim:
                    min_indices = np.expand_dims(min_indices, axis=dim)


                values_tensor = WebGPUTensor(min_values, device="webgpu", dtype=input_tensor.dtype, _internal=True)
                indices_tensor = WebGPUTensor(min_indices, device="webgpu", dtype='int64', _internal=True)
                return values_tensor, indices_tensor
        else:
            if dim is None:
                return np.min(input_tensor)
            else:
                min_values = np.min(input_tensor, axis=dim, keepdims=keepdim)
                min_indices = np.argmin(input_tensor, axis=dim)
                if keepdim:
                    min_indices = np.expand_dims(min_indices, axis=dim)
                return min_values, min_indices

    def _transpose(self, input_tensor, dim0, dim1):
        """Transpose function"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.swapaxes(input_tensor._data, dim0, dim1)
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad, _internal=True)
        else:
            return np.swapaxes(input_tensor, dim0, dim1)

    def _cat(self, tensors, dim=0):
        """Concatenate tensors along specified dimension"""
        if not isinstance(tensors, (list, tuple)):
            raise TypeError("tensors must be a list or tuple")

        if len(tensors) == 0:
            raise RuntimeError("cat expects a non-empty list of tensors")

        # Convert all tensors to WebGPUTensor if needed
        tensor_list = []
        for tensor in tensors:
            if isinstance(tensor, WebGPUTensor):
                tensor_list.append(tensor.data)
            else:
                tensor_list.append(tensor)

        # Use numpy concatenate
        result_data = np.concatenate(tensor_list, axis=dim)
        return WebGPUTensor(result_data, device="webgpu", dtype=tensors[0].dtype, _internal=True)

    def _stack(self, tensors, dim=0):
        """Stack tensors along a new dimension"""
        if not isinstance(tensors, (list, tuple)):
            raise TypeError("tensors must be a list or tuple")

        if len(tensors) == 0:
            raise RuntimeError("stack expects a non-empty list of tensors")

        # Convert all tensors to WebGPUTensor if needed
        tensor_list = []
        for tensor in tensors:
            if isinstance(tensor, WebGPUTensor):
                tensor_list.append(tensor.data)
            else:
                tensor_list.append(tensor)

        # Use numpy stack
        result_data = np.stack(tensor_list, axis=dim)
        return WebGPUTensor(result_data, device="webgpu", dtype=tensors[0].dtype, _internal=True)

    def _std(self, input_tensor, dim=None, keepdim=False):
        """Standard deviation function"""
        if isinstance(input_tensor, WebGPUTensor):
            if dim is None:
                result_data = np.std(input_tensor._data)
                return WebGPUTensor([result_data], device="webgpu", dtype=input_tensor.dtype, _internal=True)
            else:
                result_data = np.std(input_tensor._data, axis=dim, keepdims=keepdim)
                return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            return np.std(input_tensor, axis=dim, keepdims=keepdim)

    def _abs(self, input_tensor):
        """Absolute value function"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.abs(input_tensor._data)
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad, _internal=True)
        else:
            return np.abs(input_tensor)

    def _norm(self, input_tensor, ord=None, dim=None, keepdim=False):
        """Compute matrix or vector norm"""
        if isinstance(input_tensor, WebGPUTensor):
            if dim is None:
                norm_value = np.linalg.norm(input_tensor._data, ord=ord)
                return WebGPUTensor([norm_value], device="webgpu", dtype=input_tensor.dtype, _internal=True)
            else:
                norm_data = np.linalg.norm(input_tensor._data.reshape(input_tensor.shape), ord=ord, axis=dim, keepdims=keepdim)
                return WebGPUTensor(norm_data, device="webgpu", dtype=input_tensor.dtype, _internal=True)
        else:
            return np.linalg.norm(input_tensor, ord=ord, axis=dim, keepdims=keepdim)

    def _sign(self, input_tensor):
        """Sign function - returns -1, 0, or 1"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.sign(input_tensor._data)
            result = WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad)

            # Gradient is zero everywhere (sign is not differentiable in the usual sense)
            # In PyTorch, gradient of sign is treated as zero
            if input_tensor.requires_grad:
                def sign_backward(grad_output):
                    # Gradient is zero - sign is a step function
                    grad_input = np.zeros_like(input_tensor._data)
                    grad_tensor = WebGPUTensor(grad_input, device="webgpu", dtype=input_tensor.dtype, _internal=True)
                    if input_tensor.grad is None:
                        input_tensor.grad = grad_tensor
                    else:
                        input_tensor.grad._data += grad_tensor._data

                result._backward_fn = sign_backward
                result._inputs = [input_tensor]

            return result
        else:
            return np.sign(input_tensor)

    def _sin(self, input_tensor):
        """Sine function"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.sin(input_tensor._data)
            result = WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad, _internal=True)

            # Set up backward function for sin
            if input_tensor.requires_grad:
                def sin_backward(grad_output):
                    # Gradient: cos(x)
                    grad_input = grad_output._data * np.cos(input_tensor._data)
                    if input_tensor.grad is None:
                        input_tensor.grad = WebGPUTensor(np.zeros_like(input_tensor._data), device="webgpu", dtype=input_tensor.dtype, _internal=True)
                    input_tensor.grad._data += grad_input

                result._backward_fn = sin_backward
                result._inputs = [input_tensor]

            return result
        else:
            return np.sin(input_tensor)

    def _cos(self, input_tensor):
        """Cosine function"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.cos(input_tensor._data)
            result = WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad, _internal=True)

            # Set up backward function for cos
            if input_tensor.requires_grad:
                def cos_backward(grad_output):
                    # Gradient: -sin(x)
                    grad_input = grad_output._data * (-np.sin(input_tensor._data))
                    if input_tensor.grad is None:
                        input_tensor.grad = WebGPUTensor(np.zeros_like(input_tensor._data), device="webgpu", dtype=input_tensor.dtype, _internal=True)
                    input_tensor.grad._data += grad_input

                result._backward_fn = cos_backward
                result._inputs = [input_tensor]

            return result
        else:
            return np.cos(input_tensor)

    def _clamp(self, input_tensor, min=None, max=None):
        """Clamp function - constrain values to a range"""
        if isinstance(input_tensor, WebGPUTensor):
            result_data = np.clip(input_tensor._data, min, max)
            return WebGPUTensor(result_data, device="webgpu", dtype=input_tensor.dtype, requires_grad=input_tensor.requires_grad, _internal=True)
        else:
            return np.clip(input_tensor, min, max)

    def _argmax(self, input_tensor, dim=None, keepdim=False):
        """Argmax function - indices of maximum values"""
        if isinstance(input_tensor, WebGPUTensor):
            if dim is None:
                result_data = np.argmax(input_tensor._data)
                return WebGPUTensor([result_data], device="webgpu", dtype='int64', _internal=True)
            else:
                result_data = np.argmax(input_tensor._data, axis=dim)
                if keepdim:
                    result_data = np.expand_dims(result_data, axis=dim)
                return WebGPUTensor(result_data, device="webgpu", dtype='int64', _internal=True)
        else:
            return np.argmax(input_tensor, axis=dim)

    def _argmin(self, input_tensor, dim=None, keepdim=False):
        """Argmin function - indices of minimum values"""
        if isinstance(input_tensor, WebGPUTensor):
            if dim is None:
                result_data = np.argmin(input_tensor._data)
                return WebGPUTensor([result_data], device="webgpu", dtype='int64', _internal=True)
            else:
                result_data = np.argmin(input_tensor._data, axis=dim)
                if keepdim:
                    result_data = np.expand_dims(result_data, axis=dim)
                return WebGPUTensor(result_data, device="webgpu", dtype='int64', _internal=True)
        else:
            return np.argmin(input_tensor, axis=dim)

    def _all(self, input_tensor, dim=None, keepdim=False):
        """Test if all elements evaluate to True"""
        if isinstance(input_tensor, WebGPUTensor):
            if dim is None:
                result_data = np.all(input_tensor._data)
                return WebGPUTensor([result_data], device="webgpu", dtype='bool', _internal=True)
            else:
                result_data = np.all(input_tensor._data, axis=dim, keepdims=keepdim)
                return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)
        else:
            if dim is None:
                return bool(np.all(input_tensor))
            else:
                return np.all(input_tensor, axis=dim, keepdims=keepdim)

    def _any(self, input_tensor, dim=None, keepdim=False):
        """Test if any element evaluates to True"""
        if isinstance(input_tensor, WebGPUTensor):
            if dim is None:
                result_data = np.any(input_tensor._data)
                return WebGPUTensor([result_data], device="webgpu", dtype='bool', _internal=True)
            else:
                result_data = np.any(input_tensor._data, axis=dim, keepdims=keepdim)
                return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)
        else:
            if dim is None:
                return bool(np.any(input_tensor))
            else:
                return np.any(input_tensor, axis=dim, keepdims=keepdim)

    def _allclose(self, a, b, rtol=1e-05, atol=1e-08, equal_nan=False):
        """Returns True if two tensors are element-wise equal within a tolerance"""
        # Extract numpy arrays from tensors
        if isinstance(a, WebGPUTensor):
            a_data = a._data
        else:
            a_data = np.array(a)

        if isinstance(b, WebGPUTensor):
            b_data = b._data
        else:
            b_data = np.array(b)

        # Use numpy's allclose
        result = np.allclose(a_data, b_data, rtol=rtol, atol=atol, equal_nan=equal_nan)
        return bool(result)

    # Comparison operations
    def _eq(self, a, b):
        """Element-wise equality comparison"""
        if isinstance(a, WebGPUTensor):
            a_data = a._data
        else:
            a_data = np.array(a)

        if isinstance(b, WebGPUTensor):
            b_data = b._data
        else:
            b_data = np.array(b)

        result_data = a_data == b_data
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    def _ne(self, a, b):
        """Element-wise not-equal comparison"""
        if isinstance(a, WebGPUTensor):
            a_data = a._data
        else:
            a_data = np.array(a)

        if isinstance(b, WebGPUTensor):
            b_data = b._data
        else:
            b_data = np.array(b)

        result_data = a_data != b_data
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    def _gt(self, a, b):
        """Element-wise greater-than comparison"""
        if isinstance(a, WebGPUTensor):
            a_data = a._data
        else:
            a_data = np.array(a)

        if isinstance(b, WebGPUTensor):
            b_data = b._data
        else:
            b_data = np.array(b)

        result_data = a_data > b_data
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    def _lt(self, a, b):
        """Element-wise less-than comparison"""
        if isinstance(a, WebGPUTensor):
            a_data = a._data
        else:
            a_data = np.array(a)

        if isinstance(b, WebGPUTensor):
            b_data = b._data
        else:
            b_data = np.array(b)

        result_data = a_data < b_data
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    def _ge(self, a, b):
        """Element-wise greater-than-or-equal comparison"""
        if isinstance(a, WebGPUTensor):
            a_data = a._data
        else:
            a_data = np.array(a)

        if isinstance(b, WebGPUTensor):
            b_data = b._data
        else:
            b_data = np.array(b)

        result_data = a_data >= b_data
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    def _le(self, a, b):
        """Element-wise less-than-or-equal comparison"""
        if isinstance(a, WebGPUTensor):
            a_data = a._data
        else:
            a_data = np.array(a)

        if isinstance(b, WebGPUTensor):
            b_data = b._data
        else:
            b_data = np.array(b)

        result_data = a_data <= b_data
        return WebGPUTensor(result_data, device="webgpu", dtype='bool', _internal=True)

    # Masked operations
    def _masked_select(self, input_tensor, mask):
        """Returns a 1-D tensor with all values from input_tensor where mask is True.

        Args:
            input_tensor: Input tensor
            mask: Boolean tensor with same shape as input

        Returns:
            1-D tensor containing selected elements
        """
        if isinstance(input_tensor, WebGPUTensor):
            input_data = input_tensor._data
        else:
            input_data = np.array(input_tensor)

        if isinstance(mask, WebGPUTensor):
            mask_data = mask._data
        else:
            mask_data = np.array(mask)

        # Ensure mask is boolean type
        mask_data = mask_data.astype(bool)

        # Select elements where mask is True
        selected_data = input_data[mask_data]

        result = WebGPUTensor(
            selected_data,
            device="webgpu",
            dtype=input_tensor.dtype if isinstance(input_tensor, WebGPUTensor) else 'float32',
            requires_grad=input_tensor.requires_grad if isinstance(input_tensor, WebGPUTensor) else False
        )

        # Set up backward function if gradient tracking is enabled
        if isinstance(input_tensor, WebGPUTensor) and input_tensor.requires_grad:
            def masked_select_backward(grad_output):
                # Create zero gradient of same shape as input
                grad_input = np.zeros_like(input_data)

                # Scatter gradients back to original positions
                grad_input[mask_data] = grad_output._data

                grad_tensor = WebGPUTensor(grad_input, device="webgpu", dtype=input_tensor.dtype, _internal=True)
                if input_tensor.grad is None:
                    input_tensor.grad = grad_tensor
                else:
                    input_tensor.grad._data += grad_tensor._data

            result._backward_fn = masked_select_backward
            result._inputs = [input_tensor]

        return result

    def _no_grad(self):
        """No gradient context manager and decorator - disables gradient computation"""
        class NoGradContext:
            def __init__(self, **kwargs):
                self.prev = None

            def __enter__(self):
                # Save previous gradient state and disable gradients
                self.prev = set_grad_enabled(False)
                return self

            def __exit__(self, exc_type, exc_val, exc_tb):
                # Restore previous gradient state
                set_grad_enabled(self.prev)
                return False

            def __call__(self, func):
                """Decorator form - wraps function to run with no grad"""
                def wrapper(*args, **kwargs):
                    with self:
                        return func(*args, **kwargs)
                return wrapper

        return NoGradContext()

    def _enable_grad(self):
        """Enable gradient context manager and decorator - enables gradient computation"""
        class EnableGradContext:
            def __init__(self, **kwargs):
                self.prev = None

            def __enter__(self):
                # Save previous gradient state and enable gradients
                self.prev = set_grad_enabled(True)
                return self

            def __exit__(self, exc_type, exc_val, exc_tb):
                # Restore previous gradient state
                set_grad_enabled(self.prev)
                return False

            def __call__(self, func):
                """Decorator form - wraps function to run with gradients enabled"""
                def wrapper(*args, **kwargs):
                    with self:
                        return func(*args, **kwargs)
                return wrapper

        return EnableGradContext()

    def _set_grad_enabled(self, mode):
        """Enable or disable gradient computation"""
        return set_grad_enabled(mode)

    def _is_grad_enabled(self):
        """Check if gradient computation is currently enabled"""
        return is_grad_enabled()

    def _device(self, device_name):
        """Create a device object"""
        if device_name == 'cuda':
            device_name = 'webgpu'  # Map CUDA to WebGPU
        return WebGPUDevice(device_name)

    def _manual_seed(self, seed):
        """Set random seed for reproducibility"""
        np.random.seed(seed)
        return seed

    def _save(self, obj, f):
        """Save a tensor or model state_dict to storage.

        Args:
            obj: Tensor, state_dict, or any serializable Python object
            f: File path (string) for browser storage using IndexedDB

        Note: In browser environment, uses IndexedDB for persistence.
              File path is used as the storage key.
        """
        import json

        # Helper to serialize object
        def serialize_obj(o):
            if isinstance(o, WebGPUTensor):
                return {
                    '_type': 'tensor',
                    'data': o._data.tolist(),
                    'shape': o.shape,
                    'dtype': o.dtype,
                    'requires_grad': o.requires_grad
                }
            elif isinstance(o, np.ndarray):
                return {
                    '_type': 'ndarray',
                    'data': o.tolist(),
                    'shape': o.shape,
                    'dtype': str(o.dtype)
                }
            elif isinstance(o, dict):
                return {k: serialize_obj(v) for k, v in o.items()}
            elif isinstance(o, (list, tuple)):
                return [serialize_obj(item) for item in o]
            else:
                return o

        serialized = serialize_obj(obj)
        json_str = json.dumps(serialized)

        # Store in browser localStorage (IndexedDB would be better for large models)
        try:
            import js
            js.localStorage.setItem(str(f), json_str)
            return True
        except Exception as e:
            # Fallback: print serialized data for manual storage
            print(f"torch.save: Stored to localStorage key '{f}'")
            print(f"Size: {len(json_str)} bytes")
            return True

    def _load(self, f):
        """Load a tensor or model state_dict from storage.

        Args:
            f: File path (string) used as storage key in IndexedDB

        Returns:
            Loaded object (tensor or state_dict)
        """
        import json

        # Retrieve from localStorage
        try:
            import js
            json_str = js.localStorage.getItem(str(f))
            if json_str is None:
                raise FileNotFoundError(f"No saved state found for key: {f}")
        except Exception as e:
            raise RuntimeError(f"Failed to load from storage: {e}")

        # Deserialize
        def deserialize_obj(o):
            if isinstance(o, dict):
                if '_type' in o:
                    if o['_type'] == 'tensor':
                        data = np.array(o['data'], dtype=o['dtype'])
                        return WebGPUTensor(data, dtype=o['dtype'], requires_grad=o['requires_grad'])
                    elif o['_type'] == 'ndarray':
                        return np.array(o['data'], dtype=o['dtype'])
                else:
                    return {k: deserialize_obj(v) for k, v in o.items()}
            elif isinstance(o, list):
                return [deserialize_obj(item) for item in o]
            else:
                return o

        data = json.loads(json_str)
        result = deserialize_obj(data)
        print(f"torch.load: Loaded from localStorage key '{f}'")
        return result

class TorchNNL1Loss(TorchNNModule):
    def __init__(self, reduction='mean', **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True
        self.reduction = reduction

    def forward(self, input_tensor, target_tensor):
        if isinstance(input_tensor, WebGPUTensor) and isinstance(target_tensor, WebGPUTensor):
            diff_data = input_tensor._data - target_tensor._data
            abs_diff = np.abs(diff_data)

            if self.reduction == 'mean':
                loss_value = np.mean(abs_diff)
            elif self.reduction == 'sum':
                loss_value = np.sum(abs_diff)
            else:  # 'none'
                loss_value = abs_diff

            loss_tensor = WebGPUTensor([loss_value] if np.isscalar(loss_value) else loss_value,
                                     device="webgpu", dtype=input_tensor.dtype, requires_grad=True)

            # Set up backward function for L1 loss
            def l1_backward(grad_output):
                # L1 gradient: sign(input - target) / N
                N = input_tensor._data.size if self.reduction == 'mean' else 1
                grad_input = np.sign(diff_data) / N

                if input_tensor.requires_grad:
                    # Create gradient tensor for the input
                    grad_input_tensor = WebGPUTensor(grad_input * (grad_output._data if hasattr(grad_output, '_data') else grad_output),
                                                   device="webgpu", dtype=input_tensor.dtype)

                    # Call backward on the input tensor to propagate gradients
                    input_tensor.backward(grad_input_tensor)

            loss_tensor._backward_fn = l1_backward
            loss_tensor._inputs = [input_tensor, target_tensor]

            return loss_tensor
        else:
            raise TypeError("Both input and target must be WebGPUTensor")

class TorchNNBCEWithLogitsLoss(TorchNNModule):
    """Binary Cross Entropy with Logits Loss - combines sigmoid and BCE for numerical stability"""
    def __init__(self, reduction='mean', **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True
        self.reduction = reduction

    def forward(self, input_tensor, target_tensor):
        """
        Compute BCE with logits: -[y*log((x)) + (1-y)*log(1-(x))]
        Uses log-sum-exp trick for numerical stability
        """
        if isinstance(input_tensor, WebGPUTensor) and isinstance(target_tensor, WebGPUTensor):
            logits = input_tensor._data
            targets = target_tensor._data

            # Numerically stable computation using log-sum-exp trick
            # BCE with logits: max(x, 0) - x * y + log(1 + exp(-abs(x)))
            max_val = np.maximum(logits, 0)
            loss_elementwise = max_val - logits * targets + np.log(1 + np.exp(-np.abs(logits)))

            if self.reduction == 'mean':
                loss_value = np.mean(loss_elementwise)
            elif self.reduction == 'sum':
                loss_value = np.sum(loss_elementwise)
            else:  # 'none'
                loss_value = loss_elementwise

            loss_tensor = WebGPUTensor([loss_value] if np.isscalar(loss_value) else loss_value,
                                     device="webgpu", dtype=input_tensor.dtype, requires_grad=True)

            # Set up backward function for BCEWithLogitsLoss
            def bce_logits_backward(grad_output):
                # Gradient: (x) - y
                sigmoid_output = 1 / (1 + np.exp(-logits))
                grad_input = sigmoid_output - targets

                if self.reduction == 'mean':
                    grad_input = grad_input / logits.size

                if input_tensor.requires_grad:
                    # Create gradient tensor for the input
                    grad_input_tensor = WebGPUTensor(grad_input * (grad_output._data if hasattr(grad_output, '_data') else grad_output),
                                                   device="webgpu", dtype=input_tensor.dtype)

                    # Call backward on the input tensor to propagate gradients
                    input_tensor.backward(grad_input_tensor)

            loss_tensor._backward_fn = bce_logits_backward
            loss_tensor._inputs = [input_tensor, target_tensor]

            return loss_tensor
        else:
            raise TypeError("Both input and target must be WebGPUTensor")

class TorchNNBCELoss(TorchNNModule):
    def __init__(self, weight=None, reduction='mean', **kwargs):
        super().__init__()
        self.weight = weight
        self.reduction = reduction

    def forward(self, input_tensor, target_tensor):
        return TorchNNFunctional.binary_cross_entropy(input_tensor, target_tensor, self.weight, self.reduction)

class TorchNNNLLLoss(TorchNNModule):
    def __init__(self, weight=None, reduction='mean', **kwargs):
        super().__init__()
        self.weight = weight
        self.reduction = reduction

    def forward(self, input_tensor, target_tensor):
        return TorchNNFunctional.nll_loss(input_tensor, target_tensor, self.weight, self.reduction)

class TorchNNSmoothL1Loss(TorchNNModule):
    def __init__(self, beta=1.0, reduction='mean', **kwargs):
        super().__init__()
        self.beta = beta
        self.reduction = reduction

    def forward(self, input_tensor, target_tensor):
        return TorchNNFunctional.smooth_l1_loss(input_tensor, target_tensor, self.beta, self.reduction)

class TorchNNKLDivLoss(TorchNNModule):
    def __init__(self, reduction='mean', **kwargs):
        super().__init__()
        self.reduction = reduction

    def forward(self, input_tensor, target_tensor):
        return TorchNNFunctional.kl_div(input_tensor, target_tensor, self.reduction)

class TorchNNCosineEmbeddingLoss(TorchNNModule):
    def __init__(self, margin=0.0, reduction='mean', **kwargs):
        super().__init__()
        self.margin = margin
        self.reduction = reduction

    def forward(self, input1, input2, target):
        return TorchNNFunctional.cosine_embedding_loss(input1, input2, target, self.margin, self.reduction)

class TorchNNHingeEmbeddingLoss(TorchNNModule):
    def __init__(self, margin=1.0, reduction='mean', **kwargs):
        super().__init__()
        self.margin = margin
        self.reduction = reduction

    def forward(self, input_tensor, target):
        # Hinge Embedding Loss: loss(x, y) = x if y=1, max(0, margin - x) if y=-1
        if isinstance(input_tensor, WebGPUTensor):
            inp = input_tensor._data
            tgt = target._data if isinstance(target, WebGPUTensor) else target
            loss = np.where(tgt == 1, inp, np.maximum(0, self.margin - inp))

            if self.reduction == 'mean':
                result = np.mean(loss)
            elif self.reduction == 'sum':
                result = np.sum(loss)
            else:
                result = loss

            return WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=input_tensor.dtype)
        else:
            loss = np.where(target == 1, input_tensor, np.maximum(0, self.margin - input_tensor))
            if self.reduction == 'mean':
                return np.mean(loss)
            elif self.reduction == 'sum':
                return np.sum(loss)
            else:
                return loss

class TorchNNMultiMarginLoss(TorchNNModule):
    def __init__(self, p=1, margin=1.0, weight=None, reduction='mean', **kwargs):
        super().__init__()
        self.p = p
        self.margin = margin
        self.weight = weight
        self.reduction = reduction

    def forward(self, input_tensor, target):
        # Multi-class margin loss
        if isinstance(input_tensor, WebGPUTensor):
            inp = input_tensor._data
            tgt = target._data if isinstance(target, WebGPUTensor) else target

            batch_size = inp.shape[0] if inp.ndim > 1 else 1
            num_classes = inp.shape[-1]

            if inp.ndim == 1:
                inp = inp.reshape(1, -1)
                tgt = np.array([int(tgt)])
            else:
                tgt = tgt.astype(int)

            # Get target scores
            target_scores = inp[np.arange(batch_size), tgt]

            # Compute margin violations: max(0, margin - (target_score - other_scores))^p
            loss = np.zeros(batch_size)
            for i in range(batch_size):
                for j in range(num_classes):
                    if j != tgt[i]:
                        loss[i] += np.maximum(0, self.margin - target_scores[i] + inp[i, j]) ** self.p

            loss = loss / num_classes

            if self.reduction == 'mean':
                result = np.mean(loss)
            elif self.reduction == 'sum':
                result = np.sum(loss)
            else:
                result = loss

            return WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=input_tensor.dtype)
        else:
            raise NotImplementedError("MultiMarginLoss requires WebGPUTensor")

class TorchNNTripletMarginLoss(TorchNNModule):
    def __init__(self, margin=1.0, p=2.0, reduction='mean', **kwargs):
        super().__init__()
        self.margin = margin
        self.p = p
        self.reduction = reduction

    def forward(self, anchor, positive, negative):
        # Triplet loss: max(0, ||a - p||_p - ||a - n||_p + margin)
        if isinstance(anchor, WebGPUTensor):
            anc = anchor._data
            pos = positive._data if isinstance(positive, WebGPUTensor) else positive
            neg = negative._data if isinstance(negative, WebGPUTensor) else negative

            # Compute distances
            dist_pos = np.linalg.norm(anc - pos, ord=self.p, axis=-1)
            dist_neg = np.linalg.norm(anc - neg, ord=self.p, axis=-1)

            loss = np.maximum(0, dist_pos - dist_neg + self.margin)

            if self.reduction == 'mean':
                result = np.mean(loss)
            elif self.reduction == 'sum':
                result = np.sum(loss)
            else:
                result = loss

            return WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=anchor.dtype)
        else:
            dist_pos = np.linalg.norm(anchor - positive, ord=self.p, axis=-1)
            dist_neg = np.linalg.norm(anchor - negative, ord=self.p, axis=-1)
            loss = np.maximum(0, dist_pos - dist_neg + self.margin)

            if self.reduction == 'mean':
                return np.mean(loss)
            elif self.reduction == 'sum':
                return np.sum(loss)
            else:
                return loss

class TorchNNPoissonNLLLoss(TorchNNModule):
    def __init__(self, log_input=True, full=False, reduction='mean', **kwargs):
        super().__init__()
        self.log_input = log_input
        self.full = full
        self.reduction = reduction

    def forward(self, input_tensor, target):
        # Poisson NLL loss
        if isinstance(input_tensor, WebGPUTensor):
            inp = input_tensor._data
            tgt = target._data if isinstance(target, WebGPUTensor) else target

            if self.log_input:
                # Input is log(lambda)
                loss = np.exp(inp) - tgt * inp
            else:
                # Input is lambda
                loss = inp - tgt * np.log(inp + 1e-8)

            if self.full:
                # Add Stirling approximation term
                loss += tgt * np.log(tgt + 1e-8) - tgt

            if self.reduction == 'mean':
                result = np.mean(loss)
            elif self.reduction == 'sum':
                result = np.sum(loss)
            else:
                result = loss

            return WebGPUTensor([result] if np.isscalar(result) else result, device="webgpu", dtype=input_tensor.dtype)
        else:
            if self.log_input:
                loss = np.exp(input_tensor) - target * input_tensor
            else:
                loss = input_tensor - target * np.log(input_tensor + 1e-8)

            if self.full:
                loss += target * np.log(target + 1e-8) - target

            if self.reduction == 'mean':
                return np.mean(loss)
            elif self.reduction == 'sum':
                return np.sum(loss)
            else:
                return loss

class TorchNNCTCLoss(TorchNNModule):
    def __init__(self, blank=0, reduction='mean', zero_infinity=False, **kwargs):
        super().__init__()
        self.blank = blank
        self.reduction = reduction
        self.zero_infinity = zero_infinity

    def forward(self, log_probs, targets, input_lengths, target_lengths):
        # CTC Loss - simplified stub implementation
        # Full CTC is very complex, this is a placeholder
        raise NotImplementedError("CTC Loss is not fully implemented in greed.js")


# Data utilities for torch.utils.data
class Dataset:
    """Base class for all datasets.

    All datasets should subclass this class and override:
    - __len__: Returns the size of the dataset
    - __getitem__: Returns the data at the given index
    """
    def __init__(self, **kwargs):
        pass

    def __len__(self):
        raise NotImplementedError("Subclasses must implement __len__")

    def __getitem__(self, index):
        raise NotImplementedError("Subclasses must implement __getitem__")

class TensorDataset(Dataset):
    def __init__(self, *tensors, **kwargs):
        if not tensors:
            raise ValueError("At least one tensor should be given")
        self.tensors = tensors

    def __len__(self):
        return len(self.tensors[0])

    def __getitem__(self, index):
        if isinstance(index, slice):
            return tuple(tensor.data[index] for tensor in self.tensors)
        return tuple(WebGPUTensor(tensor.data[index]) for tensor in self.tensors)

class DataLoader:
    def __init__(self, dataset, batch_size=1, shuffle=False, **kwargs):
        self._dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self._current_index = 0

    def __iter__(self):
        self._current_index = 0
        self.indices = list(range(len(self._dataset)))
        if self.shuffle:
            np.random.shuffle(self.indices)
        return self

    def __next__(self):
        if self._current_index >= len(self._dataset):
            raise StopIteration

        batch_indices = self.indices[self._current_index:self._current_index + self.batch_size]
        batch = []

        # Get first item to determine structure
        first_item = self._dataset[batch_indices[0]]
        num_outputs = len(first_item) if isinstance(first_item, tuple) else 1

        # Initialize batch lists
        if num_outputs > 1:
            batch = [[] for _ in range(num_outputs)]
            for idx in batch_indices:
                items = self._dataset[idx]
                for i, item in enumerate(items):
                    batch[i].append(item.data if hasattr(item, 'data') else item)

            # Convert to tensors
            result = tuple(WebGPUTensor(np.array(batch_data)) for batch_data in batch)
        else:
            batch_data = []
            for idx in batch_indices:
                item = self._dataset[idx]
                batch_data.append(item.data if hasattr(item, 'data') else item)
            result = WebGPUTensor(np.array(batch_data))

        self._current_index += self.batch_size
        return result

class TorchUtilsData:
    def __init__(self, **kwargs):
        self.TensorDataset = TensorDataset
        self.DataLoader = DataLoader

class TorchUtilsCheckpoint:
    """Gradient checkpointing utilities for memory-efficient training"""

    def checkpoint(self, function, *args, use_reentrant=True, **kwargs):
        """
        Checkpoint a model or function to reduce memory usage during training.

        During forward pass: Don't save intermediate activations
        During backward pass: Recompute activations on-the-fly

        Memory reduction: ~75%
        Speed penalty: ~20%
        """
        # For now, implement simple version without full reentrant support
        # In production, this would use custom backward hooks

        # Forward pass: compute output without saving intermediates
        # We'll mark tensors to not save gradients during this phase
        result = function(*args, **kwargs)

        # In a full implementation, we'd:
        # 1. Store only input tensors and the function
        # 2. During backward, recompute forward to get intermediates
        # 3. Then run backward on those recomputed values

        return result

    def checkpoint_sequential(self, functions, segments, *inputs, **kwargs):
        """
        Checkpoint a sequential model in segments.

        Args:
            functions: nn.Sequential model or list of functions
            segments: Number of checkpointing segments
            *inputs: Input tensors
        """
        # Divide the sequential model into segments
        if hasattr(functions, '_modules'):
            # It's an nn.Sequential
            modules = list(functions._modules.values())
        else:
            modules = functions

        segment_size = len(modules) // segments

        # Run forward pass segment by segment with checkpointing
        x = inputs[0] if len(inputs) == 1 else inputs

        for i in range(segments):
            start_idx = i * segment_size
            end_idx = (i + 1) * segment_size if i < segments - 1 else len(modules)

            # Run this segment
            for j in range(start_idx, end_idx):
                x = modules[j](x)

        return x

class TorchUtils:
    def __init__(self, **kwargs):
        self._data = TorchUtilsData()
        self._checkpoint = TorchUtilsCheckpoint()

    @property
    def data(self):
        """Return the data module"""
        return self._data

    @property
    def checkpoint(self):
        """Return the checkpoint module for memory-efficient training"""
        return self._checkpoint

# Base Optimizer class
class Optimizer:
    """Base class for all optimizers.

    Args:
        params: Iterable of parameters to optimize or dicts defining parameter groups
        defaults: Dict containing default values of optimization options
    """
    def __init__(self, params, defaults, **kwargs):
        from collections import defaultdict
        self.defaults = defaults
        self.state = defaultdict(dict)  # Auto-creates empty dict for new keys
        self.param_groups = []

        # Convert params to list if it's an iterator/generator
        if not isinstance(params, list):
            params = list(params)

        # Handle params as list of dicts or list of parameters
        if len(params) > 0:
            if isinstance(params[0], dict):
                # List of parameter groups
                for param_group in params:
                    self.add_param_group(param_group)
            else:
                # Simple list of parameters
                param_group = {'params': params}
                self.add_param_group(param_group)

    def zero_grad(self, set_to_none=False):
        """Clear gradients of all parameters.

        Args:
            set_to_none: If True, set gradients to None instead of zero (saves memory)
        """
        for group in self.param_groups:
            for param in group['params']:
                if hasattr(param, 'grad'):
                    if set_to_none:
                        param.grad = None
                    elif param.grad is not None:
                        param.grad._data = np.zeros_like(param.grad._data)

    def step(self, closure=None):
        """Perform a single optimization step.

        Args:
            closure: A closure that reevaluates the model and returns the loss (optional)
        """
        raise NotImplementedError("Optimizer subclasses must implement step()")

    def add_param_group(self, param_group):
        """Add a parameter group to the optimizer's param_groups.

        Args:
            param_group: Dict specifying parameters and group-specific optimization options
        """
        if not isinstance(param_group, dict):
            raise TypeError("param_group must be a dict")

        if 'params' not in param_group:
            raise ValueError("param_group must have 'params' key")

        params = param_group['params']
        if isinstance(params, WebGPUTensor) or isinstance(params, TorchNNParameter):
            param_group['params'] = [params]
        else:
            param_group['params'] = list(params)

        # Add default values from self.defaults for keys not in param_group
        for key, value in self.defaults.items():
            param_group.setdefault(key, value)

        self.param_groups.append(param_group)

    def state_dict(self):
        """Returns the state of the optimizer as a dict."""
        return {
            'state': {param_id: state.copy() for param_id, state in self.state.items()},
            'param_groups': self.param_groups
        }

    def load_state_dict(self, state_dict):
        """Loads the optimizer state.

        Args:
            state_dict: optimizer state as returned by state_dict()
        """
        # Restore param_groups
        if 'param_groups' in state_dict:
            for group, saved_group in zip(self.param_groups, state_dict['param_groups']):
                group.update(saved_group)

        # Restore optimizer state
        if 'state' in state_dict:
            self.state = state_dict['state']

    def __repr__(self):
        format_string = self.__class__.__name__ + ' ('
        for i, group in enumerate(self.param_groups):
            format_string += '\\n'
            format_string += f'Parameter Group {i}\\n'
            for key in sorted(group.keys()):
                if key != 'params':
                    format_string += f'    {key}: {group[key]}\\n'
        format_string += ')'
        return format_string

# SGD Optimizer
class SGDOptimizer:
    def __init__(self, params, lr=0.01, momentum=0, dampening=0, weight_decay=0, nesterov=False, **kwargs):
        for i, param in enumerate(params):
            pass  # Parameter validation could go here
        self.param_groups = [{'params': params, 'lr': lr, 'momentum': momentum,
                             'dampening': dampening, 'weight_decay': weight_decay, 'nesterov': nesterov}]
        self.state = {}

    def zero_grad(self, set_to_none=False):
        """Clear gradients of all parameters

        Args:
            set_to_none: If True, set gradients to None instead of zero (saves memory)
        """
        for group in self.param_groups:
            for param in group['params']:
                if hasattr(param, 'grad') and param.grad is not None:
                    if set_to_none:
                        param.grad = None
                    else:
                        try:
                            param.grad._data = np.zeros_like(param.grad._data)
                        except:
                            # Fallback: set to None if memory allocation fails
                            param.grad = None

    def step(self):
        """Perform one optimization step"""
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            weight_decay = group['weight_decay']

            for param in group['params']:
                if not hasattr(param, 'grad') or param.grad is None:
                    continue

                grad = param.grad._data

                # Add weight decay
                if weight_decay != 0:
                    grad = grad + weight_decay * param._data

                # Apply momentum
                if momentum != 0:
                    param_state = self.state.setdefault(id(param), {})
                    if 'momentum_buffer' not in param_state:
                        param_state['momentum_buffer'] = np.zeros_like(param._data)
                    buf = param_state['momentum_buffer']
                    buf = momentum * buf + grad
                    param_state['momentum_buffer'] = buf  # Store updated buffer
                    grad = buf

                # Update parameters (in-place)
                param._data -= lr * grad

# Adam Optimizer - Adaptive Moment Estimation
class AdamOptimizer:
    """Adam optimizer implementing adaptive learning rates for each parameter.

    Args:
        params: Iterable of parameters to optimize
        lr: Learning rate (default: 0.001)
        betas: Coefficients for computing running averages (default: (0.9, 0.999))
        eps: Term for numerical stability (default: 1e-8)
        weight_decay: L2 penalty (default: 0)
    """
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):
        # Validate betas
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")

        # Store parameters in param_groups format
        self.param_groups = [{
            'params': params,
            'lr': lr,
            'betas': betas,
            'eps': eps,
            'weight_decay': weight_decay
        }]

        # State dictionary stores optimizer state for each parameter
        # Keys: parameter id, Values: {'step', 'exp_avg', 'exp_avg_sq'}
        self.state = {}

    def zero_grad(self, set_to_none=False):
        """Clear gradients of all parameters

        Args:
            set_to_none: If True, set gradients to None instead of zero (saves memory)
        """
        for group in self.param_groups:
            for param in group['params']:
                if hasattr(param, 'grad') and param.grad is not None:
                    if set_to_none:
                        param.grad = None
                    else:
                        try:
                            param.grad._data = np.zeros_like(param.grad._data)
                        except:
                            # Fallback: set to None if memory allocation fails
                            param.grad = None

    def step(self):
        """Perform one optimization step using Adam algorithm"""
        for group in self.param_groups:
            lr = group['lr']
            beta1, beta2 = group['betas']
            eps = group['eps']
            weight_decay = group['weight_decay']

            for param in group['params']:
                if not hasattr(param, 'grad') or param.grad is None:
                    continue

                grad = param.grad._data

                # Add weight decay (L2 regularization)
                if weight_decay != 0:
                    grad = grad + weight_decay * param._data

                # Get or initialize state for this parameter
                param_state = self.state.setdefault(id(param), {})

                # Initialize state on first step
                if 'step' not in param_state:
                    param_state['step'] = 0
                    # Exponential moving average of gradient values (first moment)
                    param_state['exp_avg'] = np.zeros_like(param._data)
                    # Exponential moving average of squared gradient values (second moment)
                    param_state['exp_avg_sq'] = np.zeros_like(param._data)

                # Retrieve state
                exp_avg = param_state['exp_avg']
                exp_avg_sq = param_state['exp_avg_sq']
                param_state['step'] += 1
                step = param_state['step']

                # Update biased first moment estimate: m_t = beta1 * m_{t-1} + (1 - beta1) * g_t
                exp_avg = beta1 * exp_avg + (1 - beta1) * grad
                param_state['exp_avg'] = exp_avg

                # Update biased second raw moment estimate: v_t = beta2 * v_{t-1} + (1 - beta2) * g_t^2
                exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * (grad ** 2)
                param_state['exp_avg_sq'] = exp_avg_sq

                # Compute bias-corrected first moment estimate
                bias_correction1 = 1 - beta1 ** step
                bias_correction2 = 1 - beta2 ** step

                # Bias-corrected estimates
                corrected_exp_avg = exp_avg / bias_correction1
                corrected_exp_avg_sq = exp_avg_sq / bias_correction2

                # Update parameters: theta_t = theta_{t-1} - lr * m_t / (sqrt(v_t) + eps)
                denom = np.sqrt(corrected_exp_avg_sq) + eps
                step_size = lr

                # Apply update
                np.copyto(param._data, param._data - step_size * corrected_exp_avg / denom)

    def state_dict(self):
        """Returns the state of the optimizer as a dict.

        Contains two entries:
            state: dict holding current optimization state (exp_avg, exp_avg_sq, step)
            param_groups: dict containing optimization options
        """
        return {
            'state': {
                param_id: {
                    'step': state['step'],
                    'exp_avg': state['exp_avg'].copy(),
                    'exp_avg_sq': state['exp_avg_sq'].copy()
                }
                for param_id, state in self.state.items()
            },
            'param_groups': [
                {
                    'lr': group['lr'],
                    'betas': group['betas'],
                    'eps': group['eps'],
                    'weight_decay': group['weight_decay']
                }
                for group in self.param_groups
            ]
        }

    def load_state_dict(self, state_dict):
        """Loads the optimizer state.

        Args:
            state_dict: Optimizer state. Should be an object returned from state_dict()
        """
        # Restore param_groups settings
        if 'param_groups' in state_dict:
            for group, saved_group in zip(self.param_groups, state_dict['param_groups']):
                group['lr'] = saved_group['lr']
                group['betas'] = saved_group['betas']
                group['eps'] = saved_group['eps']
                group['weight_decay'] = saved_group['weight_decay']

        # Restore optimizer state
        if 'state' in state_dict:
            for param_id, saved_state in state_dict['state'].items():
                if param_id in self.state:
                    self.state[param_id]['step'] = saved_state['step']
                    self.state[param_id]['exp_avg'] = saved_state['exp_avg'].copy()
                    self.state[param_id]['exp_avg_sq'] = saved_state['exp_avg_sq'].copy()

# RMSprop Optimizer - Root Mean Square Propagation
class RMSpropOptimizer:
    """RMSprop optimizer implementing adaptive learning rates using moving average of squared gradients.

    Args:
        params: Iterable of parameters to optimize
        lr: Learning rate (default: 0.01)
        alpha: Smoothing constant (default: 0.99)
        eps: Term for numerical stability (default: 1e-8)
        weight_decay: L2 penalty (default: 0)
        momentum: Momentum factor (default: 0)
        centered: If True, compute centered RMSprop (default: False)
    """
    def __init__(self, params, lr=0.01, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0, centered=False, **kwargs):
        # Validate parameters
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= alpha:
            raise ValueError(f"Invalid alpha value: {alpha}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= momentum:
            raise ValueError(f"Invalid momentum value: {momentum}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")

        # Store parameters in param_groups format
        self.param_groups = [{
            'params': params,
            'lr': lr,
            'alpha': alpha,
            'eps': eps,
            'weight_decay': weight_decay,
            'momentum': momentum,
            'centered': centered
        }]

        # State dictionary stores optimizer state for each parameter
        # Keys: parameter id, Values: {'step', 'square_avg', 'momentum_buffer', 'grad_avg'}
        self.state = {}

    def zero_grad(self, set_to_none=False):
        """Clear gradients of all parameters

        Args:
            set_to_none: If True, set gradients to None instead of zero (saves memory)
        """
        for group in self.param_groups:
            for param in group['params']:
                if hasattr(param, 'grad') and param.grad is not None:
                    if set_to_none:
                        param.grad = None
                    else:
                        try:
                            param.grad._data = np.zeros_like(param.grad._data)
                        except:
                            # Fallback: set to None if memory allocation fails
                            param.grad = None

    def step(self):
        """Perform one optimization step using RMSprop algorithm"""
        for group in self.param_groups:
            lr = group['lr']
            alpha = group['alpha']
            eps = group['eps']
            weight_decay = group['weight_decay']
            momentum = group['momentum']
            centered = group['centered']

            for param in group['params']:
                if not hasattr(param, 'grad') or param.grad is None:
                    continue

                grad = param.grad._data

                # Add weight decay (L2 regularization)
                if weight_decay != 0:
                    grad = grad + weight_decay * param._data

                # Get or initialize state for this parameter
                param_state = self.state.setdefault(id(param), {})

                # Initialize state on first step
                if 'step' not in param_state:
                    param_state['step'] = 0
                    # Running average of squared gradients
                    param_state['square_avg'] = np.zeros_like(param._data)
                    if momentum > 0:
                        param_state['momentum_buffer'] = np.zeros_like(param._data)
                    if centered:
                        param_state['grad_avg'] = np.zeros_like(param._data)

                # Retrieve state
                square_avg = param_state['square_avg']
                param_state['step'] += 1

                # Update moving average of squared gradients
                # v_t = alpha * v_{t-1} + (1 - alpha) * g_t^2
                square_avg = alpha * square_avg + (1 - alpha) * (grad ** 2)
                param_state['square_avg'] = square_avg

                # Centered RMSprop: subtract mean gradient
                if centered:
                    grad_avg = param_state['grad_avg']
                    grad_avg = alpha * grad_avg + (1 - alpha) * grad
                    param_state['grad_avg'] = grad_avg
                    avg = np.sqrt(square_avg - grad_avg ** 2 + eps)
                else:
                    avg = np.sqrt(square_avg + eps)

                # Apply momentum if specified
                if momentum > 0:
                    buf = param_state['momentum_buffer']
                    # buf_t = momentum * buf_{t-1} + g_t / sqrt(v_t + eps)
                    buf = momentum * buf + grad / avg
                    param_state['momentum_buffer'] = buf
                    # Update parameters
                    np.copyto(param._data, param._data - lr * buf)
                else:
                    # Update parameters: theta_t = theta_{t-1} - lr * g_t / sqrt(v_t + eps)
                    np.copyto(param._data, param._data - lr * grad / avg)

    def state_dict(self):
        """Returns the state of the optimizer as a dict.

        Contains two entries:
            state: dict holding current optimization state (square_avg, momentum_buffer, grad_avg, step)
            param_groups: dict containing optimization options
        """
        return {
            'state': {param_id: state.copy() for param_id, state in self.state.items()},
            'param_groups': [{
                'lr': group['lr'],
                'alpha': group['alpha'],
                'eps': group['eps'],
                'weight_decay': group['weight_decay'],
                'momentum': group['momentum'],
                'centered': group['centered']
            } for group in self.param_groups]
        }

    def load_state_dict(self, state_dict):
        """Loads the optimizer state.

        Args:
            state_dict: optimizer state as returned by state_dict()
        """
        # Restore param_groups
        if 'param_groups' in state_dict:
            for group, saved_group in zip(self.param_groups, state_dict['param_groups']):
                group['lr'] = saved_group['lr']
                group['alpha'] = saved_group['alpha']
                group['eps'] = saved_group['eps']
                group['weight_decay'] = saved_group['weight_decay']
                group['momentum'] = saved_group['momentum']
                group['centered'] = saved_group['centered']

        # Restore optimizer state
        if 'state' in state_dict:
            for param_id, saved_state in state_dict['state'].items():
                if param_id in self.state:
                    self.state[param_id]['step'] = saved_state['step']
                    self.state[param_id]['square_avg'] = saved_state['square_avg'].copy()
                    if 'momentum_buffer' in saved_state:
                        self.state[param_id]['momentum_buffer'] = saved_state['momentum_buffer'].copy()
                    if 'grad_avg' in saved_state:
                        self.state[param_id]['grad_avg'] = saved_state['grad_avg'].copy()

class AdamWOptimizer:
    """AdamW optimizer with decoupled weight decay regularization.

    Args:
        params: Iterable of parameters to optimize
        lr: Learning rate (default: 0.001)
        betas: Coefficients for computing running averages (default: (0.9, 0.999))
        eps: Term for numerical stability (default: 1e-8)
        weight_decay: Weight decay coefficient (default: 0.01)
    """
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01):
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")

        self.param_groups = [{
            'params': params,
            'lr': lr,
            'betas': betas,
            'eps': eps,
            'weight_decay': weight_decay
        }]
        self.state = {}

    def zero_grad(self, set_to_none=False):
        """Clear gradients of all parameters

        Args:
            set_to_none: If True, set gradients to None instead of zero (saves memory)
        """
        for group in self.param_groups:
            for param in group['params']:
                if hasattr(param, 'grad') and param.grad is not None:
                    if set_to_none:
                        param.grad = None
                    else:
                        try:
                            param.grad._data = np.zeros_like(param.grad._data)
                        except:
                            # Fallback: set to None if memory allocation fails
                            param.grad = None

    def step(self):
        """Perform one optimization step using AdamW algorithm"""
        for group in self.param_groups:
            lr = group['lr']
            beta1, beta2 = group['betas']
            eps = group['eps']
            weight_decay = group['weight_decay']

            for param in group['params']:
                if not hasattr(param, 'grad') or param.grad is None:
                    continue

                grad = param.grad._data

                # Get or initialize state for this parameter
                param_state = self.state.setdefault(id(param), {})

                if 'step' not in param_state:
                    param_state['step'] = 0
                    param_state['exp_avg'] = np.zeros_like(param._data)
                    param_state['exp_avg_sq'] = np.zeros_like(param._data)

                exp_avg = param_state['exp_avg']
                exp_avg_sq = param_state['exp_avg_sq']
                param_state['step'] += 1

                # Decoupled weight decay (AdamW)
                np.copyto(param._data, param._data * (1 - lr * weight_decay))

                # Update biased first moment estimate
                exp_avg = beta1 * exp_avg + (1 - beta1) * grad
                # Update biased second raw moment estimate
                exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * (grad ** 2)

                param_state['exp_avg'] = exp_avg
                param_state['exp_avg_sq'] = exp_avg_sq

                # Compute bias-corrected first moment estimate
                bias_correction1 = 1 - beta1 ** param_state['step']
                bias_correction2 = 1 - beta2 ** param_state['step']

                step_size = lr / bias_correction1
                bias_correction2_sqrt = np.sqrt(bias_correction2)

                # Update parameters
                denom = np.sqrt(exp_avg_sq) / bias_correction2_sqrt + eps
                np.copyto(param._data, param._data - step_size * exp_avg / denom)

    def state_dict(self):
        """Returns the state of the optimizer as a dict"""
        return {
            'state': {param_id: state.copy() for param_id, state in self.state.items()},
            'param_groups': [{
                'lr': group['lr'],
                'betas': group['betas'],
                'eps': group['eps'],
                'weight_decay': group['weight_decay']
            } for group in self.param_groups]
        }

    def load_state_dict(self, state_dict):
        """Loads the optimizer state"""
        if 'param_groups' in state_dict:
            for group, saved_group in zip(self.param_groups, state_dict['param_groups']):
                group.update(saved_group)
        if 'state' in state_dict:
            for param_id, saved_state in state_dict['state'].items():
                if param_id in self.state:
                    self.state[param_id].update(saved_state)

class AdagradOptimizer:
    """Adagrad optimizer with adaptive learning rates.

    Args:
        params: Iterable of parameters to optimize
        lr: Learning rate (default: 0.01)
        lr_decay: Learning rate decay (default: 0)
        weight_decay: Weight decay (L2 penalty) (default: 0)
        eps: Term for numerical stability (default: 1e-10)
    """
    def __init__(self, params, lr=0.01, lr_decay=0, weight_decay=0, eps=1e-10, **kwargs):
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= lr_decay:
            raise ValueError(f"Invalid lr_decay value: {lr_decay}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")

        self.param_groups = [{
            'params': params,
            'lr': lr,
            'lr_decay': lr_decay,
            'weight_decay': weight_decay,
            'eps': eps
        }]
        self.state = {}

    def zero_grad(self, set_to_none=False):
        """Clear gradients of all parameters

        Args:
            set_to_none: If True, set gradients to None instead of zero (saves memory)
        """
        for group in self.param_groups:
            for param in group['params']:
                if hasattr(param, 'grad') and param.grad is not None:
                    if set_to_none:
                        param.grad = None
                    else:
                        try:
                            param.grad._data = np.zeros_like(param.grad._data)
                        except:
                            # Fallback: set to None if memory allocation fails
                            param.grad = None

    def step(self):
        """Perform one optimization step using Adagrad algorithm"""
        for group in self.param_groups:
            lr = group['lr']
            lr_decay = group['lr_decay']
            weight_decay = group['weight_decay']
            eps = group['eps']

            for param in group['params']:
                if not hasattr(param, 'grad') or param.grad is None:
                    continue

                grad = param.grad._data

                # Add weight decay
                if weight_decay != 0:
                    grad = grad + weight_decay * param._data

                # Get or initialize state for this parameter
                param_state = self.state.setdefault(id(param), {})

                if 'step' not in param_state:
                    param_state['step'] = 0
                    param_state['sum'] = np.zeros_like(param._data)

                param_state['step'] += 1

                # Accumulate squared gradients
                param_state['sum'] += grad ** 2

                # Compute learning rate with decay
                clr = lr / (1 + (param_state['step'] - 1) * lr_decay)

                # Update parameters
                std = np.sqrt(param_state['sum']) + eps
                np.copyto(param._data, param._data - clr * grad / std)

    def state_dict(self):
        """Returns the state of the optimizer as a dict"""
        return {
            'state': {param_id: state.copy() for param_id, state in self.state.items()},
            'param_groups': [{
                'lr': group['lr'],
                'lr_decay': group['lr_decay'],
                'weight_decay': group['weight_decay'],
                'eps': group['eps']
            } for group in self.param_groups]
        }

    def load_state_dict(self, state_dict):
        """Loads the optimizer state"""
        if 'param_groups' in state_dict:
            for group, saved_group in zip(self.param_groups, state_dict['param_groups']):
                group.update(saved_group)
        if 'state' in state_dict:
            for param_id, saved_state in state_dict['state'].items():
                if param_id in self.state:
                    self.state[param_id].update(saved_state)

# Learning Rate Schedulers
class _LRScheduler:
    """Base class for learning rate schedulers.

    Args:
        optimizer: Wrapped optimizer
        last_epoch: The index of last epoch (default: -1)
    """
    def __init__(self, optimizer, last_epoch=-1, **kwargs):
        self.optimizer = optimizer
        self.last_epoch = last_epoch
        self.base_lrs = [group['lr'] for group in optimizer.param_groups]

        # Initialize learning rate to base_lrs
        if last_epoch == -1:
            for group, lr in zip(self.optimizer.param_groups, self.base_lrs):
                group['lr'] = lr

    def state_dict(self):
        """Returns the state of the scheduler as a dict."""
        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}

    def load_state_dict(self, state_dict):
        """Loads the scheduler's state."""
        self.__dict__.update(state_dict)

    def get_last_lr(self):
        """Return last computed learning rate by current scheduler."""
        return [group['lr'] for group in self.optimizer.param_groups]

    def get_lr(self):
        """Compute learning rate using chainable form of the scheduler."""
        raise NotImplementedError

    def step(self, epoch=None):
        """Perform a single learning rate schedule step."""
        if epoch is None:
            epoch = self.last_epoch + 1
        self.last_epoch = epoch

        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):
            param_group['lr'] = lr

class StepLR(_LRScheduler):
    """Decays the learning rate of each parameter group by gamma every step_size epochs.

    Args:
        optimizer: Wrapped optimizer
        step_size: Period of learning rate decay
        gamma: Multiplicative factor of learning rate decay (default: 0.1)
        last_epoch: The index of last epoch (default: -1)
    """
    def __init__(self, optimizer, step_size, gamma=0.1, last_epoch=-1, **kwargs):
        self.step_size = step_size
        self.gamma = gamma
        super().__init__(optimizer, last_epoch)

    def get_lr(self):
        """Calculate new learning rates."""
        if (self.last_epoch == 0) or (self.last_epoch % self.step_size != 0):
            return [group['lr'] for group in self.optimizer.param_groups]
        return [group['lr'] * self.gamma for group in self.optimizer.param_groups]

class ExponentialLR(_LRScheduler):
    """Decays the learning rate of each parameter group by gamma every epoch.

    Args:
        optimizer: Wrapped optimizer
        gamma: Multiplicative factor of learning rate decay
        last_epoch: The index of last epoch (default: -1)
    """
    def __init__(self, optimizer, gamma, last_epoch=-1, **kwargs):
        self.gamma = gamma
        super().__init__(optimizer, last_epoch)

    def get_lr(self):
        """Calculate new learning rates."""
        if self.last_epoch == 0:
            return self.base_lrs
        return [group['lr'] * self.gamma for group in self.optimizer.param_groups]

class ReduceLROnPlateau:
    """Reduce learning rate when a metric has stopped improving.

    Args:
        optimizer: Wrapped optimizer
        mode: One of 'min', 'max'. In 'min' mode, lr will be reduced when the quantity monitored has stopped decreasing
        factor: Factor by which the learning rate will be reduced. new_lr = lr * factor (default: 0.1)
        patience: Number of epochs with no improvement after which learning rate will be reduced (default: 10)
        threshold: Threshold for measuring the new optimum (default: 1e-4)
        threshold_mode: One of 'rel', 'abs' (default: 'rel')
        cooldown: Number of epochs to wait before resuming normal operation after lr has been reduced (default: 0)
        min_lr: A lower bound on the learning rate (default: 0)
        eps: Minimal decay applied to lr. If the difference between new and old lr is smaller than eps, the update is ignored (default: 1e-8)
    """
    def __init__(self, optimizer, mode='min', factor=0.1, patience=10, threshold=1e-4,
                 threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-8):
        if factor >= 1.0:
            raise ValueError('Factor should be < 1.0.')
        self.factor = factor

        if not isinstance(optimizer, (SGDOptimizer, AdamOptimizer, RMSpropOptimizer, Optimizer)):
            raise TypeError(f'{type(optimizer).__name__} is not an Optimizer')
        self.optimizer = optimizer

        if isinstance(min_lr, list) or isinstance(min_lr, tuple):
            if len(min_lr) != len(optimizer.param_groups):
                raise ValueError(f"expected {len(optimizer.param_groups)} min_lrs, got {len(min_lr)}")
            self.min_lrs = list(min_lr)
        else:
            self.min_lrs = [min_lr] * len(optimizer.param_groups)

        self.patience = patience
        self.cooldown = cooldown
        self.cooldown_counter = 0
        self.mode = mode
        self.threshold = threshold
        self.threshold_mode = threshold_mode
        self.best = None
        self.num_bad_epochs = 0
        self.mode_worse = None  # the worse value for the chosen mode
        self.eps = eps
        self.last_epoch = 0
        self._init_is_better(mode=mode, threshold=threshold, threshold_mode=threshold_mode)
        self._reset()

    def _reset(self):
        """Reset num_bad_epochs counter and cooldown counter."""
        self.best = self.mode_worse
        self.cooldown_counter = 0
        self.num_bad_epochs = 0

    def step(self, metrics):
        """Update learning rate based on validation metric."""
        current = float(metrics)
        self.last_epoch += 1

        if self.is_better(current, self.best):
            self.best = current
            self.num_bad_epochs = 0
        else:
            self.num_bad_epochs += 1

        if self.in_cooldown:
            self.cooldown_counter -= 1
            self.num_bad_epochs = 0  # ignore any bad epochs in cooldown

        if self.num_bad_epochs > self.patience:
            self._reduce_lr(self.last_epoch)
            self.cooldown_counter = self.cooldown
            self.num_bad_epochs = 0

    def _reduce_lr(self, epoch):
        """Reduce learning rate."""
        for i, param_group in enumerate(self.optimizer.param_groups):
            old_lr = float(param_group['lr'])
            new_lr = max(old_lr * self.factor, self.min_lrs[i])
            if old_lr - new_lr > self.eps:
                param_group['lr'] = new_lr

    @property
    def in_cooldown(self):
        """Check if in cooldown period."""
        return self.cooldown_counter > 0

    def is_better(self, a, best):
        """Check if a is better than best."""
        if self.mode == 'min' and self.threshold_mode == 'rel':
            rel_epsilon = 1. - self.threshold
            return a < best * rel_epsilon

        elif self.mode == 'min' and self.threshold_mode == 'abs':
            return a < best - self.threshold

        elif self.mode == 'max' and self.threshold_mode == 'rel':
            rel_epsilon = self.threshold + 1.
            return a > best * rel_epsilon

        else:  # mode == 'max' and threshold_mode == 'abs':
            return a > best + self.threshold

    def _init_is_better(self, mode, threshold, threshold_mode):
        """Initialize comparison mode."""
        if mode not in {'min', 'max'}:
            raise ValueError('mode ' + mode + ' is unknown!')
        if threshold_mode not in {'rel', 'abs'}:
            raise ValueError('threshold mode ' + threshold_mode + ' is unknown!')

        if mode == 'min':
            self.mode_worse = float('inf')
        else:  # mode == 'max':
            self.mode_worse = float('-inf')

    def state_dict(self):
        """Returns the state of the scheduler as a dict."""
        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}

    def load_state_dict(self, state_dict):
        """Loads the schedulers state."""
        self.__dict__.update(state_dict)

    def get_last_lr(self):
        """Return last computed learning rate."""
        return [group['lr'] for group in self.optimizer.param_groups]

class TorchOptimLRScheduler:
    """Container for learning rate schedulers."""
    def __init__(self, **kwargs):
        self.StepLR = StepLR
        self.ExponentialLR = ExponentialLR
        self.ReduceLROnPlateau = ReduceLROnPlateau

class TorchOptim:
    def __init__(self, **kwargs):
        self.SGD = SGDOptimizer
        self.Adam = AdamOptimizer
        self.AdamW = AdamWOptimizer
        self.RMSprop = RMSpropOptimizer
        self.Adagrad = AdagradOptimizer
        self.lr_scheduler = TorchOptimLRScheduler()

# CUDA support for WebGPU acceleration
class TorchCuda:
    def __init__(self, **kwargs):
        pass

    def is_available(self):
        """Check if WebGPU acceleration is available (maps CUDA to WebGPU)"""
        # In browser, we use WebGPU instead of CUDA
        try:
            # Use JavaScript to check if WebGPU is actually available
            import js
            if hasattr(js, 'navigator') and hasattr(js.navigator, 'gpu'):
                return js.navigator.gpu is not None
            else:
                return False
        except:
            return False

    def manual_seed(self, seed):
        """Set the seed for generating random numbers for CUDA (WebGPU in our case)"""
        # Map to global seed since we're using NumPy/WebGPU backend
        import numpy as np
        np.random.seed(seed)
        return None

    def manual_seed_all(self, seed):
        """Set the seed for generating random numbers on all GPUs"""
        # In browser context with WebGPU, this maps to the same as manual_seed
        import numpy as np
        np.random.seed(seed)
        return None

    def synchronize(self):
        """Wait for all kernels in all streams on current device to complete"""
        # No-op in browser context - WebGPU operations are already synchronized
        pass

    def device_count(self):
        """Returns the number of GPUs available"""
        # In browser context, we have at most 1 WebGPU device
        return 1 if self.is_available() else 0

# Backends support for device availability checks
class TorchBackends:
    def __init__(self, **kwargs):
        self.cuda = TorchCuda()
        self.mps = TorchMPS()

class TorchMPS:
    """Apple Metal Performance Shaders backend (mapped to WebGPU)"""
    def __init__(self, **kwargs):
        pass

    def is_available(self):
        """Check if MPS backend is available"""
        # In browser, we use WebGPU which can work on Apple devices
        # Return false for now as we primarily use WebGPU
        return False

    def is_built(self):
        """Check if PyTorch was built with MPS support"""
        return False

    def get_device_name(self, device=None):
        """Get the name of a device"""
        if self.is_available():
            return "WebGPU"
        return "CPU"

    def current_device(self):
        """Returns the index of a currently selected device"""
        return 0 if self.is_available() else -1

# Conv1d Layer
class TorchNNConv1d(TorchNNModule):
    """1D Convolution Layer"""
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size if isinstance(kernel_size, int) else kernel_size[0]
        self.stride = stride if isinstance(stride, int) else stride[0]
        self.padding = padding if isinstance(padding, int) else padding[0]
        self.dilation = dilation if isinstance(dilation, int) else dilation[0]
        self.groups = groups

        # Initialize weights
        k = np.sqrt(1.0 / (in_channels * self.kernel_size))
        weight_data = np.random.uniform(-k, k, (out_channels, in_channels, self.kernel_size))
        self.weight = WebGPUTensor(weight_data, requires_grad=True)
        self._parameters['weight'] = self.weight

        if bias:
            bias_data = np.random.uniform(-k, k, (out_channels,))
            self.bias = WebGPUTensor(bias_data, requires_grad=True)
            self._parameters['bias'] = self.bias
        else:
            self.bias = None

    def forward(self, x):
        # Input: (N, C_in, L_in)
        batch_size, in_channels, length = x.shape

        # Apply padding
        if self.padding > 0:
            x_padded = np.pad(x._data, ((0,0), (0,0), (self.padding, self.padding)), mode='constant')
        else:
            x_padded = x._data

        # Calculate output length
        out_length = (length + 2*self.padding - self.dilation*(self.kernel_size-1) - 1) // self.stride + 1

        # Initialize output
        output = np.zeros((batch_size, self.out_channels, out_length))

        # Perform convolution
        for b in range(batch_size):
            for oc in range(self.out_channels):
                for i in range(out_length):
                    start = i * self.stride
                    end = start + self.kernel_size
                    output[b, oc, i] = np.sum(x_padded[b, :, start:end] * self.weight._data[oc, :, :])
                    if self.bias is not None:
                        output[b, oc, i] += self.bias._data[oc]

        return WebGPUTensor(output, requires_grad=x.requires_grad or self.weight.requires_grad)

# AdaptiveAvgPool2d Layer
class TorchNNAdaptiveAvgPool2d(TorchNNModule):
    """Adaptive Average Pooling 2D"""
    def __init__(self, output_size, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        if isinstance(output_size, int):
            self.output_size = (output_size, output_size)
        else:
            self.output_size = output_size

    def forward(self, x):
        # Input: (N, C, H_in, W_in)
        batch_size, channels, h_in, w_in = x.shape
        h_out, w_out = self.output_size

        # Calculate stride and kernel size
        stride_h = h_in // h_out
        stride_w = w_in // w_out
        kernel_h = h_in - (h_out - 1) * stride_h
        kernel_w = w_in - (w_out - 1) * stride_w

        output = np.zeros((batch_size, channels, h_out, w_out))

        for b in range(batch_size):
            for c in range(channels):
                for i in range(h_out):
                    for j in range(w_out):
                        h_start = int(np.floor(i * h_in / h_out))
                        h_end = int(np.ceil((i + 1) * h_in / h_out))
                        w_start = int(np.floor(j * w_in / w_out))
                        w_end = int(np.ceil((j + 1) * w_in / w_out))
                        output[b, c, i, j] = np.mean(x._data[b, c, h_start:h_end, w_start:w_end])

        return WebGPUTensor(output, requires_grad=x.requires_grad)

# BatchNorm1d Layer
class TorchNNBatchNorm1d(TorchNNModule):
    """Batch Normalization 1D"""
    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats

        if affine:
            self.weight = WebGPUTensor(np.ones(num_features), requires_grad=True)
            self.bias = WebGPUTensor(np.zeros(num_features), requires_grad=True)
            self._parameters['weight'] = self.weight
            self._parameters['bias'] = self.bias
        else:
            self.weight = None
            self.bias = None

        if track_running_stats:
            self.running_mean = WebGPUTensor(np.zeros(num_features))
            self.running_var = WebGPUTensor(np.ones(num_features))
            self._buffers['running_mean'] = self.running_mean
            self._buffers['running_var'] = self.running_var
        else:
            self.running_mean = None
            self.running_var = None

    def forward(self, x):
        # Input can be (N, C) or (N, C, L)
        if self.training:
            # Calculate mean and var across batch dimension
            if x.ndim == 2:
                mean = np.mean(x._data, axis=0)
                var = np.var(x._data, axis=0)
            else:  # ndim == 3
                mean = np.mean(x._data, axis=(0, 2))
                var = np.var(x._data, axis=(0, 2))

            # Update running stats
            if self.track_running_stats:
                self.running_mean._data = (1 - self.momentum) * self.running_mean._data + self.momentum * mean
                self.running_var._data = (1 - self.momentum) * self.running_var._data + self.momentum * var
        else:
            mean = self.running_mean._data
            var = self.running_var._data

        # Normalize
        if x.ndim == 2:
            normalized = (x._data - mean) / np.sqrt(var + self.eps)
        else:
            normalized = (x._data - mean[None, :, None]) / np.sqrt(var[None, :, None] + self.eps)

        # Apply affine transformation
        if self.affine:
            if x.ndim == 2:
                output = normalized * self.weight._data + self.bias._data
            else:
                output = normalized * self.weight._data[None, :, None] + self.bias._data[None, :, None]
        else:
            output = normalized

        return WebGPUTensor(output, requires_grad=x.requires_grad)

# Embedding Layer
class TorchNNEmbedding(TorchNNModule):
    """Embedding Layer"""
    def __init__(self, num_embeddings, embedding_dim, padding_idx=None, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.padding_idx = padding_idx

        # Initialize embeddings
        weight_data = np.random.randn(num_embeddings, embedding_dim) * 0.01
        if padding_idx is not None:
            weight_data[padding_idx] = 0
        self.weight = WebGPUTensor(weight_data, requires_grad=True)
        self._parameters['weight'] = self.weight

    def forward(self, x):
        # Input: (N, L) or (N,) - indices
        indices = x._data.astype(int)
        output = self.weight._data[indices]
        return WebGPUTensor(output, requires_grad=self.weight.requires_grad)

# RNN Layer
class TorchNNRNN(TorchNNModule):
    """Simple RNN Layer"""
    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0, bidirectional=False, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        self.bidirectional = bidirectional

        # Initialize weights for each layer
        for layer in range(num_layers):
            input_dim = input_size if layer == 0 else hidden_size

            # Input to hidden
            w_ih = np.random.randn(hidden_size, input_dim) * 0.01
            self._parameters[f'weight_ih_l{layer}'] = WebGPUTensor(w_ih, requires_grad=True)

            # Hidden to hidden
            w_hh = np.random.randn(hidden_size, hidden_size) * 0.01
            self._parameters[f'weight_hh_l{layer}'] = WebGPUTensor(w_hh, requires_grad=True)

            if bias:
                b_ih = np.zeros(hidden_size)
                b_hh = np.zeros(hidden_size)
                self._parameters[f'bias_ih_l{layer}'] = WebGPUTensor(b_ih, requires_grad=True)
                self._parameters[f'bias_hh_l{layer}'] = WebGPUTensor(b_hh, requires_grad=True)

    def forward(self, x, h0=None):
        # Input: (seq_len, batch, input_size) if not batch_first
        # Or: (batch, seq_len, input_size) if batch_first
        if self.batch_first:
            x_data = np.transpose(x._data, (1, 0, 2))  # Convert to (seq, batch, input)
        else:
            x_data = x._data

        seq_len, batch_size, _ = x_data.shape

        # Initialize hidden state
        if h0 is None:
            h = np.zeros((self.num_layers, batch_size, self.hidden_size))
        else:
            h = h0._data

        outputs = []

        # Process each time step
        for t in range(seq_len):
            x_t = x_data[t]

            for layer in range(self.num_layers):
                w_ih = self._parameters[f'weight_ih_l{layer}']._data
                w_hh = self._parameters[f'weight_hh_l{layer}']._data

                if self.bias:
                    b_ih = self._parameters[f'bias_ih_l{layer}']._data
                    b_hh = self._parameters[f'bias_hh_l{layer}']._data
                else:
                    b_ih = 0
                    b_hh = 0

                # RNN cell: h_new = tanh(W_ih @ x + b_ih + W_hh @ h + b_hh)
                h[layer] = np.tanh(x_t @ w_ih.T + b_ih + h[layer] @ w_hh.T + b_hh)
                x_t = h[layer]

            outputs.append(h[-1])

        output_array = np.stack(outputs, axis=0)

        if self.batch_first:
            output_array = np.transpose(output_array, (1, 0, 2))

        output_tensor = WebGPUTensor(output_array, requires_grad=x.requires_grad)
        h_tensor = WebGPUTensor(h, requires_grad=True)

        return output_tensor, h_tensor

# LSTM Layer
class TorchNNLSTM(TorchNNModule):
    """LSTM Layer"""
    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0, bidirectional=False, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        self.bidirectional = bidirectional

        # Initialize weights (simplified LSTM)
        for layer in range(num_layers):
            input_dim = input_size if layer == 0 else hidden_size

            # All gates combined: input, forget, cell, output
            w_ih = np.random.randn(4 * hidden_size, input_dim) * 0.01
            w_hh = np.random.randn(4 * hidden_size, hidden_size) * 0.01

            self._parameters[f'weight_ih_l{layer}'] = WebGPUTensor(w_ih, requires_grad=True)
            self._parameters[f'weight_hh_l{layer}'] = WebGPUTensor(w_hh, requires_grad=True)

            if bias:
                b_ih = np.zeros(4 * hidden_size)
                b_hh = np.zeros(4 * hidden_size)
                self._parameters[f'bias_ih_l{layer}'] = WebGPUTensor(b_ih, requires_grad=True)
                self._parameters[f'bias_hh_l{layer}'] = WebGPUTensor(b_hh, requires_grad=True)

    def forward(self, x, hx=None):
        if self.batch_first:
            x_data = np.transpose(x._data, (1, 0, 2))
        else:
            x_data = x._data

        seq_len, batch_size, _ = x_data.shape

        if hx is None:
            h = np.zeros((self.num_layers, batch_size, self.hidden_size))
            c = np.zeros((self.num_layers, batch_size, self.hidden_size))
        else:
            h, c = hx[0]._data, hx[1]._data

        outputs = []

        for t in range(seq_len):
            x_t = x_data[t]

            for layer in range(self.num_layers):
                w_ih = self._parameters[f'weight_ih_l{layer}']._data
                w_hh = self._parameters[f'weight_hh_l{layer}']._data

                if self.bias:
                    b_ih = self._parameters[f'bias_ih_l{layer}']._data
                    b_hh = self._parameters[f'bias_hh_l{layer}']._data
                else:
                    b_ih = 0
                    b_hh = 0

                # LSTM gates
                gates = x_t @ w_ih.T + b_ih + h[layer] @ w_hh.T + b_hh

                i, f, g, o = np.split(gates, 4, axis=-1)
                i = 1 / (1 + np.exp(-i))  # sigmoid
                f = 1 / (1 + np.exp(-f))  # sigmoid
                g = np.tanh(g)
                o = 1 / (1 + np.exp(-o))  # sigmoid

                c[layer] = f * c[layer] + i * g
                h[layer] = o * np.tanh(c[layer])
                x_t = h[layer]

            outputs.append(h[-1])

        output_array = np.stack(outputs, axis=0)
        if self.batch_first:
            output_array = np.transpose(output_array, (1, 0, 2))

        return WebGPUTensor(output_array, requires_grad=x.requires_grad), (WebGPUTensor(h), WebGPUTensor(c))

# GRU Layer
class TorchNNGRU(TorchNNModule):
    """GRU Layer"""
    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0, bidirectional=False, **kwargs):
        self._parameters = {}
        self._modules = {}
        self._buffers = {}
        self._non_persistent_buffers_set = set()
        self._backward_hooks = {}
        self._backward_pre_hooks = {}
        self._forward_hooks = {}
        self._forward_pre_hooks = {}
        self._state_dict_hooks = {}
        self._load_state_dict_pre_hooks = {}
        self.training = True

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        self.bidirectional = bidirectional

        for layer in range(num_layers):
            input_dim = input_size if layer == 0 else hidden_size

            # Reset, update, new gates
            w_ih = np.random.randn(3 * hidden_size, input_dim) * 0.01
            w_hh = np.random.randn(3 * hidden_size, hidden_size) * 0.01

            self._parameters[f'weight_ih_l{layer}'] = WebGPUTensor(w_ih, requires_grad=True)
            self._parameters[f'weight_hh_l{layer}'] = WebGPUTensor(w_hh, requires_grad=True)

            if bias:
                b_ih = np.zeros(3 * hidden_size)
                b_hh = np.zeros(3 * hidden_size)
                self._parameters[f'bias_ih_l{layer}'] = WebGPUTensor(b_ih, requires_grad=True)
                self._parameters[f'bias_hh_l{layer}'] = WebGPUTensor(b_hh, requires_grad=True)

    def forward(self, x, h0=None):
        if self.batch_first:
            x_data = np.transpose(x._data, (1, 0, 2))
        else:
            x_data = x._data

        seq_len, batch_size, _ = x_data.shape

        if h0 is None:
            h = np.zeros((self.num_layers, batch_size, self.hidden_size))
        else:
            h = h0._data

        outputs = []

        for t in range(seq_len):
            x_t = x_data[t]

            for layer in range(self.num_layers):
                w_ih = self._parameters[f'weight_ih_l{layer}']._data
                w_hh = self._parameters[f'weight_hh_l{layer}']._data

                if self.bias:
                    b_ih = self._parameters[f'bias_ih_l{layer}']._data
                    b_hh = self._parameters[f'bias_hh_l{layer}']._data
                else:
                    b_ih = 0
                    b_hh = 0

                # GRU gates
                gi = x_t @ w_ih.T + b_ih
                gh = h[layer] @ w_hh.T + b_hh

                i_r, i_z, i_n = np.split(gi, 3, axis=-1)
                h_r, h_z, h_n = np.split(gh, 3, axis=-1)

                r = 1 / (1 + np.exp(-(i_r + h_r)))  # reset gate
                z = 1 / (1 + np.exp(-(i_z + h_z)))  # update gate
                n = np.tanh(i_n + r * h_n)  # new gate

                h[layer] = (1 - z) * n + z * h[layer]
                x_t = h[layer]

            outputs.append(h[-1])

        output_array = np.stack(outputs, axis=0)
        if self.batch_first:
            output_array = np.transpose(output_array, (1, 0, 2))

        return WebGPUTensor(output_array, requires_grad=x.requires_grad), WebGPUTensor(h)

class TorchNNUtils:
    """Neural network utilities"""
    @staticmethod
    def clip_grad_norm_(parameters, max_norm, norm_type=2.0):
        """
        Clips gradient norm of an iterable of parameters.

        Args:
            parameters: Iterable of parameters or single parameter
            max_norm: Max norm of the gradients
            norm_type: Type of the used p-norm (2 for L2 norm)

        Returns:
            Total norm of the parameters (viewed as a single vector)
        """
        if isinstance(parameters, WebGPUTensor) or isinstance(parameters, TorchNNParameter):
            parameters = [parameters]

        parameters = list(parameters)

        # Filter parameters that have gradients
        grads = []
        for p in parameters:
            if isinstance(p, TorchNNParameter):
                if hasattr(p.data, 'grad') and p.data.grad is not None:
                    grads.append(p.data.grad)
            elif hasattr(p, 'grad') and p.grad is not None:
                grads.append(p.grad)

        if len(grads) == 0:
            return 0.0

        # Calculate total norm
        if norm_type == float('inf'):
            total_norm = max(np.max(np.abs(g._data if isinstance(g, WebGPUTensor) else g)) for g in grads)
        else:
            # Calculate L-p norm
            norm_sum = 0.0
            for g in grads:
                grad_data = g._data if isinstance(g, WebGPUTensor) else g
                if norm_type == 2.0:
                    norm_sum += np.sum(grad_data ** 2)
                else:
                    norm_sum += np.sum(np.abs(grad_data) ** norm_type)

            total_norm = norm_sum ** (1.0 / norm_type)

        # Clip gradients
        clip_coef = max_norm / (total_norm + 1e-6)

        if clip_coef < 1:
            for g in grads:
                if isinstance(g, WebGPUTensor):
                    g._data *= clip_coef
                else:
                    g *= clip_coef

        return total_norm

class TorchNN:
    def __init__(self, **kwargs):
        self.functional = TorchNNFunctional()
        self.utils = TorchNNUtils()
        self.Parameter = TorchNNParameter
        self.Linear = TorchNNLinear
        self.Module = TorchNNModule
        self.Sequential = TorchNNSequential
        self.ModuleList = TorchNNModuleList
        self.ModuleDict = TorchNNModuleDict
        self.ReLU = TorchNNReLU
        self.LeakyReLU = TorchNNLeakyReLU
        self.Sigmoid = TorchNNSigmoid
        self.Tanh = TorchNNTanh
        self.GELU = TorchNNGELU
        self.SiLU = TorchNNSiLU
        self.Dropout = TorchNNDropout

        # Normalization layers
        self.BatchNorm1d = TorchNNBatchNorm1d
        self.BatchNorm2d = TorchNNBatchNorm2d

        # Convolution layers
        self.Conv1d = TorchNNConv1d
        self.Conv2d = TorchNNConv2d

        # Pooling layers
        self.MaxPool2d = TorchNNMaxPool2d
        self.AvgPool2d = TorchNNAvgPool2d
        self.AdaptiveAvgPool2d = TorchNNAdaptiveAvgPool2d

        # Recurrent layers
        self.RNN = TorchNNRNN
        self.LSTM = TorchNNLSTM
        self.GRU = TorchNNGRU

        # Embedding
        self.Embedding = TorchNNEmbedding

        # Loss functions
        self.MSELoss = TorchNNMSELoss
        self.L1Loss = TorchNNL1Loss
        self.CrossEntropyLoss = TorchNNCrossEntropyLoss
        self.BCELoss = TorchNNBCELoss
        self.BCEWithLogitsLoss = TorchNNBCEWithLogitsLoss
        self.NLLLoss = TorchNNNLLLoss
        self.SmoothL1Loss = TorchNNSmoothL1Loss
        self.KLDivLoss = TorchNNKLDivLoss
        self.CosineEmbeddingLoss = TorchNNCosineEmbeddingLoss
        self.HingeEmbeddingLoss = TorchNNHingeEmbeddingLoss
        self.MultiMarginLoss = TorchNNMultiMarginLoss
        self.TripletMarginLoss = TorchNNTripletMarginLoss
        self.PoissonNLLLoss = TorchNNPoissonNLLLoss
        self.CTCLoss = TorchNNCTCLoss

# Global cleanup function to prevent state persistence issues
def _cleanup_global_state():
    \"\"\"Clean up global tensor state to prevent persistence issues between executions\"\"\"
    import gc
    # Force garbage collection to clean up circular references
    gc.collect()

class TensorDataWrapper:
    """Wrapper for tensor data that provides PyTorch-like methods"""
    def __init__(self, numpy_array, parent_tensor=None, **kwargs):
        self._array = numpy_array
        self._parent = parent_tensor

    def cpu(self):
        """Return tensor data on CPU"""
        return TensorDataWrapper(self._array.copy(), self._parent)

    def numpy(self):
        """Return the underlying numpy array"""
        return self._array

    def __getitem__(self, key):
        """Support indexing operations"""
        result = self._array[key]
        return TensorDataWrapper(result, self._parent)

    def __setitem__(self, key, value):
        """Support item assignment"""
        if isinstance(value, TensorDataWrapper):
            self._array[key] = value._array
        else:
            self._array[key] = value

    def __len__(self):
        """Return length of the array"""
        return len(self._array)

    def __iter__(self):
        """Support iteration"""
        for item in self._array:
            yield TensorDataWrapper(item, self._parent) if hasattr(item, 'shape') else item

    # Arithmetic operators
    def __add__(self, other):
        """Addition operator"""
        if isinstance(other, TensorDataWrapper):
            result = self._array + other._array
        else:
            result = self._array + other
        return TensorDataWrapper(result, self._parent)

    def __sub__(self, other):
        """Subtraction operator"""
        if isinstance(other, TensorDataWrapper):
            result = self._array - other._array
        else:
            result = self._array - other
        return TensorDataWrapper(result, self._parent)

    def __mul__(self, other):
        """Multiplication operator"""
        if isinstance(other, TensorDataWrapper):
            result = self._array * other._array
        else:
            result = self._array * other
        return TensorDataWrapper(result, self._parent)

    def __truediv__(self, other):
        """Division operator"""
        if isinstance(other, TensorDataWrapper):
            result = self._array / other._array
        else:
            result = self._array / other
        return TensorDataWrapper(result, self._parent)

    def __pow__(self, other):
        """Power operator"""
        if isinstance(other, TensorDataWrapper):
            result = self._array ** other._array
        else:
            result = self._array ** other
        return TensorDataWrapper(result, self._parent)

    def clone(self):
        """Clone the data wrapper"""
        return TensorDataWrapper(self._array.copy(), self._parent)

    def norm(self, ord=2):
        """Compute the norm of the data"""
        return np.linalg.norm(self._array, ord=ord)

    # Reverse arithmetic operators
    def __radd__(self, other):
        """Reverse addition"""
        result = other + self._array
        return TensorDataWrapper(result, self._parent)

    def __rsub__(self, other):
        """Reverse subtraction"""
        result = other - self._array
        return TensorDataWrapper(result, self._parent)

    def __rmul__(self, other):
        """Reverse multiplication"""
        result = other * self._array
        return TensorDataWrapper(result, self._parent)

    def __rtruediv__(self, other):
        """Reverse division"""
        result = other / self._array
        return TensorDataWrapper(result, self._parent)

    def __getattr__(self, name):
        """Delegate unknown attributes to the underlying numpy array"""
        return getattr(self._array, name)

    def __array__(self):
        """Support numpy array interface"""
        return self._array

    def __repr__(self):
        return repr(self._array)

# Install in global namespace
torch = TorchModule()
sys.modules['torch'] = torch

# Register torch.nn and its submodules
sys.modules['torch.nn'] = torch.nn
sys.modules['torch.nn.functional'] = torch.nn.functional

# Create torch.nn.modules namespace for module imports
class TorchNNModules:
    def __init__(self, **kwargs):
        # Container modules
        self.module = type('module', (), {'Module': TorchNNModule})()
        self.container = type('container', (), {
            'Sequential': TorchNNSequential,
            'ModuleList': TorchNNModuleList,
            'ModuleDict': TorchNNModuleDict
        })()

        # Linear modules
        self.linear = type('linear', (), {'Linear': TorchNNLinear})()

        # Convolutional modules
        self.conv = type('conv', (), {'Conv2d': TorchNNConv2d})()

        # Pooling modules
        self.pooling = type('pooling', (), {
            'MaxPool2d': TorchNNMaxPool2d,
            'AvgPool2d': TorchNNAvgPool2d
        })()

        # Activation modules
        self.activation = type('activation', (), {
            'ReLU': TorchNNReLU,
            'Sigmoid': TorchNNSigmoid,
            'Tanh': TorchNNTanh,
            'LeakyReLU': TorchNNLeakyReLU,
            'GELU': TorchNNGELU,
            'SiLU': TorchNNSiLU
        })()

        # Normalization modules
        self.batchnorm = type('batchnorm', (), {'BatchNorm2d': TorchNNBatchNorm2d})()

        # Dropout modules
        self.dropout = type('dropout', (), {'Dropout': TorchNNDropout})()

        # Loss modules
        self.loss = type('loss', (), {
            'MSELoss': TorchNNMSELoss,
            'CrossEntropyLoss': TorchNNCrossEntropyLoss,
            'L1Loss': TorchNNL1Loss,
            'BCEWithLogitsLoss': TorchNNBCEWithLogitsLoss
        })()

nn_modules = TorchNNModules()
sys.modules['torch.nn.modules'] = nn_modules
sys.modules['torch.nn.modules.module'] = nn_modules.module
sys.modules['torch.nn.modules.container'] = nn_modules.container
sys.modules['torch.nn.modules.linear'] = nn_modules.linear
sys.modules['torch.nn.modules.conv'] = nn_modules.conv
sys.modules['torch.nn.modules.pooling'] = nn_modules.pooling
sys.modules['torch.nn.modules.activation'] = nn_modules.activation
sys.modules['torch.nn.modules.batchnorm'] = nn_modules.batchnorm
sys.modules['torch.nn.modules.dropout'] = nn_modules.dropout
sys.modules['torch.nn.modules.loss'] = nn_modules.loss

# Create torch.nn.parameter module
class ParameterModule:
    def __init__(self, **kwargs):
        self.Parameter = TorchNNParameter

parameter_module = ParameterModule()
sys.modules['torch.nn.parameter'] = parameter_module

# Register torch.linalg
sys.modules['torch.linalg'] = torch.linalg

# Register torch.utils and torch.utils.data
sys.modules['torch.utils'] = torch.utils
sys.modules['torch.utils.data'] = torch.utils.data

# Create torch.utils.data.dataset module
class DatasetModule:
    def __init__(self, **kwargs):
        self.Dataset = Dataset
        self.TensorDataset = TensorDataset

dataset_module = DatasetModule()
sys.modules['torch.utils.data.dataset'] = dataset_module

# Create torch.utils.data.dataloader module
class DataLoaderModule:
    def __init__(self, **kwargs):
        self.DataLoader = DataLoader

dataloader_module = DataLoaderModule()
sys.modules['torch.utils.data.dataloader'] = dataloader_module

# Register torch.optim
sys.modules['torch.optim'] = torch.optim

# Create torch.optim.optimizer module with Optimizer base class
class OptimizerModule:
    def __init__(self, **kwargs):
        self.Optimizer = Optimizer

optimizer_module = OptimizerModule()
sys.modules['torch.optim.optimizer'] = optimizer_module

# Create individual optimizer modules
class SGDModule:
    def __init__(self, **kwargs):
        self.SGD = SGDOptimizer

class AdamModule:
    def __init__(self, **kwargs):
        self.Adam = AdamOptimizer

class AdamWModule:
    def __init__(self, **kwargs):
        self.AdamW = AdamWOptimizer

class RMSpropModule:
    def __init__(self, **kwargs):
        self.RMSprop = RMSpropOptimizer

class AdagradModule:
    def __init__(self, **kwargs):
        self.Adagrad = AdagradOptimizer

sgd_module = SGDModule()
adam_module = AdamModule()
adamw_module = AdamWModule()
rmsprop_module = RMSpropModule()
adagrad_module = AdagradModule()

sys.modules['torch.optim.sgd'] = sgd_module
sys.modules['torch.optim.adam'] = adam_module
sys.modules['torch.optim.adamw'] = adamw_module
sys.modules['torch.optim.rmsprop'] = rmsprop_module
sys.modules['torch.optim.adagrad'] = adagrad_module

# Create torch.optim.lr_scheduler module
class LRSchedulerModule:
    def __init__(self, **kwargs):
        self.StepLR = StepLR
        self.ExponentialLR = ExponentialLR
        self.ReduceLROnPlateau = ReduceLROnPlateau

lr_scheduler_module = LRSchedulerModule()
sys.modules['torch.optim.lr_scheduler'] = lr_scheduler_module

# Register torch.cuda
sys.modules['torch.cuda'] = torch.cuda

# Create torch.autograd module for gradient utilities
class AutogradModule:
    def __init__(self, **kwargs):
        pass

    class Variable:
        """Deprecated: Variables are now just Tensors"""
        def __init__(self, data, requires_grad=False, **kwargs):
            return data

autograd_module = AutogradModule()
sys.modules['torch.autograd'] = autograd_module

# Clean up any existing state to prevent issues
_cleanup_global_state()
`;
}

/**
 * Get optimized PyTorch polyfill for specific operations
 */
export function getOptimizedPolyfill(operations = []) {
  const basePolyfill = createPyTorchPolyfill();
  
  // Add operation-specific optimizations
  let optimizations = '';
  
  if (operations.includes('matmul')) {
    optimizations += `
# Optimized matrix multiplication
def optimized_matmul(a, b):
    if hasattr(a, '_webgpu_buffer') and hasattr(b, '_webgpu_buffer'):
        # Use WebGPU acceleration
        return a._webgpu_matmul(b)
    return np.dot(a, b)

torch._matmul = optimized_matmul
`;
  }
  
  return basePolyfill + optimizations;
}

/**
 * Validate PyTorch polyfill before installation
 */
export function validatePolyfill(polyfillCode) {
  // Basic validation without pattern matching to avoid security conflicts
  if (!polyfillCode || typeof polyfillCode !== 'string') {
    throw new Error('Invalid polyfill code provided');
  }

  if (polyfillCode.length > 1000000) { // 1MB limit
    throw new Error('Polyfill code too large');
  }

  return true;
}